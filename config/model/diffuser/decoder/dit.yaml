defaults:
  # - backbone: pointnet2
  - backbone: ptv3_light

name: dit
rot_type: ${rot_type}

d_model: 512
num_layers: 12
num_heads: 8
d_head: 64
dropout: 0.1

max_sequence_length: 100
use_learnable_pos_embedding: false

time_embed_dim: 1024
time_embed_mult: 4

use_adaptive_norm: true
use_text_condition: ${use_text_condition}
text_dropout_prob: 0.01
use_negative_prompts: ${use_negative_prompts}
use_object_mask: ${use_object_mask}
use_rgb: ${use_rgb}

# Text Token 级别特征配置（与 FM 保持一致的开关）
use_text_tokens: false

# AdaLN-Zero 多条件融合配置（与 FM 对齐）
# use_adaln_zero: 是否使用 AdaLN-Zero 多条件调制（默认关闭以保持兼容）
# use_scene_pooling: 是否对场景特征进行池化作为条件（在 use_adaln_zero=true 时生效）
use_adaln_zero: false
use_scene_pooling: false
# 调制模式：multi(时间+场景+文本) | simple(仅时间)
adaln_mode: multi
# 双流-单流混合架构设置
use_double_stream: false
num_double_blocks: 0
single_block_variant: legacy 

# Attention dropout configuration
# attention_dropout: 应用于self-attention的softmax后dropout概率，用于正则化防止过拟合
# cross_attention_dropout: 应用于cross-attention的softmax后dropout概率
# 默认设置为0.0（禁用），训练时可根据需要启用（推荐0.05-0.1）
attention_dropout: 0.0
cross_attention_dropout: 0.0

ff_mult: 4
ff_dropout: 0.1

# MLP ratio for parallel single-stream blocks (Hunyuan3D-DiT style)
# 用于双流+单流架构中的并行单流块
# 仅在 use_double_stream=true 且 single_block_variant=parallel 时生效
mlp_ratio: 4.0

# Debug settings（可选，与 FM 对齐）
debug:
  check_nan: true
  log_tensor_stats: false

# Memory optimization
# 注意：如果使用 PyTorch 2.0+，模型会自动优先使用 SDPA (scaled_dot_product_attention)
# SDPA 会自动选择最快的后端（FlashAttention 2、Memory-efficient 或 Math），无需手动配置
gradient_checkpointing: false
use_flash_attention: true  # 仅在 SDPA 不可用时作为回退选项
attention_chunk_size: 512
memory_monitoring: false
max_batch_size: 32

enable_memory_efficient_attention: true
use_gradient_accumulation: false
memory_fraction: 0.9

# 数值与dtype相关开关
# enable_safety_checks: 是否启用注意力的数值安全检查（可能触发GPU同步，默认关闭以提升性能）
# default_dtype: 默认dtype，用于内存估算与策略启发（可选: float32 | float16 | bfloat16）
enable_safety_checks: false
default_dtype: float32

# Geometric Attention Bias 配置（与 FM 对齐）
# use_geometric_bias: 是否在 scene cross-attention 中使用几何注意力偏置（默认关闭）
# geometric_bias_hidden_dims: MLP隐藏层维度列表
# geometric_bias_feature_types: 使用的几何特征类型（可选: relative_pos, distance, direction, distance_log）
use_geometric_bias: false
geometric_bias_feature_types: ['distance']
geometric_bias_hidden_dims: [64]

# Global-Local Scene Conditioning 配置（两阶段场景条件）
# use_global_local_conditioning: 是否使用全局-局部两阶段场景条件（默认关闭以保持兼容）
# 当启用时，scene_cond 会被处理为：
#   1. 全局阶段：通过 GlobalScenePool 压缩到 K 个全局 latent
#   2. 局部阶段：基于抓取平移，从点云选取局部邻域（kNN/球查询）
use_global_local_conditioning: false

# 全局池化配置（Perceiver-IO 思路）
global_pool:
  num_latents: 128  # 全局 latent 数量 K（128~256）
  num_layers: 1     # latent cross-attn 层数（轻量级）
  dropout: 0.0      # Dropout 比例

# 局部选择器配置
local_selector:
  type: knn         # 选择器类型：knn | ball | deformable（deformable 暂未实现）
  k: 8              # kNN 的 k 值，或 ball query 的 max_samples
  radius: 0.05      # 球查询半径（仅 ball 类型有效）
  stochastic: false # 是否在训练时加入随机扰动（暂未实现）

# 可变形注意力配置（预留，暂未实现）
deformable:
  n_points: 4       # 每个查询点的采样点数
  n_heads: ${model.decoder.num_heads}  # 注意力头数
  n_levels: 1       # 多尺度层数
  proj_dim: ${model.decoder.d_model}   # 投影维度

# Time-aware Conditioning 配置
# use_t_aware_conditioning: 是否启用时间相关的条件门控（默认关闭以保持兼容）
# 通过时间门控因子 α(t) 动态调节 cross-attention 的影响强度
# 核心思想：扩散早期施加强约束，后期给模型更多自由度
use_t_aware_conditioning: false
t_gate:
  type: "cos2"  # 门控类型: "cos2" (余弦平方) | "mlp" (可学习MLP)
  apply_to: "both"  # 应用范围: "both" (场景+文本) | "scene" | "text"
  scene_scale: 1.0  # 场景条件的缩放因子
  text_scale: 1.0   # 文本条件的缩放因子
  separate_text_gate: false  # 是否为文本条件使用独立的门控
  # MLP门控专用配置
  mlp_hidden_dims: [256, 128]  # MLP隐藏层维度
  init_value: 1.0  # 初始输出值（训练前期固定）
  warmup_steps: 1000  # 固定输出的步数
