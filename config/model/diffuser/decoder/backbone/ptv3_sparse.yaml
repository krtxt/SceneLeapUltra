# PTv3 稀疏 Token 提取器配置
# 专门用于生成稀疏的 scene tokens

name: ptv3_sparse
use_flash_attention: true

# ============= 关键参数：控制稀疏程度 =============

# grid_size: 体素化精度
#   - 越小 → 初始体素越多 → 更精细的表示（编码更慢/更占显存）
#   - 越大 → 初始体素越少 → 更粗糙的表示（更快/更省）
#   推荐范围: 0.01 - 0.03（0.02 平衡速度/表示；0.01 更细；0.03 更快）
grid_size: 0.02

# stride: Encoder 下采样步长（相邻阶段之间）
#   - 长度必须是 len(encoder_channels) - 1
#   - 每个值必须是 2 的幂: 2, 4, 8, 16
#   - 5个阶段 → 需要4个stride值
#   - (2,2,2,2) → 各层下采样 2x，总共 16x
stride: [2, 2, 2, 2]

# target_num_tokens: 目标 token 数量（可选）
#   - 如果设置，会自适应池化到这个数量
#   - 如果不设置（null），输出自然的稀疏数量
#   推荐: 128-512 for DiT
target_num_tokens: 128

# token_strategy: Token选择策略
#   - fps: FPS采样（推荐，固定长度+空间均匀，最稳）
#   - last_layer: 直接取最后层特征（快速原型；K_full<target 时会零填充）
#   - grid: 网格聚合（空间规整，较慢）
#   - learned: 可学习tokenizer（需训练，覆盖度好）
#   - multiscale: 多尺度融合（信息丰富，开销大）
token_strategy: fps

# tokens_last: 输出特征为 [B, K, D]（便于接 DiT/Transformer）
tokens_last: true

# ============= Encoder 配置（与 ptv3_light 对齐） =============

# Encoder 通道数（越大越强但越慢）
encoder_channels: [32, 64, 128, 256, 512]

# Encoder 深度（每个阶段的 Transformer 层数）
encoder_depths: [1, 1, 2, 2, 1]

# Encoder 注意力头数
encoder_num_head: [2, 4, 8, 16, 16]

# Encoder patch size
enc_patch_size: [1024, 1024, 1024, 1024, 1024]

# MLP ratio
mlp_ratio: 2

# ============= 输出配置 =============

# 输出特征维度（与 DiT 对齐）
out_dim: 256

# 输入特征维度（除了 xyz 之外的特征）
input_feature_dim: 1

# ============= 不同场景的预设配置 =============

# 预设1: 精细表示（更多 tokens）
# grid_size: 0.01
# stride: [2, 2, 2]  # 4阶段
# target_num_tokens: 1024
# → 示例范围：自然约 800–1200，或池化到 1024（数据依赖）

# 预设2: 平衡（推荐）
# grid_size: 0.02
# stride: [2, 2, 2]  # 4阶段
# target_num_tokens: 512
# → 示例范围：自然约 400–600，或池化到 512（数据依赖）

# 预设3: 紧凑表示（更少 tokens）
# grid_size: 0.04
# stride: [2, 2, 2]  # 4阶段
# target_num_tokens: 256
# → 示例范围：自然约 200–300，或池化到 256（数据依赖）

# 预设4: 极致稀疏（最少 tokens）
# grid_size: 0.05
# stride: [2, 4]  # 3阶段，更强下采样
# encoder_channels: [32, 64, 128]
# target_num_tokens: 128
# → 输出约 100-150 个 tokens (自然) 或 128 (池化后)

# ============= 预期性能 =============
# 输入: 8192 点
# grid_size=0.003, stride=(2,2,2), target_num_tokens=128:
#   - 自然稀疏输出: ~400-800 tokens
#   - FPS/池化后: 128 tokens
#   - 有效token利用率: 100% (所有128个token都是有效的)
#   - 推理时间: ~750ms (last_layer策略)
#   - 显存占用: ~133 MB
#   - DiT 计算复杂度: O(128²) = 16K (vs 8192² = 67M，降低 99.98%)
#
# 不同grid_size对比:
#   grid_size=0.02: 速度快(~170ms)，但有效token仅7% (9/128)
#   grid_size=0.003: 速度中等(~750ms)，有效token 100% (128/128) ✅推荐

