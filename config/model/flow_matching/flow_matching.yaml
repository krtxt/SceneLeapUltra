defaults:
  - decoder: dit_fm
  - criterion: loss_standardized
  - _self_

name: GraspFlowMatching
save_root: ${save_root}
device: cuda:0
rot_type: ${rot_type}
mode: ${mode}  # 坐标系模式（camera_centric_scene_mean_normalized等）
print_freq: 250
batch_size: ${batch_size}

# Flow Matching specific configuration
fm:
  variant: rectified_flow  # rectified_flow | cfm | sfm
  path: linear_ot  # linear_ot | vp | ve
  continuous_time: true
  
  # Time sampling strategy
  t_sampler: uniform  # uniform | cosine | beta
  t_weight: null  # null | cosine | beta (for loss reweighting)
  
  # Noise distribution
  noise_dist: normal  # normal | custom
  
  # Stochastic Flow Matching (SFM) - for ablation
  sfm:
    sigma: 0.0  # Set > 0 to enable stochastic bridges

# ODE Solver configuration
solver:
  type: rk4  # heun | rk4 | rk45
  nfe: 32  # Number of function evaluations (for fixed-step solvers)
  
  # Adaptive solver settings (for rk45)
  rtol: 1e-3
  atol: 1e-5
  max_step: 0.03125  # 1/32
  min_step: 1e-4
  max_nfe: 1000  # Maximum NFE for adaptive solver
  
  # Integration settings
  reverse_time: true  # Integrate from t=1 to t=0
  save_trajectories: false  # Save intermediate states for visualization

# Classifier-Free Guidance configuration
guidance:
  enable_cfg: false  # Enable CFG (set to false initially for stability)
  cond_drop_prob: 0.10  # Conditioning dropout probability during training
  scale: 3.0  # CFG scale (higher = stronger conditioning)
  
  # Stabilization techniques
  method: clipped  # basic | clipped | rescaled | adaptive
  diff_clip: 5.0  # Norm clipping threshold for (v_cond - v_uncond)
  pc_correction: false  # Enable predictor-corrector refinement
  dt_correction: 0.01  # Step size for predictor-corrector correction
  num_corrections: 1  # Number of correction iterations
  
  # Adaptive CFG settings (when method=adaptive)
  early_steps_scale: 0.0  # Scale for early timesteps
  late_steps_scale: 1.0  # Scale for late timesteps
  transition_point: 0.5  # Transition point (0 to 1)

# # Optimizer configuration
# optimizer:
#   name: adamw
#   lr: 0.0004
#   weight_decay: 0.001

# # Scheduler configuration
# scheduler:
#   name: steplr
#   t_max: 1000
#   min_lr: 1.0e-05
#   step_size: 100
#   step_gamma: 0.5
# Optimizer configuration
optimizer:
  name: adamw
  lr: 0.0004           # 保持高初始LR，以实现快速收敛
  weight_decay: 0.001

# Scheduler configuration
scheduler:
  name: cosine           # <-- 关键改动：使用余弦退火
  t_max: 1000            # <-- 设为总 epoch 数
  min_lr: 1.0e-6         # <-- 衰减到的最低学习率
  step_size: null
  step_gamma: null
  # (step_size 和 step_gamma 是 steplr 专用的，cosine不需要)

# Debug and logging
debug:
  check_nan: true
  log_tensor_stats: false  # Set to true for detailed debugging
  print_freq: 50

# Compatibility settings
compat:
  keep_ddpm_interface: true  # Maintain interface compatibility with DDPM
  export_noise_like_keys: true  # Export v_pred as "pred_noise" for loss compatibility

# Grasp count control
fix_num_grasps: ${fix_num_grasps}
target_num_grasps: ${target_num_grasps}

# Conditioning flags
use_negative_prompts: ${use_negative_prompts}
use_object_mask: ${use_object_mask}
use_rgb: ${use_rgb}

# Loss configuration (reuse from diffuser)
loss_type: l2
out_sigmoid: false
use_score: false
score_pretrain: false

# WandB optimization (same as diffuser)
wandb_optimization:
  enable_visualization: false
  visualization_freq: 20
  log_histograms: false
  histogram_freq: 50
  monitor_system: false
  log_gradients: false
  gradient_freq: 1000
  system_freq: 500

