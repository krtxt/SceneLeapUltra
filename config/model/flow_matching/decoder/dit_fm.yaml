defaults:
  # - backbone: pointnet2
  - backbone: ptv3_light

name: dit_fm
rot_type: ${rot_type}
pred_mode: velocity  # Flow Matching预测模式: velocity(FM)|epsilon(DDPM)|pose(直接)

# Model architecture (reuse DiT structure)
d_model: 512
num_layers: 12
num_heads: 8
d_head: 64
dropout: 0.1

# Sequence configuration
max_sequence_length: 100
use_learnable_pos_embedding: false

# Time embedding configuration
time_embed_dim: 1024
time_embed_mult: 4
use_adaptive_norm: true

# Flow Matching specific: continuous time embedding
continuous_time: true
freq_dim: 256  # Gaussian random Fourier features dimension

# Conditioning configuration
use_text_condition: ${use_text_condition}
text_dropout_prob: 0.01
use_negative_prompts: ${use_negative_prompts}
use_object_mask: ${use_object_mask}
use_rgb: ${use_rgb}

# Attention configuration
# attention_dropout: 应用于self-attention的softmax后dropout概率，用于正则化防止过拟合
# cross_attention_dropout: 应用于cross-attention的softmax后dropout概率
# 默认设置为0.0（禁用），训练时可根据需要启用（推荐0.05-0.1）
attention_dropout: 0.0
cross_attention_dropout: 0.0

mmdit:
  enabled: false
  separate_modality_projections: true
  qkv_bias: false
  qk_norm: false
  modality_specific_adaln: false

# Feed-forward configuration
ff_mult: 4
ff_dropout: 0.1

# Velocity head
# 是否在 velocity head 前添加 LayerNorm（默认关闭，便于与现有实验对齐）
velocity_head_use_layer_norm: false

# Memory optimization
# 注意：如果使用 PyTorch 2.0+，模型会自动优先使用 SDPA (scaled_dot_product_attention)
# SDPA 会自动选择最快的后端（FlashAttention 2、Memory-efficient 或 Math），无需手动配置
gradient_checkpointing: false
use_flash_attention: true  # 仅在 SDPA 不可用时作为回退选项
attention_chunk_size: 512
memory_monitoring: false
max_batch_size: 32

enable_memory_efficient_attention: true
use_gradient_accumulation: false
memory_fraction: 0.9

# Debug settings
debug:
  check_nan: true
  log_tensor_stats: false

