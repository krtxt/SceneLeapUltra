╔═══════════════════════════════════════════════════════════════════════════════╗
║         AdaLN-Zero + Scene Pooling 完整数据流图                               ║
╚═══════════════════════════════════════════════════════════════════════════════╝

┌─────────────────────────────────────────────────────────────────────────────┐
│  输入数据                                                                    │
└─────────────────────────────────────────────────────────────────────────────┘
         │
         ├─── scene_pc: [B, N, 3/6]          场景点云（XYZ + RGB可选）
         │
         ├─── positive_prompt: List[str]     文本提示（如 "grasp the cup"）
         │
         ├─── x_t: [B, G, d_x]              噪声抓取姿态（平移+旋转）
         │
         └─── ts: [B]                        时间步 t ∈ [0,1]


╔═════════════════════════════════════════════════════════════════════════════╗
║  阶段 1: 特征提取                                                            ║
╚═════════════════════════════════════════════════════════════════════════════╝

┌──────────────────────────────────────┐
│  场景特征提取                         │
│  (PointNet2/PTv3)                    │
└──────────────────────────────────────┘
         │
         │  scene_pc [B, N, 3/6]
         ↓
   ┌──────────────┐
   │ scene_model  │  → sampled_xyz [B, K, 3] (采样后的坐标)
   └──────────────┘
         ↓
   scene_feat [B, K, C]
         ↓
   ┌──────────────┐
   │  projection  │
   └──────────────┘
         ↓
   scene_context [B, K, d_model=512]
   scene_mask [B, K]


┌──────────────────────────────────────┐
│  时间嵌入                             │
│  (Continuous Time Embedding)         │
└──────────────────────────────────────┘
         │
         │  ts [B]
         ↓
   ┌────────────────────┐
   │  Fourier Features  │  cos/sin 编码
   └────────────────────┘
         ↓
   ┌────────────────────┐
   │       MLP          │
   └────────────────────┘
         ↓
   time_emb_base [B, d_model=512]
         ↓
   ┌────────────────────┐
   │    time_proj       │  投影到更高维
   └────────────────────┘
         ↓
   time_emb [B, time_embed_dim=1024]


┌──────────────────────────────────────┐
│  文本特征提取                         │
│  (CLIP/T5)                           │
└──────────────────────────────────────┘
         │
         │  positive_prompt: ["grasp the cup", ...]
         ↓
   ┌──────────────┐
   │ text_encoder │  CLIP text encoder
   └──────────────┘
         ↓
   text_tokens [B, L_text, d_model]
   text_pooled [B, d_model=512]
         ↓
   ┌──────────────┐
   │text_processor│  投影 + dropout
   └──────────────┘
         ↓
   text_context [B, 1, d_model=512]


╔═════════════════════════════════════════════════════════════════════════════╗
║  阶段 2: 场景池化 (use_scene_pooling=True 的关键步骤)                       ║
╚═════════════════════════════════════════════════════════════════════════════╝

   scene_context [B, K, d_model=512]
   scene_mask [B, K]
         │
         ↓
   ┌──────────────────────────────────────┐
   │  pool_scene_features()               │
   │                                      │
   │  公式:                                │
   │  pooled = Σ(features * mask) / Σ mask │
   └──────────────────────────────────────┘
         ↓
   scene_pooled [B, d_model=512]  ← 全局场景特征向量


╔═════════════════════════════════════════════════════════════════════════════╗
║  阶段 3: 多条件融合 (use_adaln_zero=True 的核心机制)                        ║
╚═════════════════════════════════════════════════════════════════════════════╝

         ┌──── time_emb [B, 1024]       时间步条件
         │
         ├──── scene_pooled [B, 512]    场景全局条件
         │
         └──── text_pooled [B, 512]     文本条件
                    ↓
         ┌──────────────────────┐
         │   torch.cat(dim=-1)   │
         └──────────────────────┘
                    ↓
         cond_vector [B, cond_dim=2048]  ← 融合的条件向量


╔═════════════════════════════════════════════════════════════════════════════╗
║  阶段 4: 抓取姿态 Tokenization                                               ║
╚═════════════════════════════════════════════════════════════════════════════╝

   x_t [B, G, d_x=23]  噪声抓取姿态
         ↓
   ┌──────────────┐
   │GraspTokenizer│  Linear + LayerNorm
   └──────────────┘
         ↓
   grasp_tokens [B, G, d_model=512]
         ↓
   ┌──────────────┐
   │pos_embedding │  (可选) 位置编码
   └──────────────┘
         ↓
   x [B, G, d_model=512]


╔═════════════════════════════════════════════════════════════════════════════╗
║  阶段 5: DiT Blocks (12 层)                                                  ║
╚═════════════════════════════════════════════════════════════════════════════╝

   for layer in [0, 1, 2, ..., 11]:

   ┌─────────────────────────────────────────────────────────────────────────┐
   │  DiT Block (Layer i)                                                    │
   │                                                                         │
   │  输入: x [B, G, d_model]                                                 │
   │       cond_vector [B, 2048]                                             │
   │       scene_context [B, K, d_model]                                     │
   │       text_context [B, 1, d_model]                                      │
   └─────────────────────────────────────────────────────────────────────────┘
         │
         │  ┌────────────────────────────────────────────────────────┐
         │  │  步骤 1: Self-Attention                                 │
         │  └────────────────────────────────────────────────────────┘
         ↓
   ┌───────────────────────┐
   │ norm1(x, cond_vector) │  AdaLN-Zero 调制
   └───────────────────────┘
         ↓                      cond_vector [B, 2048]
   ┌─────────────┐                  ↓
   │AdaLNZero    │          ┌───────────────┐
   │             │          │  modulation   │  Linear(2048, 3*512)
   │ 公式:        │          │    MLP       │
   │ scale, shift│ ←────────└───────────────┘
   │ gate = MLP  │                  ↓
   │            │          [scale, shift, gate]
   │ norm_x =   │          每个: [B, 512] → [B, 1, 512]
   │ LayerNorm  │                  ↓
   │            │          ┌──────────────────────────┐
   │ modulated =│          │ norm_x = LayerNorm(x)    │
   │ (1+scale)* │          │ modulated = (1+scale)*   │
   │ norm_x +   │          │             norm_x+shift │
   │ shift      │          │ output = x + gate*       │
   │            │          │          modulated       │
   │ output =   │          └──────────────────────────┘
   │ x + gate * │
   │ modulated  │
   └─────────────┘
         ↓
   norm_x [B, G, d_model]
         ↓
   ┌─────────────────┐
   │ Self-Attention  │  Q=K=V=norm_x
   └─────────────────┘
         ↓
   attn_out [B, G, d_model]
         ↓
   x = x + attn_out  (残差连接)


         │  ┌────────────────────────────────────────────────────────┐
         │  │  步骤 2: Scene Cross-Attention                          │
         │  └────────────────────────────────────────────────────────┘
         ↓
   ┌───────────────────────┐
   │ norm2(x, cond_vector) │  AdaLN-Zero 调制
   └───────────────────────┘
         ↓
   norm_x [B, G, d_model]
         ↓
   ┌─────────────────────────┐
   │ Scene Cross-Attention   │  Q=norm_x, K=V=scene_context
   │                         │  mask=scene_mask
   └─────────────────────────┘
         ↓
   scene_attn [B, G, d_model]
         ↓
   x = x + scene_attn  (残差连接)


         │  ┌────────────────────────────────────────────────────────┐
         │  │  步骤 3: Text Cross-Attention                           │
         │  └────────────────────────────────────────────────────────┘
         ↓
   ┌───────────────────────┐
   │ norm3(x, cond_vector) │  AdaLN-Zero 调制
   └───────────────────────┘
         ↓
   norm_x [B, G, d_model]
         ↓
   ┌─────────────────────────┐
   │ Text Cross-Attention    │  Q=norm_x, K=V=text_context
   └─────────────────────────┘
         ↓
   text_attn [B, G, d_model]
         ↓
   x = x + text_attn  (残差连接)


         │  ┌────────────────────────────────────────────────────────┐
         │  │  步骤 4: Feed-Forward                                   │
         │  └────────────────────────────────────────────────────────┘
         ↓
   ┌───────────────────────┐
   │ norm4(x, cond_vector) │  AdaLN-Zero 调制
   └───────────────────────┘
         ↓
   norm_x [B, G, d_model]
         ↓
   ┌─────────────────────────┐
   │   Feed-Forward          │
   │   Linear + GELU +       │
   │   Dropout + Linear      │
   └─────────────────────────┘
         ↓
   ff_out [B, G, d_model]
         ↓
   x = x + ff_out  (残差连接)

   [重复 12 次]


╔═════════════════════════════════════════════════════════════════════════════╗
║  阶段 6: 输出投影                                                            ║
╚═════════════════════════════════════════════════════════════════════════════╝

   x [B, G, d_model=512]
         ↓
   ┌──────────────────┐
   │  velocity_head   │  Linear(d_model, d_x)
   │  (Flow Matching) │  或 output_projection (DDPM)
   └──────────────────┘
         ↓
   output [B, G, d_x=23]  预测的速度场 (FM) 或噪声 (DDPM)


╔═════════════════════════════════════════════════════════════════════════════╗
║  关键差异对比                                                                ║
╚═════════════════════════════════════════════════════════════════════════════╝

┌────────────────────────────────────────────────────────────────────────────┐
│  原版 (use_adaln_zero=False, use_scene_pooling=False)                      │
└────────────────────────────────────────────────────────────────────────────┘

   在每个 DiT Block 中:
   
   norm1 = AdaptiveLayerNorm(d_model, time_embed_dim)
   norm_x = norm1(x, time_emb)  ← 仅使用时间步
                     └─ [B, 1024]
   
   特点:
   - ✓ 参数量较小 (每层 1.05M)
   - ✗ 只有时间步条件
   - ✗ 场景/文本信息仅通过 cross-attention


┌────────────────────────────────────────────────────────────────────────────┐
│  增强版 (use_adaln_zero=True, use_scene_pooling=True)                      │
└────────────────────────────────────────────────────────────────────────────┘

   在每个 DiT Block 中:
   
   norm1 = AdaLNZero(d_model, cond_dim=2048)
   norm_x = norm1(x, cond_vector)  ← 多条件融合
                     └─ [B, 2048] = concat([time, scene_pooled, text])
   
   特点:
   - ✓ 更丰富的条件信息 (时间 + 场景全局 + 文本)
   - ✓ 零初始化 (训练初期稳定)
   - ✓ 门控机制 (动态调整调制强度)
   - ✗ 参数量较大 (每层 3.15M)


╔═════════════════════════════════════════════════════════════════════════════╗
║  涉及的代码文件                                                              ║
╚═════════════════════════════════════════════════════════════════════════════╝

1. config/model/flow_matching/decoder/dit_fm.yaml
   第 72-73 行: use_adaln_zero, use_scene_pooling 配置

2. models/decoder/dit_fm.py
   第 131-132 行: 读取配置
   第 200-213 行: 计算 cond_dim
   第 242-261 行: 初始化 DiT Blocks
   第 459-487 行: 准备多条件向量
   第 546-559 行: 调用 DiT Blocks

3. models/decoder/dit.py
   第 288-339 行: AdaLNZero 类定义
   第 365-660 行: DiTBlock 类
   第 460-465 行: 使用 AdaLN-Zero (norm1)
   第 484-489 行: 使用 AdaLN-Zero (norm2)
   第 601-606 行: 使用 AdaLN-Zero (norm3)
   第 641-646 行: 使用 AdaLN-Zero (norm4)

4. models/decoder/dit_conditioning.py
   第 361-404 行: pool_scene_features() 函数


╔═════════════════════════════════════════════════════════════════════════════╗
║  零初始化验证                                                                ║
╚═════════════════════════════════════════════════════════════════════════════╝

在模型初始化后 (dit_fm.py 第 340-350 行):

   velocity_head 零初始化:
   ✓ velocity_head.weight ← zeros
   ✓ velocity_head.bias ← zeros

在 AdaLNZero 初始化时 (dit.py 第 315-316 行):

   modulation MLP 零初始化:
   ✓ modulation.weight ← zeros
   ✓ modulation.bias ← zeros

效果:
   在训练初期:
   - scale = 0, shift = 0, gate = 0
   - output = x + 0 * (...) = x  (恒等映射)
   - 梯度从小到大逐渐学习
   - 避免训练不稳定


╔═════════════════════════════════════════════════════════════════════════════╗
║  总结                                                                        ║
╚═════════════════════════════════════════════════════════════════════════════╝

启用 use_adaln_zero=True + use_scene_pooling=True 后:

1. 场景特征从 [B, K, D] 池化为 [B, D]
2. 条件向量融合时间步、场景全局、文本特征
3. 每个 DiT Block 使用 AdaLN-Zero 替代 AdaptiveLayerNorm
4. 零初始化确保训练稳定性
5. 门控机制允许动态调整调制强度

适用场景:
- ✅ 场景全局信息很重要
- ✅ 需要强大的多模态条件融合
- ✅ 对训练稳定性有高要求

