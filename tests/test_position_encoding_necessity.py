"""
æµ‹è¯•ä½ç½®ç¼–ç å¯¹æŠ“å–é›†åˆå­¦ä¹ çš„å½±å“

éªŒè¯å‡è®¾ï¼š
1. æŠ“å–é›†åˆåº”è¯¥æ˜¯ç½®æ¢ä¸å˜çš„
2. ä½ç½®ç¼–ç ä¼šç ´åè¿™ç§ä¸å˜æ€§
3. å»é™¤ä½ç½®ç¼–ç ä¸ä¼šå½±å“æ¨¡å‹æ€§èƒ½ï¼ˆå¯èƒ½è¿˜ä¼šæå‡ï¼‰
"""

import torch
import torch.nn as nn
import sys
import os
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

from models.fm.optimal_transport import SinkhornOT


def test_permutation_invariance_with_positional_encoding():
    """
    æµ‹è¯•åŠ äº†ä½ç½®ç¼–ç åï¼Œæ¨¡å‹æ˜¯å¦è¿˜ä¿æŒç½®æ¢ä¸å˜æ€§
    """
    print("\n" + "="*80)
    print("æµ‹è¯•1: ä½ç½®ç¼–ç å¯¹ç½®æ¢ä¸å˜æ€§çš„å½±å“")
    print("="*80)
    
    B, N, D = 2, 1024, 25
    d_model = 512
    
    # æ¨¡æ‹Ÿ grasp tokenizer + positional encoding
    class SimpleTokenizerWithPosEmb(nn.Module):
        def __init__(self, use_pos_emb=False):
            super().__init__()
            self.tokenizer = nn.Linear(D, d_model)
            self.use_pos_emb = use_pos_emb
            if use_pos_emb:
                self.pos_emb = nn.Parameter(torch.randn(1, N, d_model) * 0.02)
        
        def forward(self, x):
            tokens = self.tokenizer(x)
            if self.use_pos_emb:
                tokens = tokens + self.pos_emb
            return tokens
    
    # åˆ›å»ºä¸¤ä¸ªæ¨¡å‹ï¼šæœ‰/æ— ä½ç½®ç¼–ç 
    model_no_pos = SimpleTokenizerWithPosEmb(use_pos_emb=False)
    model_with_pos = SimpleTokenizerWithPosEmb(use_pos_emb=True)
    
    # æµ‹è¯•æ•°æ®
    x = torch.randn(B, N, D)
    perm = torch.randperm(N)
    x_shuffled = x[:, perm]
    
    # æ— ä½ç½®ç¼–ç ï¼šåº”è¯¥ç½®æ¢ç­‰å˜
    with torch.no_grad():
        out_no_pos_1 = model_no_pos(x)
        out_no_pos_2 = model_no_pos(x_shuffled)
        # è¿˜åŸé¡ºåº
        out_no_pos_2_restored = torch.zeros_like(out_no_pos_2)
        out_no_pos_2_restored[:, perm] = out_no_pos_2
        
        diff_no_pos = (out_no_pos_1 - out_no_pos_2_restored).abs().max().item()
    
    # æœ‰ä½ç½®ç¼–ç ï¼šä¼šç ´åç½®æ¢ä¸å˜æ€§
    with torch.no_grad():
        out_with_pos_1 = model_with_pos(x)
        out_with_pos_2 = model_with_pos(x_shuffled)
        out_with_pos_2_restored = torch.zeros_like(out_with_pos_2)
        out_with_pos_2_restored[:, perm] = out_with_pos_2
        
        diff_with_pos = (out_with_pos_1 - out_with_pos_2_restored).abs().max().item()
    
    print(f"\nç»“æœï¼š")
    print(f"  æ— ä½ç½®ç¼–ç æ—¶çš„å·®å¼‚: {diff_no_pos:.6f}  {'âœ… ç½®æ¢ç­‰å˜' if diff_no_pos < 1e-5 else 'âŒ'}")
    print(f"  æœ‰ä½ç½®ç¼–ç æ—¶çš„å·®å¼‚: {diff_with_pos:.6f}  {'âœ… ç½®æ¢ç­‰å˜' if diff_with_pos < 1e-5 else 'âŒ ç ´åäº†ç½®æ¢ä¸å˜æ€§'}")
    
    print(f"\nç»“è®ºï¼šä½ç½®ç¼–ç ç ´åäº†é›†åˆçš„ç½®æ¢ä¸å˜æ€§ï¼")
    print(f"      å¯¹äºæŠ“å–å§¿æ€è¿™ç§æ— åºé›†åˆï¼Œåº”è¯¥ **é¿å…** ä½¿ç”¨ä½ç½®ç¼–ç ã€‚")
    
    return diff_no_pos, diff_with_pos


def test_sinkhorn_matching_randomness():
    """
    æµ‹è¯• SinkhornOT é…å¯¹åçš„é¡ºåºæ˜¯å¦æœ‰æ„ä¹‰
    """
    print("\n" + "="*80)
    print("æµ‹è¯•2: SinkhornOT é…å¯¹åçš„ç´¢å¼•é¡ºåºåˆ†æ")
    print("="*80)
    
    B, N, D = 1, 100, 3
    
    # åˆ›å»ºä¸¤ä¸ªç‚¹é›†
    x0 = torch.randn(B, N, D)
    x1 = torch.randn(B, N, D)
    
    # è¿›è¡Œé…å¯¹
    sinkhorn_ot = SinkhornOT(reg=0.1, num_iters=50)
    matchings = sinkhorn_ot(x0, x1)
    
    print(f"\né…å¯¹ç´¢å¼•ç¤ºä¾‹ï¼ˆå‰20ä¸ªï¼‰ï¼š")
    print(f"  {matchings[0, :20].tolist()}")
    
    # åˆ†æç´¢å¼•çš„è¿ç»­æ€§
    diffs = torch.diff(matchings[0].float()).abs()
    avg_jump = diffs.mean().item()
    max_jump = diffs.max().item()
    
    print(f"\nç´¢å¼•è·³è·ƒåˆ†æï¼š")
    print(f"  å¹³å‡è·³è·ƒè·ç¦»: {avg_jump:.2f}")
    print(f"  æœ€å¤§è·³è·ƒè·ç¦»: {max_jump:.0f}")
    print(f"  ç†è®ºéšæœºè·³è·ƒ: {N/2:.2f}")
    
    # åˆ¤æ–­
    if avg_jump > N / 4:
        print(f"\nç»“è®ºï¼šé…å¯¹åçš„ç´¢å¼•æ˜¯ **é«˜åº¦æ— åº** çš„ï¼")
        print(f"      ç¬¬ i ä¸ªå’Œç¬¬ i+1 ä¸ªæŠ“å–åœ¨ç©ºé—´ä¸Šæ²¡æœ‰é‚»è¿‘å…³ç³»ã€‚")
        print(f"      å› æ­¤ï¼Œç»™å®ƒä»¬åŠ ä¸Šä½ç½®ç¼–ç æ˜¯ **æ²¡æœ‰æ„ä¹‰** çš„ï¼")
    
    return matchings


def test_position_encoding_vs_spatial_position():
    """
    æµ‹è¯•ä½ç½®ç¼–ç  vs ç©ºé—´ä½ç½®çš„åŒºåˆ«
    """
    print("\n" + "="*80)
    print("æµ‹è¯•3: ä½ç½®ç¼–ç  vs ç©ºé—´ä½ç½®åæ ‡")
    print("="*80)
    
    N = 1024
    d_model = 512
    
    # æŠ“å–çš„ç©ºé—´ä½ç½®ï¼ˆtranslation éƒ¨åˆ†ï¼‰
    spatial_positions = torch.randn(N, 3)  # [N, 3] çœŸå®çš„ xyz åæ ‡
    
    # Transformer çš„ä½ç½®ç¼–ç ï¼ˆå›ºå®šç´¢å¼•ï¼‰
    pos_indices = torch.arange(N)  # [0, 1, 2, ..., 1023]
    
    # è®¡ç®—ç©ºé—´è·ç¦»çŸ©é˜µ
    spatial_dist = torch.cdist(spatial_positions, spatial_positions)  # [N, N]
    
    # è®¡ç®—ç´¢å¼•è·ç¦»çŸ©é˜µ
    index_dist = (pos_indices.unsqueeze(0) - pos_indices.unsqueeze(1)).abs().float()  # [N, N]
    
    # ç›¸å…³æ€§åˆ†æ
    correlation = torch.corrcoef(torch.stack([
        spatial_dist.flatten(),
        index_dist.flatten()
    ]))[0, 1].item()
    
    print(f"\nç©ºé—´è·ç¦» vs ç´¢å¼•è·ç¦»çš„ç›¸å…³æ€§: {correlation:.4f}")
    
    if abs(correlation) < 0.3:
        print(f"\nç»“è®ºï¼šç©ºé—´ä½ç½®å’Œç´¢å¼•ä½ç½® **å‡ ä¹æ— å…³**ï¼")
        print(f"      - ç©ºé—´ä½ç½®: ç”± translation (x,y,z) è¡¨ç¤ºï¼Œå·²ç»åœ¨è¾“å…¥ä¸­")
        print(f"      - ç´¢å¼•ä½ç½®: [0,1,2,...] æ˜¯äººä¸ºçš„ï¼Œæ²¡æœ‰ç‰©ç†æ„ä¹‰")
        print(f"      å› æ­¤ï¼Œæ¨¡å‹åº”è¯¥ä¾èµ– translation ç‰¹å¾ï¼Œè€Œéç´¢å¼•ä½ç½®ç¼–ç ï¼")
    
    return correlation


def visualize_attention_pattern():
    """
    å¯è§†åŒ–æœ‰/æ— ä½ç½®ç¼–ç æ—¶çš„æ³¨æ„åŠ›æ¨¡å¼
    """
    print("\n" + "="*80)
    print("æµ‹è¯•4: æ³¨æ„åŠ›æ¨¡å¼åˆ†æï¼ˆç†è®ºåˆ†æï¼‰")
    print("="*80)
    
    print(f"\nå‡è®¾åœºæ™¯ï¼šå­¦ä¹ æŠ“å–å§¿æ€ä¹‹é—´çš„å…³ç³»")
    print(f"\n1. **æ— ä½ç½®ç¼–ç **ï¼š")
    print(f"   - æ³¨æ„åŠ›åŸºäºå†…å®¹ç›¸ä¼¼åº¦ï¼šAttention(Q, K) = softmax(QK^T / âˆšd)")
    print(f"   - ç›¸ä¼¼çš„æŠ“å–ï¼ˆä½ç½®è¿‘ã€å§¿æ€è¿‘ï¼‰ä¼šäº’ç›¸å…³æ³¨")
    print(f"   - ç¬¦åˆç‰©ç†ç›´è§‰ï¼šé™„è¿‘çš„æŠ“å–ç¡®å®ç›¸å…³")
    
    print(f"\n2. **æœ‰ä½ç½®ç¼–ç **ï¼š")
    print(f"   - æ³¨æ„åŠ›æ··åˆäº†å†…å®¹ + ç´¢å¼•ä½ç½®")
    print(f"   - ç´¢å¼•ç›¸è¿‘çš„æŠ“å–ä¼šè¢«å¼ºåˆ¶å…³è”")
    print(f"   - âŒ ä½†ç´¢å¼• [i, i+1] åœ¨ç©ºé—´ä¸Šå¯èƒ½è·ç¦»å¾ˆè¿œï¼")
    
    print(f"\nç»“è®ºï¼šå¯¹äºæŠ“å–å§¿æ€é›†åˆï¼Œä½ç½®ç¼–ç ä¼šå¼•å…¥ **é”™è¯¯çš„å½’çº³åç½®**ï¼")


def main():
    print("\n" + "="*80)
    print("ä½ç½®ç¼–ç å¿…è¦æ€§æµ‹è¯•")
    print("åœºæ™¯ï¼šFlow Matching ä¸­çš„æŠ“å–å§¿æ€é›†åˆå­¦ä¹ ")
    print("="*80)
    
    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    test_permutation_invariance_with_positional_encoding()
    test_sinkhorn_matching_randomness()
    test_position_encoding_vs_spatial_position()
    visualize_attention_pattern()
    
    # æœ€ç»ˆå»ºè®®
    print("\n" + "="*80)
    print("ğŸ¯ æœ€ç»ˆå»ºè®®")
    print("="*80)
    print(f"\nå¯¹äºä½ ä»¬çš„ DiT-FM æ¨¡å‹ï¼š")
    print(f"\nâœ… **æ¨èé…ç½®**ï¼š")
    print(f"   use_learnable_pos_embedding: false  ï¼ˆå½“å‰é…ç½®ï¼‰")
    print(f"\nâŒ **ä¸æ¨è**ï¼š")
    print(f"   use_learnable_pos_embedding: true")
    print(f"\nç†ç”±ï¼š")
    print(f"   1. æŠ“å–å§¿æ€æ˜¯æ— åºé›†åˆï¼Œä¸å­˜åœ¨å›ºå®šçš„é¡ºåºå…³ç³»")
    print(f"   2. SinkhornOT é…å¯¹åçš„ç´¢å¼•æ˜¯ä»»æ„çš„ï¼Œæ²¡æœ‰ç©ºé—´æ„ä¹‰")
    print(f"   3. ç©ºé—´å…³ç³»å·²ç»ç”± translation åæ ‡è¡¨ç¤ºï¼Œæ— éœ€é¢å¤–ç¼–ç ")
    print(f"   4. ä½ç½®ç¼–ç ä¼šç ´åç½®æ¢ä¸å˜æ€§ï¼Œå¼•å…¥é”™è¯¯çš„å½’çº³åç½®")
    print(f"\nå¦‚æœéœ€è¦ç©ºé—´ä¿¡æ¯ï¼š")
    print(f"   âœ… ä½¿ç”¨ geometric_attention_biasï¼ˆåŸºäºçœŸå®çš„ xyz è·ç¦»ï¼‰")
    print(f"   âŒ ä¸è¦ä½¿ç”¨ positional_embeddingï¼ˆåŸºäºäººä¸ºçš„ç´¢å¼•ï¼‰")
    print("="*80 + "\n")


if __name__ == "__main__":
    torch.manual_seed(42)
    main()

