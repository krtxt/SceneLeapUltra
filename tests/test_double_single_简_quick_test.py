"""
å¿«é€Ÿæµ‹è¯•åŒæµ+å•æµæ¶æ„çš„æ ¸å¿ƒç»„ä»¶
"""
import sys
import os
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import torch
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def test_dit_double_stream_block():
    """æµ‹è¯• DiTDoubleStreamBlock"""
    logger.info("="*60)
    logger.info("æµ‹è¯•1: DiTDoubleStreamBlock")
    logger.info("="*60)
    
    from models.decoder.dit import DiTDoubleStreamBlock
    
    d_model = 256
    num_heads = 8
    d_head = 32
    batch_size = 2
    num_grasps = 10
    num_scene = 128
    cond_dim = 512
    
    block = DiTDoubleStreamBlock(
        d_model=d_model,
        num_heads=num_heads,
        d_head=d_head,
        dropout=0.1,
        cond_dim=cond_dim,
        chunk_size=512,
        use_flash_attention=False,
        attention_dropout=0.0,
    )
    
    grasp_tokens = torch.randn(batch_size, num_grasps, d_model)
    scene_tokens = torch.randn(batch_size, num_scene, d_model)
    cond_vector = torch.randn(batch_size, cond_dim)
    scene_mask = torch.ones(batch_size, num_scene)
    
    with torch.no_grad():
        grasp_out, scene_out = block(grasp_tokens, scene_tokens, cond_vector, scene_mask)
    
    assert grasp_out.shape == grasp_tokens.shape
    assert scene_out.shape == scene_tokens.shape
    assert torch.isfinite(grasp_out).all()
    assert torch.isfinite(scene_out).all()
    
    logger.info(f"âœ“ è¾“å…¥å½¢çŠ¶: grasp={tuple(grasp_tokens.shape)}, scene={tuple(scene_tokens.shape)}")
    logger.info(f"âœ“ è¾“å‡ºå½¢çŠ¶: grasp={tuple(grasp_out.shape)}, scene={tuple(scene_out.shape)}")
    logger.info(f"âœ“ æ•°å€¼ç¨³å®šæ€§: grasp range=[{grasp_out.min():.4f}, {grasp_out.max():.4f}]")
    logger.info("")
    return True


def test_parallel_single_stream_block():
    """æµ‹è¯• ParallelSingleStreamBlock"""
    logger.info("="*60)
    logger.info("æµ‹è¯•2: ParallelSingleStreamBlock")
    logger.info("="*60)
    
    from models.decoder.dit_single_stream_parallel import ParallelSingleStreamBlock
    
    d_model = 256
    num_heads = 8
    d_head = 32
    batch_size = 2
    seq_len = 138  # 128 scene + 10 grasp
    cond_dim = 512
    
    block = ParallelSingleStreamBlock(
        d_model=d_model,
        num_heads=num_heads,
        d_head=d_head,
        mlp_ratio=4.0,
        dropout=0.1,
        cond_dim=cond_dim,
        use_flash_attention=False,
    )
    
    x = torch.randn(batch_size, seq_len, d_model)
    cond_vector = torch.randn(batch_size, cond_dim)
    mask = torch.ones(batch_size, seq_len)
    
    with torch.no_grad():
        output = block(x, cond_vector, mask)
    
    assert output.shape == x.shape
    assert torch.isfinite(output).all()
    
    logger.info(f"âœ“ è¾“å…¥å½¢çŠ¶: {tuple(x.shape)}")
    logger.info(f"âœ“ è¾“å‡ºå½¢çŠ¶: {tuple(output.shape)}")
    logger.info(f"âœ“ æ•°å€¼ç¨³å®šæ€§: range=[{output.min():.4f}, {output.max():.4f}]")
    logger.info("")
    return True


def test_concat_split_logic():
    """æµ‹è¯•æ‹¼æ¥å’Œåˆ†ç¦»é€»è¾‘"""
    logger.info("="*60)
    logger.info("æµ‹è¯•3: æ‹¼æ¥å’Œåˆ†ç¦»é€»è¾‘")
    logger.info("="*60)
    
    batch_size = 2
    num_scene = 128
    num_grasp = 10
    d_model = 256
    
    scene_tokens = torch.randn(batch_size, num_scene, d_model)
    grasp_tokens = torch.randn(batch_size, num_grasp, d_model)
    
    # æ‹¼æ¥ï¼ˆHunyuan3D-DiT é¡ºåºï¼šscene + graspï¼‰
    combined = torch.cat([scene_tokens, grasp_tokens], dim=1)
    logger.info(f"âœ“ æ‹¼æ¥å: {tuple(combined.shape)}")
    
    assert combined.shape == (batch_size, num_scene + num_grasp, d_model)
    
    # åˆ†ç¦»
    extracted_grasp = combined[:, num_scene:, ...]
    logger.info(f"âœ“ åˆ†ç¦»å‡ºgrasp: {tuple(extracted_grasp.shape)}")
    
    assert extracted_grasp.shape == grasp_tokens.shape
    assert torch.allclose(extracted_grasp, grasp_tokens)
    
    logger.info(f"âœ“ æ‹¼æ¥-åˆ†ç¦»éªŒè¯é€šè¿‡")
    logger.info("")
    return True


def test_qk_norm():
    """æµ‹è¯• QKNorm"""
    logger.info("="*60)
    logger.info("æµ‹è¯•4: QKNorm")
    logger.info("="*60)
    
    from models.decoder.dit_utils import QKNorm
    
    d_head = 32
    batch_size = 2
    num_heads = 8
    seq_len = 100
    
    qk_norm = QKNorm(d_head)
    
    q = torch.randn(batch_size, num_heads, seq_len, d_head)
    k = torch.randn(batch_size, num_heads, seq_len, d_head)
    v = torch.randn(batch_size, num_heads, seq_len, d_head)
    
    q_norm, k_norm = qk_norm(q, k, v)
    
    assert q_norm.shape == q.shape
    assert k_norm.shape == k.shape
    assert torch.isfinite(q_norm).all()
    assert torch.isfinite(k_norm).all()
    
    logger.info(f"âœ“ QKNorm è¾“å…¥å½¢çŠ¶: {tuple(q.shape)}")
    logger.info(f"âœ“ QKNorm è¾“å‡ºå½¢çŠ¶: {tuple(q_norm.shape)}")
    logger.info(f"âœ“ å½’ä¸€åŒ–æ•ˆæœ: ||q||={q.norm(dim=-1).mean():.4f} -> ||q_norm||={q_norm.norm(dim=-1).mean():.4f}")
    logger.info("")
    return True


def run_all_tests():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    logger.info("\n" + "="*60)
    logger.info("åŒæµ+å•æµ DiT æ ¸å¿ƒç»„ä»¶å¿«é€Ÿæµ‹è¯•")
    logger.info("="*60 + "\n")
    
    tests = [
        test_dit_double_stream_block,
        test_parallel_single_stream_block,
        test_concat_split_logic,
        test_qk_norm,
    ]
    
    passed = 0
    failed = 0
    
    for test_func in tests:
        try:
            test_func()
            passed += 1
        except Exception as e:
            logger.error(f"âœ— {test_func.__name__} å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            failed += 1
    
    logger.info("="*60)
    logger.info(f"æµ‹è¯•å®Œæˆ: {passed} é€šè¿‡, {failed} å¤±è´¥")
    logger.info("="*60)
    
    if failed == 0:
        logger.info("\nğŸ‰ æ‰€æœ‰æ ¸å¿ƒç»„ä»¶æµ‹è¯•é€šè¿‡ï¼åŒæµ+å•æµæ¶æ„ä¿®å¤æˆåŠŸã€‚")
    
    return failed == 0


if __name__ == "__main__":
    import sys
    success = run_all_tests()
    sys.exit(0 if success else 1)

