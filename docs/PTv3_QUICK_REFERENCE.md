# PTv3 Quick Reference Card

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. éªŒè¯é›†æˆ
```bash
python tests/verify_ptv3_integration.py
```

### 2. è¿è¡Œæµ‹è¯•
```bash
# åŸºç¡€æµ‹è¯•
python tests/test_ptv3_backbone_basic.py

# DiT å†’çƒŸæµ‹è¯•
python tests/test_smoke_dit_ptv3.py

# UNet å†’çƒŸæµ‹è¯•
python tests/test_smoke_unet_ptv3.py
```

### 3. å¼€å§‹è®­ç»ƒ
```bash
# DiT + PTv3 Light (æ¨è)
python train_lightning.py \
    model/diffuser/decoder=dit \
    model/diffuser/decoder/backbone=ptv3_light

# UNet + PTv3
python train_lightning.py \
    model/diffuser/decoder=unet \
    model/diffuser/decoder/backbone=ptv3
```

## ğŸ“Š é…ç½®é€‰æ‹©

| é…ç½® | å‚æ•°é‡ | è¾“å‡ºç»´åº¦ | é€Ÿåº¦ | Flash Attn | æ¨èåœºæ™¯ |
|------|--------|---------|------|------------|----------|
| `ptv3_light` | 8-12M | 512 | ä¸­é€Ÿ | âœ… | **æ¨èé¦–é€‰** |
| `ptv3` | 46M | 512 | æ…¢ | âœ… | é«˜ç²¾åº¦éœ€æ±‚ |
| `ptv3_no_flash` | 46M | 512 | æ›´æ…¢ | âŒ | å…¼å®¹æ€§éœ€æ±‚ |

## ğŸ”§ é…ç½®æ–‡ä»¶è·¯å¾„

```
config/model/diffuser/decoder/backbone/
â”œâ”€â”€ ptv3_light.yaml       # è½»é‡çº§ï¼ˆæ¨èï¼‰
â”œâ”€â”€ ptv3.yaml            # æ ‡å‡†ç‰ˆ
â””â”€â”€ ptv3_no_flash.yaml   # æ— Flashç‰ˆ
```

## ğŸ“¦ ä¾èµ–å®‰è£…

### å¿…éœ€
```bash
pip install spconv-cu118  # æ›¿æ¢ä¸ºä½ çš„CUDAç‰ˆæœ¬
pip install torch-scatter
pip install addict
```

### å¯é€‰ï¼ˆFlash Attentionï¼‰
```bash
pip install flash-attn
```

## ğŸ¯ æ”¯æŒçš„è¾“å…¥æ ¼å¼

| use_rgb | use_object_mask | è¾“å…¥å½¢çŠ¶ | è¯´æ˜ |
|---------|----------------|---------|------|
| âŒ | âŒ | (B,N,3) | ä»… xyz |
| âœ… | âŒ | (B,N,6) | xyz + rgb |
| âŒ | âœ… | (B,N,4) | xyz + mask |
| âœ… | âœ… | (B,N,7) | xyz + rgb + mask |

## ğŸ› å¸¸è§é—®é¢˜

### ImportError: No module named 'flash_attn'
**è§£å†³**: ä½¿ç”¨ `ptv3_no_flash` æˆ–å®‰è£… flash-attn
```bash
pip install flash-attn
```

### CUDA out of memory
**è§£å†³**:
1. ä½¿ç”¨ `ptv3_light` è€Œé `ptv3`
2. å‡å° batch size
3. å‡å°‘ç‚¹äº‘æ•°é‡

### spconv import error
**è§£å†³**: å®‰è£…å¯¹åº”CUDAç‰ˆæœ¬çš„spconv
```bash
# CUDA 11.8
pip install spconv-cu118

# CUDA 11.7
pip install spconv-cu117
```

## ğŸ“ é…ç½®è¦†ç›–ç¤ºä¾‹

### å‘½ä»¤è¡Œè¦†ç›–
```bash
# åˆ‡æ¢backbone
python train_lightning.py \
    model/diffuser/decoder/backbone=ptv3_light

# è°ƒæ•´grid_size
python train_lightning.py \
    model/diffuser/decoder/backbone=ptv3_light \
    model/diffuser/decoder/backbone.grid_size=0.05
```

### é…ç½®æ–‡ä»¶è¦†ç›–
```yaml
# config/experiment/my_experiment.yaml
defaults:
  - /model/diffuser/decoder: dit
  - /model/diffuser/decoder/backbone: ptv3_light

model:
  diffuser:
    decoder:
      backbone:
        grid_size: 0.05  # è¦†ç›–é»˜è®¤å€¼
```

## ğŸ” æ€§èƒ½ç›‘æ§

### æ£€æŸ¥æ˜¾å­˜å ç”¨
```python
import torch
print(f"Allocated: {torch.cuda.memory_allocated()/1e9:.2f} GB")
print(f"Cached: {torch.cuda.memory_reserved()/1e9:.2f} GB")
```

### åˆ†ææ¨ç†é€Ÿåº¦
```python
import time
import torch

model.eval()
with torch.no_grad():
    start = time.time()
    output = model(x_t, ts, data)
    torch.cuda.synchronize()
    elapsed = time.time() - start
    print(f"Inference time: {elapsed*1000:.2f} ms")
```

## ğŸ“š ç›¸å…³æ–‡æ¡£

- è¯¦ç»†æ–‡æ¡£: `models/backbone/PTv3_INTEGRATION.md`
- é›†æˆæ€»ç»“: `PTv3_INTEGRATION_SUMMARY.md`
- å˜æ›´æ—¥å¿—: `CHANGELOG_PTv3.md`
- Backboneé…ç½®: `config/model/diffuser/decoder/backbone/README.md`

## ğŸ“ å¼•ç”¨

å¦‚ä½¿ç”¨PTv3ï¼Œè¯·å¼•ç”¨ï¼š
```bibtex
@article{wu2023point,
  title={Point transformer v3: Simpler, faster, stronger},
  author={Wu, Xiaoyang and others},
  journal={arXiv preprint arXiv:2312.10035},
  year={2023}
}
```

## ğŸ’¡ æœ€ä½³å®è·µ

1. **é¦–æ¬¡ä½¿ç”¨**: å…ˆè¿è¡Œ `verify_ptv3_integration.py` éªŒè¯ç¯å¢ƒ
2. **é€‰æ‹©é…ç½®**: ä¼˜å…ˆä½¿ç”¨ `ptv3_light`ï¼ˆå¹³è¡¡æ€§èƒ½ä¸ç²¾åº¦ï¼‰
3. **æ˜¾å­˜ä¸è¶³**: å‡å° batch size æˆ–ä½¿ç”¨ `ptv3_light`
4. **æ— Flash Attn**: ä½¿ç”¨ `ptv3_no_flash`
5. **è°ƒè¯•**: ä½¿ç”¨å°æ•°æ®é›†å’Œå°æ¨¡å‹å…ˆéªŒè¯æµç¨‹

## âš™ï¸ é«˜çº§é…ç½®

### è‡ªå®šä¹‰grid_size
```yaml
# æ›´ç»†çš„ä½“ç´ åŒ–ï¼ˆæ›´å¤šç‚¹ï¼Œæ›´æ…¢ï¼‰
backbone:
  grid_size: 0.01

# æ›´ç²—çš„ä½“ç´ åŒ–ï¼ˆæ›´å°‘ç‚¹ï¼Œæ›´å¿«ï¼‰
backbone:
  grid_size: 0.05
```

### è°ƒæ•´encoderæ·±åº¦
```yaml
backbone:
  encoder_depths: [1, 1, 1, 1, 1]  # æ›´æµ…ï¼ˆæ›´å¿«ï¼‰
  # æˆ–
  encoder_depths: [2, 2, 3, 3, 2]  # æ›´æ·±ï¼ˆæ›´å¥½ï¼‰
```

## ğŸ“ æ”¯æŒ

- é—®é¢˜åé¦ˆ: GitHub Issues
- æ–‡æ¡£: é¡¹ç›® `docs/` ç›®å½•
- æµ‹è¯•: é¡¹ç›® `tests/` ç›®å½•

