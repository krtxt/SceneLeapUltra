# Hunyuan3DDiT æ¨¡å‹æ¶æ„è¯¦è§£

æœ¬æ–‡æ¡£è¯¦ç»†åˆ†æ `bin/hunyuan3ddit.py` ä¸­å®ç°çš„ Hunyuan3DDiT æ¨¡å‹çš„æ¶æ„ã€æ•°æ®æµå˜åŒ–å’Œå„ä¸ªæ¨¡å—çš„å·¥ä½œåŸç†ã€‚

## ç›®å½•

- [1. æ¨¡å‹æ€»è§ˆ](#1-æ¨¡å‹æ€»è§ˆ)
- [2. å®Œæ•´æ•°æ®æµç¨‹å›¾](#2-å®Œæ•´æ•°æ®æµç¨‹å›¾)
- [3. DoubleStreamBlock è¯¦è§£](#3-doublestreamblock-è¯¦è§£)
- [4. SingleStreamBlock è¯¦è§£](#4-singlestreamblock-è¯¦è§£)
- [5. Attention æœºåˆ¶è¯¦è§£](#5-attention-æœºåˆ¶è¯¦è§£)
- [6. ä¸ EfficientAttention çš„å¯¹æ¯”](#6-ä¸-efficientattention-çš„å¯¹æ¯”)

---

## 1. æ¨¡å‹æ€»è§ˆ

### 1.1 åŸºæœ¬ä¿¡æ¯

**Hunyuan3DDiT** æ˜¯åŸºäº Diffusion Transformer (DiT) æ¶æ„çš„3Dæ‰©æ•£æ¨¡å‹ï¼Œä¸»è¦ç‰¹ç‚¹ï¼š

- **æ¶æ„ç±»å‹**: DiT (Diffusion Transformer)
- **æ·±åº¦**: 16å±‚åŒæµå— + 32å±‚å•æµå—
- **éšè—ç»´åº¦**: 1024
- **æ³¨æ„åŠ›å¤´æ•°**: 16
- **å¤´ç»´åº¦**: 64 (1024/16)
- **è°ƒåˆ¶æ–¹å¼**: AdaLN-Zero

### 1.2 æ¨¡å‹å‚æ•°

```python
Hunyuan3DDiT(
    in_channels=64,           # è¾“å…¥æ½œåœ¨è¡¨ç¤ºé€šé“æ•°
    context_in_dim=1536,      # æ–‡æœ¬æ¡ä»¶ç»´åº¦
    hidden_size=1024,         # éšè—å±‚ç»´åº¦
    mlp_ratio=4.0,           # MLPæ‰©å±•æ¯”ä¾‹
    num_heads=16,            # æ³¨æ„åŠ›å¤´æ•°
    depth=16,                # DoubleStreamBlockå±‚æ•°
    depth_single_blocks=32,  # SingleStreamBlockå±‚æ•°
    axes_dim=[64],           # ä½ç½®ç¼–ç ç»´åº¦
    theta=10_000,            # ä½ç½®ç¼–ç é¢‘ç‡åŸºæ•°
    qkv_bias=True,           # QKVæŠ•å½±æ˜¯å¦ä½¿ç”¨åç½®
    time_factor=1000,        # æ—¶é—´æ­¥åµŒå…¥ç¼©æ”¾å› å­
    guidance_embed=False,    # æ˜¯å¦ä½¿ç”¨guidanceåµŒå…¥
)
```

### 1.3 å…³é”®ç»„ä»¶

| ç»„ä»¶ | åŠŸèƒ½ | è¾“å…¥ â†’ è¾“å‡º |
|------|------|-----------|
| `latent_in` | æ½œåœ¨è¡¨ç¤ºæŠ•å½± | `[B, L, 64]` â†’ `[B, L, 1024]` |
| `time_in` | æ—¶é—´æ­¥åµŒå…¥ | `[B, 256]` â†’ `[B, 1024]` |
| `cond_in` | æ–‡æœ¬æ¡ä»¶æŠ•å½± | `[B, L_txt, 1536]` â†’ `[B, L_txt, 1024]` |
| `guidance_in` | GuidanceåµŒå…¥ï¼ˆå¯é€‰ï¼‰ | `[B, 256]` â†’ `[B, 1024]` |
| `double_blocks` | åŒæµå¤„ç†å— Ã— 16 | å›¾åƒæµ + æ–‡æœ¬æµå¹¶è¡Œå¤„ç† |
| `single_blocks` | å•æµå¤„ç†å— Ã— 32 | åˆå¹¶åçš„ç»Ÿä¸€å¤„ç† |
| `final_layer` | è¾“å‡ºå±‚ | `[B, L, 1024]` â†’ `[B, L, 64]` |

---

## 2. å®Œæ•´æ•°æ®æµç¨‹å›¾

### 2.1 æ€»ä½“æ¶æ„æµç¨‹å›¾

```mermaid
graph TB
    subgraph "è¾“å…¥å±‚"
        INPUT["è¾“å…¥æ•°æ®<br/>x: [B, L, 64]<br/>t: [B]<br/>contexts['main']: [B, L_txt, 1536]<br/>guidance (å¯é€‰): [B]"]
    end

    subgraph "åµŒå…¥å±‚å¤„ç†"
        LATENT_IN["latent_in<br/>Linear(64 â†’ 1024)"]
        TIME_EMB["timestep_embedding<br/>t â†’ [B, 256]<br/>æ­£å¼¦ä½™å¼¦ä½ç½®ç¼–ç "]
        TIME_IN["time_in: MLPEmbedder<br/>Linear(256 â†’ 1024) â†’ SiLU<br/>â†’ Linear(1024 â†’ 1024)"]
        COND_IN["cond_in<br/>Linear(1536 â†’ 1024)"]
        GUIDANCE_EMB["guidance_embedding<br/>(å¯é€‰)<br/>guidance â†’ [B, 256]"]
        GUIDANCE_IN["guidance_in: MLPEmbedder<br/>(å¯é€‰)<br/>Linear(256 â†’ 1024) â†’ SiLU<br/>â†’ Linear(1024 â†’ 1024)"]
        
        INPUT --> |"x: [B, L, 64]"| LATENT_IN
        INPUT --> |"t: [B]"| TIME_EMB
        TIME_EMB --> |"[B, 256]"| TIME_IN
        TIME_IN --> |"vec_time: [B, 1024]"| VEC_SUM
        INPUT --> |"contexts['main']<br/>[B, L_txt, 1536]"| COND_IN
        INPUT -.-> |"guidance: [B]<br/>(å¦‚æœguidance_embed=True)"| GUIDANCE_EMB
        GUIDANCE_EMB -.-> |"[B, 256]"| GUIDANCE_IN
        GUIDANCE_IN -.-> |"vec_guidance: [B, 1024]"| VEC_SUM
        
        VEC_SUM["vecç›¸åŠ <br/>vec = vec_time + vec_guidance<br/>[B, 1024]"]
        
        LATENT_IN --> |"latent: [B, L, 1024]"| DOUBLE_STAGE
        COND_IN --> |"cond: [B, L_txt, 1024]"| DOUBLE_STAGE
        VEC_SUM --> |"vec: [B, 1024]"| DOUBLE_STAGE
    end

    subgraph "Double Stream é˜¶æ®µ"
        DOUBLE_STAGE["DoubleStreamBlocks Ã— 16<br/>å›¾åƒæµå’Œæ–‡æœ¬æµå¹¶è¡Œå¤„ç†<br/>é€šè¿‡è”åˆæ³¨æ„åŠ›äº¤äº’"]
    end

    subgraph "Single Stream é˜¶æ®µ"
        CONCAT["æ‹¼æ¥<br/>latent = cat(cond, latent, dim=1)<br/>[B, L_txt+L, 1024]"]
        SINGLE_STAGE["SingleStreamBlocks Ã— 32<br/>ç»Ÿä¸€å¤„ç†æ‰€æœ‰ç‰¹å¾"]
        EXTRACT["æå–å›¾åƒç‰¹å¾<br/>latent = latent[:, L_txt:, ...]<br/>[B, L, 1024]"]
    end

    subgraph "è¾“å‡ºå±‚"
        FINAL_LAYER["LastLayer<br/>AdaLNè°ƒåˆ¶ + çº¿æ€§æŠ•å½±"]
        OUTPUT["è¾“å‡º<br/>latent: [B, L, 64]"]
    end

    DOUBLE_STAGE --> |"latent: [B, L, 1024]<br/>cond: [B, L_txt, 1024]"| CONCAT
    CONCAT --> SINGLE_STAGE
    SINGLE_STAGE --> EXTRACT
    EXTRACT --> |"latent: [B, L, 1024]"| FINAL_LAYER
    VEC_SUM --> |"vec: [B, 1024]"| FINAL_LAYER
    FINAL_LAYER --> OUTPUT

    style INPUT fill:#e1f5ff
    style OUTPUT fill:#ffe1f5
    style DOUBLE_STAGE fill:#fff4e1
    style SINGLE_STAGE fill:#e1ffe1
    style FINAL_LAYER fill:#f5e1ff
```

### 2.2 æ•°æ®ç»´åº¦å˜åŒ–æ€»ç»“

| é˜¶æ®µ | è¾“å…¥ç»´åº¦ | è¾“å‡ºç»´åº¦ | è¯´æ˜ |
|------|---------|---------|------|
| **è¾“å…¥** | `x: [B, L, 64]` | - | åŸå§‹æ½œåœ¨è¡¨ç¤º |
| **åµŒå…¥å±‚** | `x: [B, L, 64]` | `latent: [B, L, 1024]` | æŠ•å½±åˆ°éšè—ç»´åº¦ |
| | `t: [B]` | `vec: [B, 1024]` | æ—¶é—´æ­¥åµŒå…¥ |
| | `contexts: [B, L_txt, 1536]` | `cond: [B, L_txt, 1024]` | æ–‡æœ¬æ¡ä»¶æŠ•å½± |
| **DoubleStream** | `img: [B, L, 1024]`<br/>`txt: [B, L_txt, 1024]` | `img: [B, L, 1024]`<br/>`txt: [B, L_txt, 1024]` | åŒæµå¹¶è¡Œå¤„ç† Ã— 16å±‚ |
| **æ‹¼æ¥** | ä¸¤ä¸ªç‹¬ç«‹æµ | `[B, L_txt+L, 1024]` | åˆå¹¶æ–‡æœ¬å’Œå›¾åƒ |
| **SingleStream** | `[B, L_txt+L, 1024]` | `[B, L_txt+L, 1024]` | å•æµå¤„ç† Ã— 32å±‚ |
| **æå–** | `[B, L_txt+L, 1024]` | `[B, L, 1024]` | åªä¿ç•™å›¾åƒéƒ¨åˆ† |
| **è¾“å‡ºå±‚** | `[B, L, 1024]` | `[B, L, 64]` | æŠ•å½±å›åŸå§‹ç»´åº¦ |

---

## 3. DoubleStreamBlock è¯¦è§£

### 3.1 æ•´ä½“æ¶æ„

DoubleStreamBlock æ˜¯**åŒæµå¤„ç†**æ¨¡å—ï¼ŒåŒæ—¶å¤„ç†å›¾åƒç‰¹å¾æµå’Œæ–‡æœ¬ç‰¹å¾æµï¼Œå¹¶é€šè¿‡**è”åˆæ³¨æ„åŠ›**å®ç°è·¨æ¨¡æ€äº¤äº’ã€‚

**åˆå§‹åŒ–å‚æ•°**:
```python
DoubleStreamBlock(
    hidden_size=1024,      # éšè—å±‚ç»´åº¦
    num_heads=16,          # æ³¨æ„åŠ›å¤´æ•°
    mlp_ratio=4.0,         # MLPæ‰©å±•æ¯”ä¾‹ â†’ mlp_hidden_dim=4096
    qkv_bias=False         # QKVæŠ•å½±æ˜¯å¦ä½¿ç”¨åç½®
)
```

**è¾“å…¥æ•°æ®**:
- `img`: `[B, L, 1024]` - å›¾åƒæ½œåœ¨ç‰¹å¾
- `txt`: `[B, L_txt, 1024]` - æ–‡æœ¬æ¡ä»¶ç‰¹å¾  
- `vec`: `[B, 1024]` - æ—¶é—´æ­¥åµŒå…¥
- `pe`: Position Embeddingï¼ˆä»£ç ä¸­ä¸ºNoneï¼‰

### 3.2 è¯¦ç»†æ•°æ®æµç¨‹

#### é˜¶æ®µ1: Modulationï¼ˆè‡ªé€‚åº”è°ƒåˆ¶ï¼‰

```python
img_mod1, img_mod2 = self.img_mod(vec)  
txt_mod1, txt_mod2 = self.txt_mod(vec)
```

**Image Modulation è¿‡ç¨‹**:
```
è¾“å…¥: vec [B, 1024]
  â†“
SiLUæ¿€æ´»
  â†“  
Linear(1024 â†’ 6144)
  â†“
[:, None, :] æ‰©å±•ç»´åº¦ â†’ [B, 1, 6144]
  â†“
chunk(6, dim=-1) åˆ†æˆ6ä»½
  â†“
è¾“å‡º: img_mod1 (shift, scale, gate)  [B, 1, 1024] Ã— 3
      img_mod2 (shift, scale, gate)  [B, 1, 1024] Ã— 3
```

Text Modulation åŒæ ·çš„å¤„ç†ã€‚

**ä½œç”¨**: ä½¿ç”¨æ—¶é—´æ­¥ä¿¡æ¯ç”Ÿæˆè°ƒåˆ¶å‚æ•°ï¼Œæ§åˆ¶å½’ä¸€åŒ–å±‚çš„scaleå’Œshiftï¼Œä»¥åŠæœ€ç»ˆè¾“å‡ºçš„gateã€‚

#### é˜¶æ®µ2: Imageæµ - æ³¨æ„åŠ›å‡†å¤‡

**2.1 å½’ä¸€åŒ–ä¸è°ƒåˆ¶**:
```python
img_modulated = self.img_norm1(img)
img_modulated = (1 + img_mod1.scale) * img_modulated + img_mod1.shift
```

```
img: [B, L, 1024]
  â†“
LayerNorm (æ— ä»¿å°„å‚æ•°)
  â†“ [B, L, 1024]
è°ƒåˆ¶: (1 + scale[B,1,1024]) * norm + shift[B,1,1024]
  â†“
img_modulated: [B, L, 1024]
```

**2.2 QKVæŠ•å½±**:
```python
img_qkv = self.img_attn.qkv(img_modulated)
img_q, img_k, img_v = rearrange(img_qkv, "B L (K H D) -> K B H L D", K=3, H=16)
```

```
img_modulated: [B, L, 1024]
  â†“
Linear(1024 â†’ 3072)  # 3 * 1024
  â†“
img_qkv: [B, L, 3072]
  â†“
rearrange é‡æ’åˆ—
  â†“
[3, B, 16, L, 64]
  â†“
åˆ†è§£ä¸º3ä¸ªå¼ é‡:
img_q: [B, 16, L, 64]  # Query
img_k: [B, 16, L, 64]  # Key  
img_v: [B, 16, L, 64]  # Value
```

**2.3 QKå½’ä¸€åŒ–**:
```python
img_q, img_k = self.img_attn.norm(img_q, img_k, img_v)
```

ä½¿ç”¨ **RMSNorm** åœ¨æœ€åä¸€ç»´ï¼ˆdim=64ï¼‰å½’ä¸€åŒ–ï¼š
```
rrms = rsqrt(mean(q^2, dim=-1, keepdim=True) + 1e-6)
normalized = q * rrms * learnable_scale
```

**ä½œç”¨**: ç¨³å®šè®­ç»ƒï¼Œé˜²æ­¢æ³¨æ„åŠ›åˆ†æ•°è¿‡å¤§ã€‚

#### é˜¶æ®µ3: Textæµ - æ³¨æ„åŠ›å‡†å¤‡

```python
txt_modulated = self.txt_norm1(txt)
txt_modulated = (1 + txt_mod1.scale) * txt_modulated + txt_mod1.shift
txt_qkv = self.txt_attn.qkv(txt_modulated)
txt_q, txt_k, txt_v = rearrange(txt_qkv, "B L (K H D) -> K B H L D", K=3, H=16)
txt_q, txt_k = self.txt_attn.norm(txt_q, txt_k, txt_v)
```

ä¸Imageæµå®Œå…¨ç›¸åŒçš„å¤„ç†æµç¨‹ï¼š
```
txt: [B, L_txt, 1024]
  â†’ å½’ä¸€åŒ–è°ƒåˆ¶ â†’ [B, L_txt, 1024]
  â†’ QKVæŠ•å½± â†’ [B, L_txt, 3072]
  â†’ é‡æ’åˆ— â†’ txt_q, txt_k, txt_v: [B, 16, L_txt, 64]
  â†’ QKå½’ä¸€åŒ– â†’ txt_q, txt_k: [B, 16, L_txt, 64]
```

#### é˜¶æ®µ4: è”åˆæ³¨æ„åŠ›ï¼ˆæ ¸å¿ƒäº¤äº’æœºåˆ¶ï¼‰

```python
q = torch.cat((txt_q, img_q), dim=2)
k = torch.cat((txt_k, img_k), dim=2)
v = torch.cat((txt_v, img_v), dim=2)

attn = attention(q, k, v, pe=pe)
txt_attn, img_attn = attn[:, :txt.shape[1]], attn[:, txt.shape[1]:]
```

**4.1 æ‹¼æ¥QKV**:
```
txt_q: [B, 16, L_txt, 64]
img_q: [B, 16, L,     64]
  â†“ cat(dim=2)
q: [B, 16, L_txt+L, 64]

åŒç†:
k: [B, 16, L_txt+L, 64]
v: [B, 16, L_txt+L, 64]
```

**4.2 æ³¨æ„åŠ›è®¡ç®—**:
```
q, k, v: [B, 16, L_txt+L, 64]
  â†“
scaled_dot_product_attention
  scores = softmax(q @ k^T / sqrt(64))
  output = scores @ v
  â†“
[B, 16, L_txt+L, 64]
  â†“
rearrange "B H L D -> B L (H D)"
  â†“
attn: [B, L_txt+L, 1024]
```

**å…³é”®ç‚¹**: 
- **æ–‡æœ¬tokenå¯ä»¥attendåˆ°æ‰€æœ‰å›¾åƒtoken**
- **å›¾åƒtokenå¯ä»¥attendåˆ°æ‰€æœ‰æ–‡æœ¬token**
- è¿™å®ç°äº†è·¨æ¨¡æ€çš„ä¿¡æ¯äº¤äº’ï¼

**æ³¨æ„åŠ›çŸ©é˜µå¯è§†åŒ–**:
```
              txt_1  txt_2  ... txt_Ltxt | img_1  img_2  ... img_L
         +----------------------------------------------------+
    txt_1|    0.3    0.2   ...   0.1    |  0.05   0.05  ... 0.05  |
    txt_2|    0.2    0.35  ...   0.15   |  0.03   0.04  ... 0.03  |
      ...|    ...    ...   ...   ...    |  ...    ...   ... ...   |
txt_Ltxt |    0.1    0.15  ...   0.4    |  0.02   0.03  ... 0.02  |
         +----------------------------------------------------+
    img_1|    0.1    0.08  ...   0.05   |  0.3    0.15  ... 0.1   |
    img_2|    0.08   0.12  ...   0.04   |  0.2    0.35  ... 0.08  |
      ...|    ...    ...   ...   ...    |  ...    ...   ... ...   |
    img_L|    0.05   0.06  ...   0.03   |  0.15   0.1   ... 0.4   |
         +----------------------------------------------------+
```

**4.3 åˆ†å‰²æ³¨æ„åŠ›è¾“å‡º**:
```python
txt_attn = attn[:, :txt.shape[1]]       # [B, L_txt, 1024]
img_attn = attn[:, txt.shape[1]:]       # [B, L, 1024]
```

#### é˜¶æ®µ5: Imageæµ - ç¬¬ä¸€æ¬¡æ®‹å·®æ›´æ–°

```python
img = img + img_mod1.gate * self.img_attn.proj(img_attn)
```

```
img_attn: [B, L, 1024]
  â†“
img_attn.proj: Linear(1024 â†’ 1024)
  â†“ [B, L, 1024]
ä¹˜ä»¥é—¨æ§: img_mod1.gate [B, 1, 1024] (å¹¿æ’­)
  â†“ [B, L, 1024]
æ®‹å·®è¿æ¥: img = img + gated_output
  â†“
img: [B, L, 1024] (æ›´æ–°å)
```

**ä½œç”¨**: 
- `gate`æ§åˆ¶æ³¨æ„åŠ›è¾“å‡ºå¯¹åŸå§‹ç‰¹å¾çš„å½±å“ç¨‹åº¦
- AdaLN-Zeroåˆå§‹åŒ–gateæ¥è¿‘0ï¼Œè®©æ¨¡å‹ä»æ’ç­‰æ˜ å°„å¼€å§‹å­¦ä¹ 

#### é˜¶æ®µ6: Imageæµ - MLPå¤„ç†

```python
img = img + img_mod2.gate * self.img_mlp(
    (1 + img_mod2.scale) * self.img_norm2(img) + img_mod2.shift
)
```

**6.1 å½’ä¸€åŒ–ä¸è°ƒåˆ¶**:
```
img: [B, L, 1024]
  â†“
LayerNorm (æ— ä»¿å°„å‚æ•°)
  â†“ [B, L, 1024]
è°ƒåˆ¶: (1 + img_mod2.scale) * norm + img_mod2.shift
  â†“ [B, L, 1024]
```

**6.2 MLPå‰é¦ˆç½‘ç»œ**:
```python
self.img_mlp = nn.Sequential(
    nn.Linear(1024, 4096),    # æ‰©å±•
    GELU(approximate="tanh"),  # æ¿€æ´»
    nn.Linear(4096, 1024),    # å‹ç¼©
)
```

```
è°ƒåˆ¶å: [B, L, 1024]
  â†“
Linear(1024 â†’ 4096)
  â†“ [B, L, 4096]
GELUæ¿€æ´»
  â†“ [B, L, 4096]
Linear(4096 â†’ 1024)
  â†“ [B, L, 1024]
```

**6.3 é—¨æ§ä¸æ®‹å·®**:
```
mlpè¾“å‡º: [B, L, 1024]
  â†“
ä¹˜ä»¥é—¨æ§: img_mod2.gate [B, 1, 1024]
  â†“
æ®‹å·®è¿æ¥: img = img + gated_mlp_output
  â†“
img: [B, L, 1024] (æœ€ç»ˆè¾“å‡º)
```

#### é˜¶æ®µ7: Textæµ - æ®‹å·®æ›´æ–°

```python
txt = txt + txt_mod1.gate * self.txt_attn.proj(txt_attn)
txt = txt + txt_mod2.gate * self.txt_mlp(
    (1 + txt_mod2.scale) * self.txt_norm2(txt) + txt_mod2.shift
)
```

å¤„ç†æµç¨‹ä¸Imageæµå®Œå…¨ä¸€è‡´ã€‚

### 3.3 å®Œæ•´æ•°æ®æµæ€»ç»“

```
è¾“å…¥:
  img: [B, L, 1024]
  txt: [B, L_txt, 1024]
  vec: [B, 1024]

Modulationç”Ÿæˆ:
  img_mod1, img_mod2: å„3ä¸ª[B,1,1024] (shift,scale,gate)
  txt_mod1, txt_mod2: å„3ä¸ª[B,1,1024]

Imageåˆ†æ”¯å‡†å¤‡:
  img â†’ norm â†’ modulate â†’ qkv â†’ [B,16,L,64] Ã— 3 â†’ qk_norm

Textåˆ†æ”¯å‡†å¤‡:
  txt â†’ norm â†’ modulate â†’ qkv â†’ [B,16,L_txt,64] Ã— 3 â†’ qk_norm

è”åˆæ³¨æ„åŠ›:
  cat(txt_qkv, img_qkv) â†’ [B,16,L_txt+L,64]
  â†’ attention â†’ [B,L_txt+L,1024]
  â†’ split â†’ txt_attn[B,L_txt,1024], img_attn[B,L,1024]

Imageæ›´æ–°:
  img = img + gate1 * proj(img_attn)  # æ³¨æ„åŠ›æ®‹å·®
  img = img + gate2 * mlp(...)         # MLPæ®‹å·®

Textæ›´æ–°:
  txt = txt + gate1 * proj(txt_attn)  # æ³¨æ„åŠ›æ®‹å·®
  txt = txt + gate2 * mlp(...)         # MLPæ®‹å·®

è¾“å‡º:
  img: [B, L, 1024]
  txt: [B, L_txt, 1024]
```

### 3.4 å…³é”®è®¾è®¡ç‰¹ç‚¹

#### 1. åŒæµæ¶æ„çš„ä¼˜åŠ¿
- å›¾åƒå’Œæ–‡æœ¬ä¿æŒç‹¬ç«‹çš„å¤„ç†è·¯å¾„
- é€šè¿‡è”åˆæ³¨æ„åŠ›å®ç°è·¨æ¨¡æ€äº¤äº’
- å„è‡ªçš„MLPä¿æŒæ¨¡æ€ç‰¹å®šçš„ç‰¹å¾å¤„ç†

#### 2. AdaLN-Zeroè°ƒåˆ¶
- ä½¿ç”¨æ—¶é—´æ­¥ä¿¡æ¯åŠ¨æ€è°ƒæ•´å½’ä¸€åŒ–
- `shift`å’Œ`scale`æ”¹å˜ç‰¹å¾åˆ†å¸ƒ
- `gate`æ§åˆ¶æ–°ä¿¡æ¯çš„èåˆç¨‹åº¦
- åˆå§‹åŒ–ç­–ç•¥è®©æ¨¡å‹ä»æ’ç­‰æ˜ å°„å¼€å§‹

#### 3. QKå½’ä¸€åŒ–
- RMSNormç¨³å®šæ³¨æ„åŠ›è®¡ç®—
- é˜²æ­¢å¤§å€¼å¯¼è‡´çš„æ¢¯åº¦é—®é¢˜
- å¯¹é•¿åºåˆ—ç‰¹åˆ«é‡è¦

#### 4. è”åˆæ³¨æ„åŠ›æœºåˆ¶
æ–‡æœ¬å’Œå›¾åƒtokenåœ¨åŒä¸€ä¸ªattentionä¸­è®¡ç®—ï¼Œæ¯ä¸ªtokenéƒ½å¯ä»¥attendåˆ°æ‰€æœ‰å…¶ä»–tokenï¼ˆæ–‡æœ¬+å›¾åƒï¼‰ï¼

---

## 4. SingleStreamBlock è¯¦è§£

### 4.1 æ•´ä½“æ¶æ„

SingleStreamBlock æ˜¯**å•æµå¤„ç†**æ¨¡å—ï¼Œä¸ DoubleStreamBlock ä¸åŒï¼Œå®ƒå°†æ‰€æœ‰ç‰¹å¾ï¼ˆæ–‡æœ¬+å›¾åƒï¼‰ä½œä¸ºä¸€ä¸ªç»Ÿä¸€çš„åºåˆ—å¤„ç†ã€‚é‡‡ç”¨**å¹¶è¡Œæ¶æ„**ï¼ŒQKVæŠ•å½±å’ŒMLPåœ¨åŒä¸€ä¸ªçº¿æ€§å±‚ä¸­å¹¶è¡Œè®¡ç®—ï¼Œæé«˜æ•ˆç‡ã€‚

**åˆå§‹åŒ–å‚æ•°**:
```python
SingleStreamBlock(
    hidden_size=1024,      # éšè—å±‚ç»´åº¦
    num_heads=16,          # æ³¨æ„åŠ›å¤´æ•°
    mlp_ratio=4.0,         # MLPæ‰©å±•æ¯”ä¾‹ â†’ mlp_hidden_dim=4096
    qk_scale=None          # æ³¨æ„åŠ›ç¼©æ”¾å› å­ï¼ˆé»˜è®¤head_dim^-0.5=0.125ï¼‰
)
```

**å…³é”®ç»„ä»¶**:
```python
# å¹¶è¡Œçº¿æ€§å±‚ï¼šåŒæ—¶è®¡ç®—QKVå’ŒMLPè¾“å…¥
self.linear1 = nn.Linear(1024, 1024*3 + 4096)  # â†’ 7168

# å¹¶è¡Œè¾“å‡ºå±‚ï¼šåŒæ—¶å¤„ç†æ³¨æ„åŠ›è¾“å‡ºå’ŒMLPè¾“å‡º
self.linear2 = nn.Linear(1024 + 4096, 1024)    # 5120 â†’ 1024

self.norm = QKNorm(head_dim=64)
self.pre_norm = LayerNorm(1024, elementwise_affine=False)
self.mlp_act = GELU(approximate="tanh")
self.modulation = Modulation(1024, double=False)  # åªæœ‰ä¸€ç»„è°ƒåˆ¶å‚æ•°
```

**è¾“å…¥æ•°æ®**:
- `x`: `[B, L_txt+L, 1024]` - åˆå¹¶åçš„æ–‡æœ¬+å›¾åƒç‰¹å¾åºåˆ—
- `vec`: `[B, 1024]` - æ—¶é—´æ­¥åµŒå…¥
- `pe`: Position Embeddingï¼ˆä»£ç ä¸­ä¸ºNoneï¼‰

### 4.2 è¯¦ç»†æ•°æ®æµç¨‹

#### é˜¶æ®µ1: Modulationï¼ˆè‡ªé€‚åº”è°ƒåˆ¶ï¼‰

```python
mod, _ = self.modulation(vec)
```

```
è¾“å…¥: vec [B, 1024]
  â†“
SiLUæ¿€æ´»
  â†“
Linear(1024 â†’ 3072)  # multiplier=3 (å› ä¸ºdouble=False)
  â†“
[:, None, :] æ‰©å±•ç»´åº¦ â†’ [B, 1, 3072]
  â†“
chunk(3, dim=-1) åˆ†æˆ3ä»½
  â†“
è¾“å‡º: mod (shift, scale, gate)  [B, 1, 1024] Ã— 3
ç¬¬äºŒä¸ªè¿”å›å€¼ä¸ºNoneï¼ˆå› ä¸ºdouble=Falseï¼‰
```

**ä½œç”¨**: ç”Ÿæˆå•ç»„è°ƒåˆ¶å‚æ•°ï¼Œæ§åˆ¶æ•´ä¸ªblockçš„å½’ä¸€åŒ–å’Œè¾“å‡ºé—¨æ§ã€‚

#### é˜¶æ®µ2: å½’ä¸€åŒ–ä¸è°ƒåˆ¶

```python
x_mod = (1 + mod.scale) * self.pre_norm(x) + mod.shift
```

```
x: [B, L_txt+L, 1024]
  â†“
pre_norm: LayerNorm(æ— ä»¿å°„å‚æ•°)
  è®¡ç®—: normalized = (x - mean) / sqrt(var + eps)
  â†“ [B, L_txt+L, 1024]
è°ƒåˆ¶:
  (1 + mod.scale[B,1,1024]) * normalized + mod.shift[B,1,1024]
  å¹¿æ’­æœºåˆ¶: [B,1,1024] â†’ [B,L_txt+L,1024]
  â†“
x_mod: [B, L_txt+L, 1024]
```

#### é˜¶æ®µ3: å¹¶è¡Œçº¿æ€§æŠ•å½±ï¼ˆå…³é”®è®¾è®¡ï¼ï¼‰

```python
qkv, mlp = torch.split(
    self.linear1(x_mod), 
    [3 * self.hidden_size, self.mlp_hidden_dim], 
    dim=-1
)
```

**å•ä¸ªçº¿æ€§å±‚åŒæ—¶è®¡ç®—**:
```
x_mod: [B, L_txt+L, 1024]
  â†“
linear1: Linear(1024 â†’ 7168)
  æƒé‡çŸ©é˜µ: [1024, 7168]
  åˆ†ä¸ºä¸¤éƒ¨åˆ†:
    - QKVéƒ¨åˆ†: [1024, 3072]  (å‰3*1024ç»´)
    - MLPéƒ¨åˆ†: [1024, 4096]  (å4096ç»´)
  â†“
output: [B, L_txt+L, 7168]
  â†“
torch.split([3072, 4096], dim=-1)
  â†“
qkv: [B, L_txt+L, 3072]
mlp: [B, L_txt+L, 4096]
```

**ä¼˜åŠ¿**: 
- **å¹¶è¡Œè®¡ç®—**: ä¸€æ¬¡çŸ©é˜µä¹˜æ³•åŒæ—¶å¾—åˆ°QKVå’ŒMLPçš„è¾“å…¥
- **å†…å­˜æ•ˆç‡**: å‡å°‘å†…å­˜è®¿é—®æ¬¡æ•°
- **è®¡ç®—æ•ˆç‡**: åˆ©ç”¨GPUå¹¶è¡Œæ€§ï¼Œæ¯”ä¸¤æ¬¡çº¿æ€§å±‚æ›´å¿«

#### é˜¶æ®µ4: æ³¨æ„åŠ›åˆ†æ”¯å¤„ç†

**4.1 QKVé‡æ’åˆ—**:
```python
q, k, v = rearrange(qkv, "B L (K H D) -> K B H L D", K=3, H=16)
```

```
qkv: [B, L_txt+L, 3072]
  â†“
reshapeç†è§£:
  3072 = K * H * D = 3 * 16 * 64
  â†“
rearrange: [B, L_txt+L, 3, 16, 64] â†’ [3, B, 16, L_txt+L, 64]
  â†“
åˆ†è§£ä¸º3ä¸ªå¼ é‡:
q: [B, 16, L_txt+L, 64]  # Query
k: [B, 16, L_txt+L, 64]  # Key
v: [B, 16, L_txt+L, 64]  # Value
```

**4.2 QKå½’ä¸€åŒ–**:
```python
q, k = self.norm(q, k, v)
```

ä½¿ç”¨ RMSNorm åœ¨æœ€åä¸€ç»´ï¼ˆdim=64ï¼‰å½’ä¸€åŒ–ã€‚

**4.3 æ³¨æ„åŠ›è®¡ç®—**:
```python
attn = attention(q, k, v, pe=pe)
```

```
q, k, v: [B, 16, L_txt+L, 64]
  â†“
scaled_dot_product_attention
  scores = softmax(q @ k^T / sqrt(64))  # [B, 16, L_txt+L, L_txt+L]
  output = scores @ v                    # [B, 16, L_txt+L, 64]
  â†“
rearrange "B H L D -> B L (H D)"
  â†“
attn: [B, L_txt+L, 1024]
```

#### é˜¶æ®µ5: MLPåˆ†æ”¯å¤„ç†

```python
mlp_activated = self.mlp_act(mlp)
```

```
mlp: [B, L_txt+L, 4096]
  â†“
GELU(approximate="tanh")
  â†“
mlp_activated: [B, L_txt+L, 4096]
```

#### é˜¶æ®µ6: å¹¶è¡Œè¾“å‡ºåˆå¹¶

```python
output = self.linear2(torch.cat((attn, mlp_activated), dim=-1))
```

```
attn: [B, L_txt+L, 1024]
mlp_activated: [B, L_txt+L, 4096]
  â†“
cat(dim=-1)
  â†“
[B, L_txt+L, 5120]
  â†“
linear2: Linear(5120 â†’ 1024)
  â†“
output: [B, L_txt+L, 1024]
```

**å…³é”®**: æ³¨æ„åŠ›å’ŒMLPçš„è¾“å‡ºåœ¨è¿™ä¸€æ­¥æ‰åˆå¹¶ï¼

#### é˜¶æ®µ7: é—¨æ§ä¸æ®‹å·®è¿æ¥

```python
return x + mod.gate * output
```

```
output: [B, L_txt+L, 1024]
  â†“
ä¹˜ä»¥é—¨æ§: mod.gate [B, 1, 1024]
  â†“
gated_output: [B, L_txt+L, 1024]
  â†“
æ®‹å·®è¿æ¥: x + gated_output
  â†“
final_output: [B, L_txt+L, 1024]
```

### 4.3 å®Œæ•´æ•°æ®æµæ€»ç»“

```
è¾“å…¥: x [B, L_txt+L, 1024], vec [B, 1024]

Modulation:
  vec â†’ mod (shift, scale, gate) [B, 1, 1024] Ã— 3

å½’ä¸€åŒ–è°ƒåˆ¶:
  x â†’ LayerNorm â†’ modulate â†’ x_mod [B, L_txt+L, 1024]

å¹¶è¡ŒæŠ•å½±:
  x_mod â†’ linear1(1024â†’7168) â†’ split
    â”œâ”€ qkv [B, L_txt+L, 3072]
    â””â”€ mlp [B, L_txt+L, 4096]

æ³¨æ„åŠ›åˆ†æ”¯:
  qkv â†’ rearrange â†’ q,k,v [B, 16, L_txt+L, 64]
      â†’ qk_norm â†’ attention â†’ [B, L_txt+L, 1024]

MLPåˆ†æ”¯:
  mlp â†’ GELU â†’ [B, L_txt+L, 4096]

åˆå¹¶è¾“å‡º:
  cat(attn, mlp) â†’ [B, L_txt+L, 5120]
                 â†’ linear2 â†’ [B, L_txt+L, 1024]

é—¨æ§æ®‹å·®:
  x + gate * output â†’ [B, L_txt+L, 1024]
```

### 4.4 ä¸ DoubleStreamBlock çš„å¯¹æ¯”

| ç‰¹æ€§ | DoubleStreamBlock | SingleStreamBlock |
|------|------------------|-------------------|
| **æµæ•°é‡** | åŒæµï¼ˆå›¾åƒ+æ–‡æœ¬åˆ†ç¦»ï¼‰ | å•æµï¼ˆåˆå¹¶å¤„ç†ï¼‰ |
| **æ³¨æ„åŠ›èŒƒå›´** | è”åˆæ³¨æ„åŠ›ï¼ˆè·¨æ¨¡æ€ï¼‰ | ç»Ÿä¸€æ³¨æ„åŠ›ï¼ˆæ‰€æœ‰tokenï¼‰ |
| **è°ƒåˆ¶å‚æ•°** | ä¸¤ç»„ï¼ˆå›¾åƒå’Œæ–‡æœ¬å„è‡ªï¼‰ | ä¸€ç»„ï¼ˆç»Ÿä¸€æ§åˆ¶ï¼‰ |
| **MLPç»“æ„** | ä¸²è¡Œï¼ˆæ ‡å‡†ç»“æ„ï¼‰ | å¹¶è¡Œï¼ˆä¸QKVå¹¶è¡Œï¼‰ |
| **è®¡ç®—æ•ˆç‡** | ä¸­ç­‰ | é«˜ï¼ˆå¹¶è¡Œè®¾è®¡ï¼‰ |
| **å‚æ•°é‡** | æ›´å¤šï¼ˆä¸¤å¥—ç‹¬ç«‹å‚æ•°ï¼‰ | è¾ƒå°‘ï¼ˆå…±äº«å¤„ç†ï¼‰ |
| **é€‚ç”¨åœºæ™¯** | éœ€è¦ä¿æŒæ¨¡æ€åŒºåˆ† | ç‰¹å¾å·²å……åˆ†èåˆ |

---

## 5. Attention æœºåˆ¶è¯¦è§£

### 5.1 ä»£ç ç»“æ„

```python
# å¯é€‰çš„é«˜æ•ˆæ³¨æ„åŠ›å®ç°
scaled_dot_product_attention = nn.functional.scaled_dot_product_attention
if os.environ.get('USE_SAGEATTN', '0') == '1':
    try:
        from sageattention import sageattn
        scaled_dot_product_attention = sageattn
    except ImportError:
        raise ImportError('Please install "sageattention"')


def attention(q: Tensor, k: Tensor, v: Tensor, **kwargs) -> Tensor:
    """
    è½»é‡çº§æ³¨æ„åŠ›å‡½æ•°
    è¾“å…¥: q, k, v éƒ½æ˜¯ [B, H, L, D]
    è¾“å‡º: [B, L, H*D]
    """
    x = scaled_dot_product_attention(q, k, v)
    x = rearrange(x, "B H L D -> B L (H D)")
    return x
```

### 5.2 è¾“å…¥æ•°æ®æ ¼å¼

```
q: [B, H, L, D]
k: [B, H, L, D]  
v: [B, H, L, D]

å…¶ä¸­:
  B = batch_size (æ‰¹æ¬¡å¤§å°)
  H = num_heads (æ³¨æ„åŠ›å¤´æ•°ï¼Œä¾‹å¦‚16)
  L = sequence_length (åºåˆ—é•¿åº¦)
  D = head_dim (æ¯ä¸ªå¤´çš„ç»´åº¦ï¼Œä¾‹å¦‚64)
```

### 5.3 Scaled Dot-Product Attention æ ¸å¿ƒè®¡ç®—

æ•°å­¦å…¬å¼ï¼š
$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$

#### æ­¥éª¤1: è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°

```python
scores = torch.matmul(q, k.transpose(-2, -1))  # QK^T
```

```
q: [B, H, L, D]
k: [B, H, L, D]
k.transpose(-2, -1): [B, H, D, L]
  â†“
matmul(q, k^T)
  â†“
scores: [B, H, L, L]  # æ³¨æ„åŠ›åˆ†æ•°çŸ©é˜µ
```

**æ•°å€¼ç¤ºä¾‹**ï¼ˆç®€åŒ–ä¸ºå•å¤´ï¼ŒL=3ï¼ŒD=2ï¼‰:
```python
q = [[1.0, 0.5],    # token_1çš„query
     [0.8, 1.2],    # token_2çš„query  
     [0.3, 0.9]]    # token_3çš„query

k = [[1.2, 0.4],    # token_1çš„key
     [0.7, 1.1],    # token_2çš„key
     [0.5, 0.8]]    # token_3çš„key

scores = q @ k^T:
         k1   k2   k3
    q1 [[1.4, 1.25, 1.22],
    q2  [1.8, 1.88, 1.36],
    q3  [0.99, 1.2, 0.87]]
```

**å«ä¹‰**: `scores[i, j]` è¡¨ç¤ºç¬¬ i ä¸ª token å¯¹ç¬¬ j ä¸ª token çš„**åŸå§‹å…³æ³¨åº¦**ã€‚

#### æ­¥éª¤2: ç¼©æ”¾ (Scaling)

```python
scores = scores / math.sqrt(D)  # é™¤ä»¥sqrt(head_dim)
```

```
D = 64 (head_dim)
sqrt(D) = 8.0

scores: [B, H, L, L]
  â†“
é™¤ä»¥ 8.0
  â†“
scaled_scores: [B, H, L, L]
```

**ä¸ºä»€ä¹ˆè¦ç¼©æ”¾ï¼Ÿ**

å‡è®¾ q å’Œ k çš„å…ƒç´ æ˜¯ç‹¬ç«‹åŒåˆ†å¸ƒçš„ï¼Œå‡å€¼ä¸º0ï¼Œæ–¹å·®ä¸º1ï¼š
- `qÂ·k` çš„æ–¹å·®çº¦ä¸º `d_k`ï¼ˆç»´åº¦ï¼‰
- å½“ `d_k` å¾ˆå¤§æ—¶ï¼Œç‚¹ç§¯ä¼šéå¸¸å¤§
- å¯¼è‡´ softmax è¿›å…¥é¥±å’ŒåŒºï¼Œæ¢¯åº¦æ¶ˆå¤±

**æ•°å€¼å¯¹æ¯”**:
```
ä¸ç¼©æ”¾æ—¶ (d=64):
  scores å¯èƒ½åœ¨ [-50, 50] èŒƒå›´
  softmax(50) â‰ˆ 1.0, softmax(-50) â‰ˆ 0.0  # æ¢¯åº¦æ¥è¿‘0

ç¼©æ”¾å (é™¤ä»¥8):
  scores åœ¨ [-6.25, 6.25] èŒƒå›´
  æ¢¯åº¦æ›´å¥åº·ï¼
```

#### æ­¥éª¤3: Softmax å½’ä¸€åŒ–

```python
attention_weights = torch.softmax(scaled_scores, dim=-1)
```

```
scaled_scores: [B, H, L, L]
  â†“
å¯¹æœ€åä¸€ä¸ªç»´åº¦åšsoftmax
å¯¹äºæ¯ä¸€è¡Œ: exp(x_i) / sum(exp(x_j))
  â†“
attention_weights: [B, H, L, L]
æ¯ä¸€è¡Œçš„å’Œä¸º 1.0
```

**æ•°å­¦å…¬å¼**:
$$\text{softmax}(x_i) = \frac{e^{x_i}}{\sum_{j=1}^{L} e^{x_j}}$$

**æ•°å€¼ç¤ºä¾‹**:
```python
# å¯¹ç¬¬ä¸€è¡Œ [0.99, 0.88, 0.86] åšsoftmax:
exp_values = [2.69, 2.41, 2.36]
sum_exp = 7.46

attention_weights[0] = [0.36, 0.32, 0.32]  # å’Œä¸º1.0

# å®Œæ•´çš„attention_weights:
         k1    k2    k3
    q1 [[0.36, 0.32, 0.32],   # token_1çš„æ³¨æ„åŠ›åˆ†å¸ƒ
    q2  [0.31, 0.37, 0.32],   # token_2çš„æ³¨æ„åŠ›åˆ†å¸ƒ
    q3  [0.30, 0.37, 0.33]]   # token_3çš„æ³¨æ„åŠ›åˆ†å¸ƒ
```

**å«ä¹‰**: 
- æ¯ä¸€è¡Œæ˜¯ä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒ
- `attention_weights[i, j]` = token_i åˆ†é…ç»™ token_j çš„æ³¨æ„åŠ›æƒé‡
- æƒé‡å’Œä¸º1ï¼Œè¡¨ç¤º"å…³æ³¨åº¦"çš„åˆ†é…

#### æ­¥éª¤4: åŠ æƒæ±‚å’Œ (Weighted Sum)

```python
output = torch.matmul(attention_weights, v)
```

```
attention_weights: [B, H, L, L]
v: [B, H, L, D]
  â†“
matmul
  â†“
output: [B, H, L, D]
```

**æ•°å­¦å«ä¹‰**: æ¯ä¸ª token çš„è¾“å‡ºæ˜¯æ‰€æœ‰ token çš„ value çš„**åŠ æƒå¹³å‡**ã€‚

**æ•°å€¼ç¤ºä¾‹**:
```python
v = [[2.0, 1.5],    # token_1çš„value
     [1.0, 2.5],    # token_2çš„value
     [1.8, 0.8]]    # token_3çš„value

# token_1çš„è¾“å‡º = 0.36*[2.0,1.5] + 0.32*[1.0,2.5] + 0.32*[1.8,0.8]
#              = [1.62, 1.60]

# token_2çš„è¾“å‡º = 0.31*[2.0,1.5] + 0.37*[1.0,2.5] + 0.32*[1.8,0.8]
#              = [1.56, 1.85]

# token_3çš„è¾“å‡º = [1.56, 1.64]

output = [[1.62, 1.60],
          [1.56, 1.85],
          [1.56, 1.64]]
```

**å…³é”®ç†è§£**: æ¯ä¸ª token éƒ½èåˆäº†å…¶ä»– token çš„ä¿¡æ¯ï¼

### 5.4 å¤šå¤´åˆå¹¶

```python
x = rearrange(x, "B H L D -> B L (H D)")
```

```
x: [B, H, L, D]
ä¾‹å¦‚: [B, 16, L, 64]
  â†“
rearrange: å°† H å’Œ D ç»´åº¦åˆå¹¶
  â†“
output: [B, L, H*D]
ä¾‹å¦‚: [B, L, 1024]  (16*64=1024)
```

**è¯¦ç»†è¿‡ç¨‹ç¤ºä¾‹**ï¼ˆB=1, H=3, L=4, D=2ï¼‰:
```python
# è¾“å…¥:
x[0, 0, :, :] = [[1,2], [3,4], [5,6], [7,8]]     # head_0
x[0, 1, :, :] = [[9,10], [11,12], [13,14], [15,16]]  # head_1
x[0, 2, :, :] = [[17,18], [19,20], [21,22], [23,24]] # head_2

# rearrangeåï¼ŒæŒ‰tokenæ’åˆ—ï¼Œæ‹¼æ¥æ‰€æœ‰å¤´:
output[0, 0, :] = [1,2, 9,10, 17,18]    # token_0 æ‰€æœ‰å¤´æ‹¼æ¥
output[0, 1, :] = [3,4, 11,12, 19,20]   # token_1 æ‰€æœ‰å¤´æ‹¼æ¥
output[0, 2, :] = [5,6, 13,14, 21,22]   # token_2 æ‰€æœ‰å¤´æ‹¼æ¥
output[0, 3, :] = [7,8, 15,16, 23,24]   # token_3 æ‰€æœ‰å¤´æ‹¼æ¥

# å½¢çŠ¶: [B, L, H*D] = [1, 4, 6]
```

### 5.5 PyTorch çš„ scaled_dot_product_attention

PyTorch çš„å†…ç½®å‡½æ•°ä¼šè‡ªåŠ¨é€‰æ‹©æœ€ä¼˜å®ç°ï¼š

#### 1. æ ‡å‡†å®ç° (naive)
```python
scores = (q @ k.transpose(-2, -1)) / math.sqrt(q.size(-1))
attn_weights = F.softmax(scores, dim=-1)
output = attn_weights @ v
```

#### 2. Flash Attentionï¼ˆè‡ªåŠ¨é€‰æ‹©ï¼‰
å¦‚æœæ¡ä»¶æ»¡è¶³ï¼Œä¼šä½¿ç”¨é«˜æ•ˆçš„ Flash Attentionï¼š
- å‡å°‘ HBMï¼ˆé«˜å¸¦å®½å†…å­˜ï¼‰è®¿é—®
- åˆ†å—è®¡ç®—ï¼Œé™ä½å†…å­˜å³°å€¼
- é€Ÿåº¦æå‡ 2-4x

#### 3. Memory-Efficient Attention
- ä¸æ˜¾å¼å­˜å‚¨å®Œæ•´çš„æ³¨æ„åŠ›çŸ©é˜µ `[B, H, L, L]`
- èŠ‚çœå†…å­˜ï¼Œé€‚åˆé•¿åºåˆ—

#### 4. å¯é€‰å‚æ•°
```python
scaled_dot_product_attention(
    q, k, v,
    attn_mask=None,      # æ³¨æ„åŠ›æ©ç 
    dropout_p=0.0,       # Dropoutæ¦‚ç‡
    is_causal=False,     # æ˜¯å¦ä½¿ç”¨å› æœæ©ç 
)
```

### 5.6 SageAttention é«˜æ•ˆå®ç°

å½“è®¾ç½® `USE_SAGEATTN=1` æ—¶ä½¿ç”¨ï¼š

```python
from sageattention import sageattn
```

**ä¼˜åŠ¿**:
1. **é‡åŒ–åŠ é€Ÿ**: ä½¿ç”¨ INT8/FP8 é‡åŒ–è®¡ç®—
2. **å†…æ ¸èåˆ**: å‡å°‘å†…å­˜è®¿é—®
3. **ç¡¬ä»¶ä¼˜åŒ–**: é’ˆå¯¹ç‰¹å®šGPUä¼˜åŒ–
4. **æ›´ä½å†…å­˜**: é€‚åˆè¶…é•¿åºåˆ—

**æ€§èƒ½å¯¹æ¯”**:
```
æ ‡å‡†å®ç°:     100% æ—¶é—´, 100% å†…å­˜
Flash Attn:   40% æ—¶é—´,  50% å†…å­˜
SageAttn:     25% æ—¶é—´,  40% å†…å­˜  (å¯èƒ½ç•¥å¾®æŸå¤±ç²¾åº¦)
```

### 5.7 å®Œæ•´çš„æ³¨æ„åŠ›æµç¨‹æ€»ç»“

```
è¾“å…¥:
  q: [B, H, L, D] = [2, 16, 256, 64]
  k: [B, H, L, D] = [2, 16, 256, 64]
  v: [B, H, L, D] = [2, 16, 256, 64]

æ­¥éª¤1: è®¡ç®—åˆ†æ•°
  scores = q @ k^T
  â†’ [2, 16, 256, 256]

æ­¥éª¤2: ç¼©æ”¾
  scores = scores / sqrt(64) = scores / 8.0
  â†’ [2, 16, 256, 256]

æ­¥éª¤3: Softmax
  attention_weights = softmax(scores, dim=-1)
  â†’ [2, 16, 256, 256]
  æ¯ä¸€è¡Œçš„å’Œä¸º1.0

æ­¥éª¤4: åŠ æƒæ±‚å’Œ
  output = attention_weights @ v
  â†’ [2, 16, 256, 64]

æ­¥éª¤5: å¤šå¤´åˆå¹¶
  output = rearrange(output, "B H L D -> B L (H D)")
  â†’ [2, 256, 1024]

è¿”å›: [B, L, hidden_dim]
```

### 5.8 å…³é”®è®¾è®¡è¦ç‚¹

#### 1. å¤šå¤´æ³¨æ„åŠ›çš„æ„ä¹‰
ä¸åŒçš„å¤´å¯ä»¥å­¦ä¹ ä¸åŒçš„å…³æ³¨æ¨¡å¼ï¼š
- Head 1: å…³æ³¨å±€éƒ¨é‚»è¿‘token
- Head 2: å…³æ³¨å…¨å±€è¯­ä¹‰ç›¸ä¼¼token
- Head 3: å…³æ³¨ç‰¹å®šç±»å‹çš„token

#### 2. ç¼©æ”¾å› å­çš„é‡è¦æ€§
- æ²¡æœ‰ç¼©æ”¾: æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸
- åˆé€‚çš„ç¼©æ”¾: ç¨³å®šè®­ç»ƒ

#### 3. è®¡ç®—å¤æ‚åº¦
- æ—¶é—´å¤æ‚åº¦: O(LÂ² Â· d)
- ç©ºé—´å¤æ‚åº¦: O(LÂ²) ï¼ˆå­˜å‚¨æ³¨æ„åŠ›çŸ©é˜µï¼‰
- å¯¹é•¿åºåˆ—æ˜¯ç“¶é¢ˆï¼

#### 4. ä¼˜åŒ–æ–¹å‘
- Flash Attention: å‡å°‘å†…å­˜è®¿é—®
- Sparse Attention: åªè®¡ç®—éƒ¨åˆ†ä½ç½®
- Linear Attention: é™ä½å¤æ‚åº¦åˆ° O(L)

---

## 6. ä¸ EfficientAttention çš„å¯¹æ¯”

### 6.1 æ¶æ„å±‚çº§å¯¹æ¯”

#### Hunyuan3DDiT çš„ `attention` å‡½æ•°
```python
def attention(q: Tensor, k: Tensor, v: Tensor, **kwargs) -> Tensor:
    x = scaled_dot_product_attention(q, k, v)
    x = rearrange(x, "B H L D -> B L (H D)")
    return x
```

**ç‰¹ç‚¹**:
- **è½»é‡çº§å‡½æ•°**: ä¸æ˜¯ `nn.Module`ï¼Œåªæ˜¯ä¸€ä¸ªå‡½æ•°
- **ä¸åŒ…å«æŠ•å½±**: QKVæŠ•å½±åœ¨å¤–éƒ¨å®Œæˆ
- **æç®€è®¾è®¡**: åªåšä¸¤ä»¶äº‹ï¼šè°ƒç”¨SDPA + é‡æ’åˆ—ç»´åº¦
- **å›ºå®šåç«¯**: PyTorch SDPA æˆ– SageAttention

#### EfficientAttention æ¨¡å—
```python
class EfficientAttention(nn.Module):
    def __init__(self, d_model, num_heads, d_head, dropout, ...):
        self.to_q = nn.Linear(d_model, self.inner_dim, bias=False)
        self.to_k = nn.Linear(d_model, self.inner_dim, bias=False)
        self.to_v = nn.Linear(d_model, self.inner_dim, bias=False)
        self.to_out = nn.Sequential(nn.Linear(...), nn.Dropout(...))
```

**ç‰¹ç‚¹**:
- **å®Œæ•´çš„ nn.Module**: åŒ…å«æ‰€æœ‰ç»„ä»¶
- **é›†æˆæŠ•å½±å±‚**: QKVæŠ•å½±å’Œè¾“å‡ºæŠ•å½±éƒ½åœ¨å†…éƒ¨
- **å¤šåç«¯é€‰æ‹©**: 4ç§å®ç°å¯é€‰
- **æ›´å¤šåŠŸèƒ½**: dropoutã€maskã€attention_biasæ”¯æŒ

### 6.2 åŠŸèƒ½å¯¹æ¯”è¡¨

| åŠŸèƒ½ | Hunyuan3DDiT `attention` | EfficientAttention |
|------|-------------------------|-------------------|
| **QKVæŠ•å½±** | âŒ å¤–éƒ¨å¤„ç† | âœ… å†…éƒ¨åŒ…å« `to_q/k/v` |
| **è¾“å‡ºæŠ•å½±** | âŒ å¤–éƒ¨å¤„ç† | âœ… å†…éƒ¨åŒ…å« `to_out` |
| **Dropout** | âŒ æ—  | âœ… æ³¨æ„åŠ›dropout + è¾“å‡ºdropout |
| **Maskæ”¯æŒ** | âŒ æ—  | âœ… å¤šç§maskæ ¼å¼ |
| **Attention Bias** | âŒ æ—  | âœ… æ”¯æŒå‡ ä½•bias |
| **åç«¯é€‰æ‹©** | 2ç§ (SDPA/SageAttn) | 4ç§ (SDPA/Flash/Chunked/Standard) |
| **åˆ†å—è®¡ç®—** | âŒ æ—  | âœ… Chunked attention |
| **å†…å­˜ç›‘æ§** | âŒ æ—  | âœ… è¯¦ç»†æ—¥å¿—å’Œfallback |

### 6.3 ä½¿ç”¨æ–¹å¼å¯¹æ¯”

#### Hunyuan3DDiT é£æ ¼
```python
class SelfAttention(nn.Module):
    def __init__(self):
        self.qkv = nn.Linear(1024, 3072)
        self.proj = nn.Linear(1024, 1024)
    
    def forward(self, x):
        qkv = self.qkv(x)
        q, k, v = rearrange(qkv, "B L (K H D) -> K B H L D", K=3, H=16)
        x = attention(q, k, v)  # ç®€å•è°ƒç”¨
        x = self.proj(x)
        return x
```

**ä¼˜ç‚¹**: ä»£ç ç®€æ´ï¼Œæ§åˆ¶ç²’åº¦ç»†ï¼Œæ˜“äºç†è§£
**ç¼ºç‚¹**: éœ€è¦æ‰‹åŠ¨å¤„ç†ç»†èŠ‚ï¼Œæ²¡æœ‰mask/dropoutæ”¯æŒ

#### EfficientAttention é£æ ¼
```python
attn = EfficientAttention(
    d_model=1024, 
    num_heads=16, 
    d_head=64,
    dropout=0.1,
    chunk_size=512
)
output = attn(query, key, value, mask=mask)
```

**ä¼˜ç‚¹**: å¼€ç®±å³ç”¨ï¼ŒåŠŸèƒ½å®Œæ•´ï¼Œè‡ªåŠ¨å†…å­˜ä¼˜åŒ–
**ç¼ºç‚¹**: æ›´é‡ï¼Œå°è£…å±‚çº§å¤š

### 6.4 å†…å­˜ä¼˜åŒ–å¯¹æ¯”

#### Hunyuan3DDiT
- âœ… ä¾èµ–PyTorchè‡ªåŠ¨é€‰æ‹©backend
- âœ… SageAttentionæä¾›é‡åŒ–åŠ é€Ÿ
- âŒ æ²¡æœ‰æ˜¾å¼çš„å†…å­˜ç®¡ç†
- âŒ æ— é•¿åºåˆ—ç‰¹æ®Šå¤„ç†

#### EfficientAttention
- âœ… åˆ†å—è®¡ç®—é¿å…å­˜å‚¨å®Œæ•´ `[L, L]` çŸ©é˜µ
- âœ… å¯¹é•¿åºåˆ—ç‰¹åˆ«ä¼˜åŒ–
- âœ… è¯¦ç»†çš„å†…å­˜ç›‘æ§å’Œæ—¥å¿—
- âœ… è‡ªåŠ¨fallbackæœºåˆ¶

**å†…å­˜å¯¹æ¯”**ï¼ˆL=4096ï¼‰:
```
æ ‡å‡†æ³¨æ„åŠ›: B * 16 * 4096 * 4096 * 4 bytes = B * 1GB

åˆ†å—æ³¨æ„åŠ› (chunk_size=512):
            B * 16 * 512 * 4096 * 4 bytes = B * 128MB
            èŠ‚çœ8å€å†…å­˜ï¼
```

### 6.5 æ€§èƒ½å’Œé€‚ç”¨åœºæ™¯

#### Hunyuan3DDiT `attention`

**é€‚ç”¨åœºæ™¯**:
- âœ… å›ºå®šé•¿åº¦åºåˆ—ï¼ˆ<2048ï¼‰
- âœ… ä¸éœ€è¦maskçš„self-attention
- âœ… è¿½æ±‚æè‡´ç®€æ´
- âœ… DiT/Fluxç±»æ¶æ„

**æ€§èƒ½**:
- åºåˆ—é•¿åº¦ 256:   æå¿«
- åºåˆ—é•¿åº¦ 1024:  å¿«
- åºåˆ—é•¿åº¦ 4096:  å¯èƒ½OOM

#### EfficientAttention

**é€‚ç”¨åœºæ™¯**:
- âœ… å˜é•¿åºåˆ—
- âœ… éœ€è¦padding mask
- âœ… è¶…é•¿åºåˆ—ï¼ˆ>2048ï¼‰
- âœ… éœ€è¦attention bias
- âœ… å†…å­˜å—é™ç¯å¢ƒ

**æ€§èƒ½**:
- åºåˆ—é•¿åº¦ 256:   å¿« (SDPA)
- åºåˆ—é•¿åº¦ 1024:  å¿« (SDPA)
- åºåˆ—é•¿åº¦ 4096:  ä¸­ç­‰ (chunked)
- åºåˆ—é•¿åº¦ 8192:  å¯å¤„ç† (chunked)

### 6.6 ä»£ç è´¨é‡å¯¹æ¯”

| ç»´åº¦ | Hunyuan3DDiT | EfficientAttention |
|------|--------------|-------------------|
| **ä»£ç è¡Œæ•°** | ~10è¡Œ | ~350è¡Œ |
| **å¤æ‚åº¦** | æä½ | ä¸­ç­‰ |
| **å¯ç»´æŠ¤æ€§** | â­â­â­â­â­ | â­â­â­ |
| **é”™è¯¯å¤„ç†** | âŒ æ—  | âœ… å®Œå–„ |
| **æ—¥å¿—** | âŒ æ—  | âœ… è¯¦ç»† |
| **æµ‹è¯•å‹å¥½** | âœ… ç®€å•æ˜“æµ‹ | âš ï¸ éœ€æµ‹è¯•å¤šæ¡è·¯å¾„ |

### 6.7 æ¨èä½¿ç”¨å»ºè®®

**ä½¿ç”¨ Hunyuan3DDiT é£æ ¼å¦‚æœ**:
1. æ¶æ„å·²æœ‰å®Œæ•´çš„attentionå°è£…
2. åºåˆ—é•¿åº¦å›ºå®šä¸”ä¸é•¿ï¼ˆ<2048ï¼‰
3. ä¸éœ€è¦maskæˆ–bias
4. è¿½æ±‚ä»£ç ç®€æ´æ€§
5. ä½¿ç”¨DiT/Fluxç±»æ¶æ„

**ä½¿ç”¨ EfficientAttention å¦‚æœ**:
1. æ„å»ºé€šç”¨çš„Transformeræ¨¡å‹
2. éœ€è¦å¤„ç†å˜é•¿åºåˆ—
3. éœ€è¦padding maskæ”¯æŒ
4. åºåˆ—å¯èƒ½å¾ˆé•¿ï¼ˆ>2048ï¼‰
5. éœ€è¦attention bias
6. å†…å­˜å—é™ç¯å¢ƒ
7. éœ€è¦è¯¦ç»†çš„æ€§èƒ½ç›‘æ§

### 6.8 æ€»ç»“

**Hunyuan3DDiT çš„ `attention`**:
- ğŸ¯ **è®¾è®¡å“²å­¦**: æç®€ä¸»ä¹‰ï¼Œåšå¥½ä¸€ä»¶äº‹
- ğŸ’¡ **ä¼˜åŠ¿**: ä»£ç æ¸…æ™°ï¼Œæ˜“äºç†è§£å’Œä¿®æ”¹
- âš ï¸ **é™åˆ¶**: åŠŸèƒ½å•ä¸€ï¼Œéœ€è¦å¤–éƒ¨æ”¯æŒ

**EfficientAttention**:
- ğŸ¯ **è®¾è®¡å“²å­¦**: å…¨åŠŸèƒ½è§£å†³æ–¹æ¡ˆ
- ğŸ’¡ **ä¼˜åŠ¿**: å¼€ç®±å³ç”¨ï¼Œå†…å­˜ä¼˜åŒ–ï¼Œé”™è¯¯å¤„ç†å®Œå–„
- âš ï¸ **é™åˆ¶**: æ›´é‡ï¼Œæ›´å¤æ‚

**é€‰æ‹©å»ºè®®**: 
- DiTç±»æ¨¡å‹ â†’ Hunyuané£æ ¼
- é€šç”¨Transformer â†’ EfficientAttention
- ç”Ÿäº§ç¯å¢ƒ â†’ EfficientAttentionï¼ˆæ›´robustï¼‰
- ç ”ç©¶åŸå‹ â†’ Hunyuané£æ ¼ï¼ˆæ›´çµæ´»ï¼‰

---

## å‚è€ƒèµ„æ–™

- åŸå§‹ä»£ç : `bin/hunyuan3ddit.py`
- å†…å­˜ä¼˜åŒ–å®ç°: `models/decoder/dit_memory_optimization.py`
- Hunyuan 3D è®¸å¯åè®®: TENCENT HUNYUAN NON-COMMERCIAL LICENSE AGREEMENT
- Flash Attentionè®ºæ–‡: [FlashAttention: Fast and Memory-Efficient Exact Attention](https://arxiv.org/abs/2205.14135)
- DiTè®ºæ–‡: [Scalable Diffusion Models with Transformers](https://arxiv.org/abs/2212.09748)

---

*æ–‡æ¡£ç”Ÿæˆæ—¥æœŸ: 2025-10-29*
*åˆ†æä»£ç ç‰ˆæœ¬: SceneLeapUltra*

