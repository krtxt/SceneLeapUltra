# DiT æ¨¡å‹æ¶æ„æ–‡æ¡£

æœ¬æ–‡æ¡£æè¿°äº† SceneLeapUltra é¡¹ç›®ä¸­ DiT (Diffusion Transformer) æ¨¡å‹çš„è¯¦ç»†æ¶æ„ã€‚

## æ•´ä½“æ¶æ„

```mermaid
flowchart TB
    %% è¾“å…¥å’Œtokenization
    NoisyInput["Noisy Input<br/>(Grasp Poses)"]
    NoisyInput --> Tokenizer["GraspTokenizer<br/>(Linear + LayerNorm)"]
    Tokenizer --> PosEmbed["Positional<br/>Embedding"]
    
    %% æ—¶é—´æ­¥åµŒå…¥
    Timestep["Timestep"]
    Timestep --> TimeEmbed["MLP<br/>(Linear-SiLU-Linear)"]
    TimeEmbed --> TimeEmbedding["Time Embedding<br/>(time_embed_dim)"]
    
    %% æ–‡æœ¬æ¡ä»¶
    TextPrompt["Text Prompt"]
    TextPrompt --> TextEnc1["CLIP / T5"]
    TextEnc1 --> TextEnc2["MLP"]
    TextEnc2 --> TextEmbed["Text Embedding<br/>(d_model)"]
    
    %% åœºæ™¯æ¡ä»¶
    ScenePC["Scene Point Cloud"]
    ScenePC --> SceneBackbone["Scene Backbone<br/>(PointNet2/PTv3)"]
    SceneBackbone --> SceneProj["Linear Projection"]
    SceneProj --> SceneContext["Scene Context<br/>(N_points Ã— d_model)"]
    
    %% DiT Blocks
    PosEmbed --> DiTBlocks["DiT Blocks<br/>(Ã—N layers)"]
    TimeEmbedding -.-> DiTBlocks
    SceneContext -.-> DiTBlocks
    TextEmbed -.-> DiTBlocks
    
    %% è¾“å‡º
    DiTBlocks --> OutputProj["Output Projection<br/>(LayerNorm + Linear)"]
    OutputProj --> Output["Output<br/>(Predicted Noise)"]
    
    %% æ ·å¼å®šä¹‰
    classDef inputStyle fill:#FFE4B5,stroke:#FF8C00,stroke-width:2px
    classDef embedStyle fill:#E6E6FA,stroke:#9370DB,stroke-width:2px
    classDef condStyle fill:#E0FFE0,stroke:#32CD32,stroke-width:2px
    classDef blockStyle fill:#FFF8DC,stroke:#DAA520,stroke-width:3px
    classDef outputStyle fill:#FFE4B5,stroke:#FF8C00,stroke-width:2px
    
    class NoisyInput,TextPrompt,Timestep,ScenePC inputStyle
    class Tokenizer,PosEmbed,TimeEmbed,TimeEmbedding,TextEnc1,TextEnc2,SceneBackbone,SceneProj embedStyle
    class TextEmbed,SceneContext condStyle
    class DiTBlocks blockStyle
    class OutputProj,Output outputStyle
```

## DiT Block è¯¦ç»†ç»“æ„

```mermaid
flowchart TB
    %% è¾“å…¥
    Input["Input<br/>(B, num_grasps, d_model)"]
    TimeEmb["Time Embedding"]
    SceneCtx["Scene Context"]
    TextCtx["Text Embedding"]
    
    %% Self-Attention åˆ†æ”¯
    Input --> Norm1["Norm<br/>(AdaptiveLayerNorm)"]
    TimeEmb -.-> Norm1
    Norm1 --> SelfAttn["Self-Attention<br/>(EfficientAttention)"]
    SelfAttn --> Add1((+))
    Input --> Add1
    
    %% Scene Cross-Attention åˆ†æ”¯
    Add1 --> Norm2["Norm<br/>(AdaptiveLayerNorm)"]
    TimeEmb -.-> Norm2
    Norm2 --> SceneAttn["Cross-Attention<br/>(Scene)"]
    SceneCtx -.-> SceneAttn
    SceneAttn --> Add2((+))
    Add1 --> Add2
    
    %% Text Cross-Attention åˆ†æ”¯
    Add2 --> Norm3["Norm<br/>(AdaptiveLayerNorm)"]
    TimeEmb -.-> Norm3
    Norm3 --> TextAttn["Cross-Attention<br/>(Text)"]
    TextCtx -.-> TextAttn
    TextAttn --> Add3((+))
    Add2 --> Add3
    
    %% Feed-Forward åˆ†æ”¯
    Add3 --> Norm4["Norm<br/>(AdaptiveLayerNorm)"]
    TimeEmb -.-> Norm4
    Norm4 --> FFN["FeedForward<br/>(Linear-GELU-Linear)"]
    FFN --> Add4((+))
    Add3 --> Add4
    
    %% è¾“å‡º
    Add4 --> Output["Output<br/>(B, num_grasps, d_model)"]
    
    %% æ ·å¼å®šä¹‰
    classDef inputStyle fill:#FFE4B5,stroke:#FF8C00,stroke-width:2px
    classDef normStyle fill:#E6E6FA,stroke:#9370DB,stroke-width:2px
    classDef attnStyle fill:#FFB6C1,stroke:#DC143C,stroke-width:2px
    classDef condStyle fill:#E0FFE0,stroke:#32CD32,stroke-width:2px
    classDef ffnStyle fill:#E6E6FA,stroke:#9370DB,stroke-width:2px
    classDef outputStyle fill:#FFE4B5,stroke:#FF8C00,stroke-width:2px
    
    class Input,TimeEmb inputStyle
    class SceneCtx,TextCtx condStyle
    class Norm1,Norm2,Norm3,Norm4 normStyle
    class SelfAttn,SceneAttn,TextAttn attnStyle
    class FFN ffnStyle
    class Output outputStyle
```

## æ¶æ„ç»„ä»¶è¯´æ˜

### 1. è¾“å…¥å¤„ç†

#### GraspTokenizer
- **è¾“å…¥ç»´åº¦**: `(B, num_grasps, d_x)` æˆ– `(B, d_x)`
- **è¾“å‡ºç»´åº¦**: `(B, num_grasps, d_model)`
- **ç»„ä»¶**:
  - Linear Projection: `d_x â†’ d_model`
  - LayerNorm

#### ä½ç½®ç¼–ç  (PositionalEmbedding)
- **ç±»å‹**: å¯å­¦ä¹ çš„ä½ç½®åµŒå…¥
- **æœ€å¤§é•¿åº¦**: 1000
- **ç»´åº¦**: `(max_len, d_model)`

### 2. æ¡ä»¶ç¼–ç 

#### æ—¶é—´æ­¥åµŒå…¥ (TimestepEmbedding)
- **è¾“å…¥**: `(B,)` æ—¶é—´æ­¥æ ‡é‡
- **è¾“å‡º**: `(B, time_embed_dim)`
- **ç½‘ç»œç»“æ„**:
  ```
  Sinusoidal Embedding (d_model)
  â†’ Linear(d_model, time_embed_dim)
  â†’ SiLU
  â†’ Linear(time_embed_dim, time_embed_dim)
  ```

#### åœºæ™¯æ¡ä»¶ (Scene Context)
- **è¾“å…¥**: ç‚¹äº‘ `(B, N_points, 3/6/7)`
  - åŸºç¡€: XYZ (3)
  - å¯é€‰: RGB (3)
  - å¯é€‰: Object Mask (1)
- **ä¸»å¹²ç½‘ç»œ**: PointNet2 æˆ– PTv3
- **è¾“å‡ºç»´åº¦**: `(B, N_points, d_model)`

#### æ–‡æœ¬æ¡ä»¶ (Text Embedding)
- **ç¼–ç å™¨**: CLIP æˆ– T5
- **å¤„ç†æµç¨‹**:
  - æ–‡æœ¬ç¼–ç : Prompt â†’ CLIP/T5 â†’ 512ç»´ç‰¹å¾
  - MLPå¤„ç†: 512ç»´ â†’ d_model
  - æ”¯æŒè´Ÿé¢æç¤º (Negative Prompts)
- **æ–‡æœ¬dropout**: è®­ç»ƒæ—¶å¯é…ç½®

### 3. DiT Block

æ¯ä¸ª DiT Block åŒ…å«å››ä¸ªä¸»è¦ç»„ä»¶ï¼Œå‡é‡‡ç”¨æ®‹å·®è¿æ¥ï¼š

#### a. Self-Attention
- **ä½œç”¨**: æŠ“å–å§¿æ€ä¹‹é—´çš„è‡ªæ³¨æ„åŠ›
- **è¾“å…¥/è¾“å‡º**: `(B, num_grasps, d_model)`
- **å®ç°**: EfficientAttention (æ”¯æŒå†…å­˜ä¼˜åŒ–)

#### b. Scene Cross-Attention
- **ä½œç”¨**: ä¸åœºæ™¯ç‰¹å¾çš„äº¤å‰æ³¨æ„åŠ›
- **Query**: æŠ“å–ç‰¹å¾
- **Key/Value**: åœºæ™¯ç‰¹å¾
- **ç»´åº¦**: `(B, num_grasps, d_model)` Ã— `(B, N_points, d_model)`

#### c. Text Cross-Attention (å¯é€‰)
- **ä½œç”¨**: ä¸æ–‡æœ¬ç‰¹å¾çš„äº¤å‰æ³¨æ„åŠ›
- **Query**: æŠ“å–ç‰¹å¾
- **Key/Value**: æ–‡æœ¬ç‰¹å¾
- **ç»´åº¦**: `(B, num_grasps, d_model)` Ã— `(B, 1, d_model)`

#### d. Feed-Forward Network
- **ç»“æ„**:
  ```
  Linear(d_model, 4Ã—d_model)
  â†’ GELU
  â†’ Dropout
  â†’ Linear(4Ã—d_model, d_model)
  â†’ Dropout
  ```

#### AdaptiveLayerNorm
- **åŠŸèƒ½**: æ¡ä»¶å½’ä¸€åŒ–ï¼ŒåŸºäºæ—¶é—´æ­¥åµŒå…¥
- **æœºåˆ¶**:
  ```python
  x_norm = LayerNorm(x)
  scale, shift = Linear(time_emb) â†’ split
  output = x_norm * (1 + scale) + shift
  ```

### 4. è¾“å‡ºæŠ•å½± (OutputProjection)
- **è¾“å…¥**: `(B, num_grasps, d_model)`
- **è¾“å‡º**: `(B, num_grasps, d_x)`
- **ç»“æ„**:
  ```
  LayerNorm
  â†’ Linear(d_model, d_x)
  ```

## å†…å­˜ä¼˜åŒ–ç‰¹æ€§

### 1. EfficientAttention
- åˆ†å—è®¡ç®—æ³¨æ„åŠ›
- å¯é€‰ Flash Attention
- å¯é…ç½® chunk size

### 2. æ¢¯åº¦æ£€æŸ¥ç‚¹ (Gradient Checkpointing)
- é€å±‚æ¢¯åº¦æ£€æŸ¥ç‚¹
- æ˜¾è‘—é™ä½è®­ç»ƒå†…å­˜å ç”¨
- ç•¥å¾®å¢åŠ è®¡ç®—æ—¶é—´

### 3. æ‰¹å¤„ç†ä¼˜åŒ–
- æ”¯æŒå¯å˜é•¿åº¦åºåˆ—
- åŠ¨æ€æ‰¹å¤„ç†å¤§å°è°ƒæ•´
- å†…å­˜ç›‘æ§å’Œè­¦å‘Š

## é…ç½®å‚æ•°

### æ ¸å¿ƒå‚æ•°
| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ |
|------|--------|------|
| `d_model` | 512 | æ¨¡å‹éšè—ç»´åº¦ |
| `num_layers` | 12 | DiT Block å±‚æ•° |
| `num_heads` | 8 | æ³¨æ„åŠ›å¤´æ•° |
| `d_head` | 64 | æ¯ä¸ªæ³¨æ„åŠ›å¤´çš„ç»´åº¦ |
| `time_embed_dim` | 512 | æ—¶é—´æ­¥åµŒå…¥ç»´åº¦ |
| `dropout` | 0.1 | Dropout æ¯”ç‡ |

### æ¡ä»¶å‚æ•°
| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ |
|------|--------|------|
| `use_text_condition` | True | æ˜¯å¦ä½¿ç”¨æ–‡æœ¬æ¡ä»¶ |
| `text_dropout_prob` | 0.1 | æ–‡æœ¬æ¡ä»¶ dropout |
| `use_negative_prompts` | True | æ˜¯å¦æ”¯æŒè´Ÿé¢æç¤º |
| `use_rgb` | True | æ˜¯å¦ä½¿ç”¨ RGB ç‰¹å¾ |
| `use_object_mask` | False | æ˜¯å¦ä½¿ç”¨ç‰©ä½“æ©ç  |

### å†…å­˜ä¼˜åŒ–å‚æ•°
| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ |
|------|--------|------|
| `gradient_checkpointing` | False | æ¢¯åº¦æ£€æŸ¥ç‚¹ |
| `use_flash_attention` | False | Flash Attention |
| `attention_chunk_size` | 512 | æ³¨æ„åŠ›å—å¤§å° |
| `memory_monitoring` | True | å†…å­˜ç›‘æ§ |

## ä¸ Hunyuan-DiT çš„å¯¹æ¯”

### ç›¸ä¼¼ä¹‹å¤„
1. âœ… ä½¿ç”¨ AdaptiveLayerNorm æ³¨å…¥æ—¶é—´æ­¥ä¿¡æ¯
2. âœ… å¤šé‡äº¤å‰æ³¨æ„åŠ›æœºåˆ¶
3. âœ… æ®‹å·®è¿æ¥è®¾è®¡
4. âœ… Transformer æ¶æ„

### å·®å¼‚
1. ğŸ¯ **ä»»åŠ¡å®šä½**: Hunyuan-DiT ç”¨äºå›¾åƒç”Ÿæˆï¼Œæœ¬æ¨¡å‹ç”¨äºæŠ“å–å§¿æ€ç”Ÿæˆ
2. ğŸ¯ **è¾“å…¥æ ¼å¼**: 
   - Hunyuan: å›¾åƒ patches
   - æœ¬æ¨¡å‹: æŠ“å–å§¿æ€ + ç‚¹äº‘åœºæ™¯
3. ğŸ¯ **æ¡ä»¶æœºåˆ¶**:
   - Hunyuan: ä¸»è¦æ–‡æœ¬æ¡ä»¶
   - æœ¬æ¨¡å‹: åœºæ™¯ç‚¹äº‘ + æ–‡æœ¬åŒé‡æ¡ä»¶
4. ğŸ¯ **åºåˆ—é•¿åº¦**: æœ¬æ¨¡å‹æ”¯æŒå¯å˜é•¿åº¦æŠ“å–åºåˆ—

## ä½¿ç”¨ç¤ºä¾‹

### å‰å‘ä¼ æ’­
```python
# è¾“å…¥
x_t = torch.randn(B, num_grasps, d_x)  # å™ªå£°æŠ“å–å§¿æ€
ts = torch.randint(0, 1000, (B,))       # æ—¶é—´æ­¥
data = {
    'scene_pc': torch.randn(B, N, 6),   # åœºæ™¯ç‚¹äº‘
    'positive_prompt': ["grasp the cup", ...],  # æ–‡æœ¬æç¤º
}

# æ¡ä»¶ç¼–ç 
condition = model.condition(data)
data.update(condition)

# å‰å‘ä¼ æ’­
noise_pred = model(x_t, ts, data)
```

### æ¨ç†æ¨¡å¼
```python
model.eval()
model.optimize_for_inference()  # åº”ç”¨æ¨ç†ä¼˜åŒ–
```

### è®­ç»ƒæ¨¡å¼
```python
model.train()
model.optimize_for_training()  # åº”ç”¨è®­ç»ƒä¼˜åŒ–
```

## æ–‡ä»¶ä½ç½®

- **ä¸»æ¨¡å‹**: `/models/decoder/dit.py`
- **é…ç½®éªŒè¯**: `/models/decoder/dit_config_validation.py`
- **è¾“å…¥éªŒè¯**: `/models/decoder/dit_validation.py`
- **å†…å­˜ä¼˜åŒ–**: `/models/decoder/dit_memory_optimization.py`

