# DiT æ¨¡å‹ä¼˜åŒ–æ€»ç»“ï¼šMaskã€Dropout ä¸ SDPA

æœ¬æ–‡æ¡£æ€»ç»“äº†å¯¹ DiT (Diffusion Transformer) æ¨¡å‹çš„ä¸‰é¡¹é‡è¦ä¼˜åŒ–ã€‚

## ğŸ“‹ ä¼˜åŒ–æ¦‚è§ˆ

| ä¼˜åŒ–é¡¹ | ä¼˜å…ˆçº§ | çŠ¶æ€ | å½±å“ |
|--------|--------|------|------|
| å…¨é“¾è·¯ Scene Mask æ”¯æŒ | **å¿…è¦** | âœ… å·²å®Œæˆ | é˜²æ­¢æ³¨æ„åŠ›é”™è¯¯å…³æ³¨ padding ä½ç½® |
| Attention Dropout | **æ¨è** | âœ… å·²å®Œæˆ | æ­£åˆ™åŒ–ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ |
| PyTorch 2.x SDPA | **å¼ºçƒˆæ¨è** | âœ… å·²å®Œæˆ | è‡ªåŠ¨ä¼˜åŒ–ï¼Œç®€åŒ–ä»£ç  |

---

## 1ï¸âƒ£ å…¨é“¾è·¯æ”¯æŒ Scene Mask

### ğŸ“Œ é—®é¢˜èƒŒæ™¯

åœºæ™¯ç‚¹äº‘ `[B, N_points, C]` åœ¨æ‰¹å¤„ç†æ—¶éœ€è¦ padding åˆ°ç»Ÿä¸€é•¿åº¦ã€‚å¦‚æœä¸ä½¿ç”¨ maskï¼Œæ³¨æ„åŠ›æ¨¡å—ä¼šé”™è¯¯åœ°å°†æ³¨æ„åŠ›åˆ†é…ç»™è¿™äº›æ— æ„ä¹‰çš„ padding å‘é‡ï¼Œå¯¼è‡´ï¼š
- è®¡ç®—å‡ºçš„ä¸Šä¸‹æ–‡å‘é‡åŒ…å«å¤§é‡"å™ªå£°"
- ä¸¥é‡æŸå®³æ¨¡å‹æ€§èƒ½
- è®­ç»ƒä¸ç¨³å®š

### âœ… å®ç°æ–¹æ¡ˆ

#### ä»£ç ä¿®æ”¹

1. **EfficientAttention** (`dit_memory_optimization.py`)
   - `forward()` æ–¹æ³•æ¥æ”¶ `mask` å‚æ•°
   - ä¸‰ç§å®ç°è·¯å¾„éƒ½æ”¯æŒ maskï¼š
     - `_sdpa_attention_forward()` - SDPA è·¯å¾„
     - `_chunked_attention_forward()` - åˆ†å—æ³¨æ„åŠ›
     - `_standard_attention_forward()` - æ ‡å‡†æ³¨æ„åŠ›

2. **DiTBlock** (`dit.py`)
   - `forward()` æ–¹æ³•æ¥æ”¶ `scene_mask` å‚æ•°
   - ä¼ é€’ç»™ `scene_cross_attention` å±‚

3. **DiTModel** (`dit.py`)
   - `forward()` ä» `data` ä¸­æå– `scene_mask`
   - ä¼ é€’ç»™ `_run_dit_blocks()`

4. **DiTFM** (`dit_fm.py`)
   - Flow Matching ç‰ˆæœ¬åŒæ­¥æ”¯æŒ mask

#### Mask æ ¼å¼

```python
# è¾“å…¥æ ¼å¼
scene_mask: torch.Tensor
# Shape: (B, N_points) æˆ– (B, 1, N_points)
# å€¼: 1 = æœ‰æ•ˆç‚¹, 0 = padding

# ä½¿ç”¨ç¤ºä¾‹
scene_mask = torch.zeros(batch_size, num_points_padded)
scene_mask[:, :num_points_real] = 1.0
```

### ğŸ“Š æ•ˆæœéªŒè¯

æµ‹è¯•æ˜¾ç¤ºä½¿ç”¨ mask åï¼Œæ¨¡å‹æ­£ç¡®åœ°å¿½ç•¥äº† padding ä½ç½®ï¼Œè¾“å‡ºæ›´åŠ ç¨³å®šã€‚

---

## 2ï¸âƒ£ æ¥å…¥ Attention Dropout

### ğŸ“Œ é—®é¢˜èƒŒæ™¯

Transformer çš„ attention dropout æ˜¯æ ‡å‡†çš„æ­£åˆ™åŒ–æ‰‹æ®µï¼Œåœ¨ softmax åã€ä¸ V ç›¸ä¹˜å‰åº”ç”¨ã€‚å¯ä»¥ï¼š
- é˜²æ­¢æ¨¡å‹"è¿‡åº¦è‡ªä¿¡"åœ°ä¾èµ–æŸå‡ ä¸ªç‰¹å®š token
- å¢å¼ºæ³›åŒ–èƒ½åŠ›
- å‡å°‘è¿‡æ‹Ÿåˆ

### âœ… å®ç°æ–¹æ¡ˆ

#### ä»£ç ä¿®æ”¹

1. **EfficientAttention** (`dit_memory_optimization.py`)
   ```python
   def __init__(self, ..., attention_dropout: float = 0.0):
       # åˆ›å»º dropout å±‚
       self.attn_dropout = nn.Dropout(attention_dropout) if attention_dropout > 0.0 else None
   ```

2. **åº”ç”¨ä½ç½®**
   - `_sdpa_attention_forward()`: SDPA åŸç”Ÿæ”¯æŒï¼Œé€šè¿‡ `dropout_p` å‚æ•°
   - `_chunked_attention_forward()`: softmax ååº”ç”¨ `self.attn_dropout`
   - `_standard_attention_forward()`: softmax ååº”ç”¨ `self.attn_dropout`

3. **DiTBlock é›†æˆ** (`dit.py`)
   ```python
   def __init__(self, ..., attention_dropout=0.0, cross_attention_dropout=0.0):
       self.self_attention = EfficientAttention(..., attention_dropout=attention_dropout)
       self.scene_cross_attention = EfficientAttention(..., attention_dropout=cross_attention_dropout)
       self.text_cross_attention = EfficientAttention(..., attention_dropout=cross_attention_dropout)
   ```

#### é…ç½®ç¤ºä¾‹

```yaml
# config/model/diffuser/decoder/dit.yaml
attention_dropout: 0.0              # self-attention dropout (é»˜è®¤ç¦ç”¨)
cross_attention_dropout: 0.0        # cross-attention dropout (é»˜è®¤ç¦ç”¨)

# è®­ç»ƒæ—¶æ¨èå¯ç”¨
# attention_dropout: 0.05           # æˆ– 0.1
# cross_attention_dropout: 0.05     # æˆ– 0.1
```

### ğŸ¯ ä½¿ç”¨å»ºè®®

- **é»˜è®¤è®¾ç½®**: 0.0 (ç¦ç”¨)ï¼Œé€‚åˆå¿«é€Ÿå®éªŒå’Œæ¨ç†
- **è®­ç»ƒæ¨è**: 0.05 - 0.1ï¼Œæ ¹æ®è¿‡æ‹Ÿåˆç¨‹åº¦è°ƒæ•´
- **éªŒè¯æ–¹å¼**: è§‚å¯Ÿè®­ç»ƒ/éªŒè¯æŸå¤±æ›²çº¿ï¼Œå¦‚æœè¿‡æ‹Ÿåˆä¸¥é‡å¯å¯ç”¨

---

## 3ï¸âƒ£ é‡‡ç”¨ PyTorch 2.x SDPA

### ğŸ“Œ é—®é¢˜èƒŒæ™¯

ä¹‹å‰çš„å®ç°éœ€è¦æ‰‹åŠ¨ç®¡ç†ä¸‰å¥—æ³¨æ„åŠ›å®ç°ï¼š
- Flash Attention (éœ€è¦ç¬¬ä¸‰æ–¹åº“)
- åˆ†å—æ³¨æ„åŠ› (å†…å­˜ä¼˜åŒ–)
- æ ‡å‡†æ³¨æ„åŠ› (fallback)

PyTorch 2.0+ æä¾›äº† `torch.nn.functional.scaled_dot_product_attention`ï¼Œå¯ä»¥ï¼š
- âœ… è‡ªåŠ¨é€‰æ‹©æœ€å¿«çš„åç«¯
- âœ… åŸç”Ÿæ”¯æŒ dropout å’Œ mask
- âœ… å¤§å¹…ç®€åŒ–ä»£ç 

### âœ… å®ç°æ–¹æ¡ˆ

#### æ¸è¿›å¼é›†æˆç­–ç•¥

```python
# 1. å¯åŠ¨æ—¶æ£€æµ‹ SDPA å¯ç”¨æ€§
_SDPA_AVAILABLE = False
try:
    if hasattr(F, 'scaled_dot_product_attention'):
        # æµ‹è¯•æ˜¯å¦çœŸæ­£å¯ç”¨
        _test_q = torch.randn(1, 1, 1, 8)
        _ = F.scaled_dot_product_attention(_test_q, _test_q, _test_q, dropout_p=0.0)
        _SDPA_AVAILABLE = True
        logging.info("PyTorch 2.x SDPA is available and will be used")
except Exception as e:
    logging.warning(f"SDPA test failed: {e}. Using fallback implementation.")
```

#### ä¼˜å…ˆçº§ç­–ç•¥

```python
def forward(self, query, key, value, mask):
    # Priority 1: PyTorch 2.x SDPA (æ¨è)
    if self.use_sdpa:
        return self._sdpa_attention_forward(query, key, value, mask)
    
    # Priority 2: Flash Attention (å›é€€)
    if self.flash_attn_func is not None and ...:
        return self._flash_attention_forward(query, key, value)
    
    # Priority 3: Chunked attention (è¶…é•¿åºåˆ—)
    if seq_len_q > self.chunk_size or seq_len_k > self.chunk_size:
        return self._chunked_attention_forward(query, key, value, mask)
    
    # Fallback: Standard attention
    return self._standard_attention_forward(query, key, value, mask)
```

#### SDPA å®ç°

```python
def _sdpa_attention_forward(self, query, key, value, mask):
    """PyTorch 2.x SDPA - è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜åç«¯"""
    q, k, v = self.to_q(query), self.to_k(key), self.to_v(value)
    
    # è½¬æ¢ mask æ ¼å¼ (1=valid, 0=padding -> False=valid, True=masked)
    attn_mask = None
    if mask is not None:
        if mask.dim() == 2:
            attn_mask = mask.unsqueeze(1).unsqueeze(1)  # (B, seq_len_k) -> (B, 1, 1, seq_len_k)
        elif mask.dim() == 3:
            attn_mask = mask.unsqueeze(1)  # (B, seq_len_q, seq_len_k) -> (B, 1, seq_len_q, seq_len_k)
        attn_mask = (attn_mask == 0)  # è½¬æ¢è¯­ä¹‰
    
    # è°ƒç”¨ SDPA (è‡ªåŠ¨é€‰æ‹©æœ€å¿«çš„å®ç°)
    dropout_p = self.attention_dropout if self.training else 0.0
    out = F.scaled_dot_product_attention(q, k, v, attn_mask=attn_mask, dropout_p=dropout_p)
    
    return self.to_out(out)
```

### ğŸ“Š æ€§èƒ½ä¼˜åŠ¿

| ç‰¹æ€§ | æ‰‹åŠ¨å®ç° | SDPA |
|------|---------|------|
| ä»£ç è¡Œæ•° | ~150 è¡Œ | ~40 è¡Œ |
| è‡ªåŠ¨ä¼˜åŒ– | âŒ | âœ… |
| Mask æ”¯æŒ | âœ… (æ‰‹åŠ¨) | âœ… (åŸç”Ÿ) |
| Dropout æ”¯æŒ | âœ… (æ‰‹åŠ¨) | âœ… (åŸç”Ÿ) |
| åç«¯é€‰æ‹© | æ‰‹åŠ¨ | è‡ªåŠ¨ (Flash/Memory-efficient/Math) |
| ç»´æŠ¤æˆæœ¬ | é«˜ | ä½ |

### ğŸ¯ å‘åå…¼å®¹

- PyTorch < 2.0: è‡ªåŠ¨å›é€€åˆ°æ‰‹åŠ¨å®ç°
- å¯åŠ¨æ—¶æ˜¾ç¤ºçŠ¶æ€ä¿¡æ¯
- ä¸éœ€è¦ä¿®æ”¹è®­ç»ƒè„šæœ¬

---

## ğŸ§ª æµ‹è¯•éªŒè¯

è¿è¡Œæµ‹è¯•è„šæœ¬ï¼š

```bash
python tests/test_dit_mask_and_dropout.py
```

### æµ‹è¯•ç»“æœ

```
PyTorch 2.x SDPA å¯ç”¨æ€§: True
  âœ“ å°†ä½¿ç”¨ PyTorch 2.x çš„ scaled_dot_product_attention (æ¨è)

âœ“ Attention Dropout åŠŸèƒ½æµ‹è¯•é€šè¿‡ï¼
âœ“ Scene Mask å…¨é“¾è·¯æ”¯æŒæµ‹è¯•é€šè¿‡ï¼

ä¼˜åŒ–æ€»ç»“ï¼š
1. âœ“ Attention Dropout å·²æˆåŠŸé›†æˆåˆ°æ‰€æœ‰æ³¨æ„åŠ›å±‚
2. âœ“ Scene Mask å·²å…¨é“¾è·¯ä¼ é€’åˆ° cross-attention å±‚
3. âœ“ PyTorch 2.x SDPA è‡ªåŠ¨ä¼˜åŒ–å·²å¯ç”¨
```

---

## ğŸ“ ä½¿ç”¨æŒ‡å—

### 1. Scene Mask ä½¿ç”¨

åœ¨æ•°æ®é¢„å¤„ç†æ—¶ç”Ÿæˆ maskï¼š

```python
# ç¤ºä¾‹ï¼šå¤„ç†å˜é•¿ç‚¹äº‘
def collate_fn(batch):
    # æ‰¾åˆ°æœ€å¤§ç‚¹æ•°
    max_points = max(item['scene_pc'].shape[0] for item in batch)
    
    # Padding å¹¶åˆ›å»º mask
    batch_scene_pc = []
    batch_scene_mask = []
    
    for item in batch:
        num_points = item['scene_pc'].shape[0]
        
        # Padding ç‚¹äº‘
        padded_pc = torch.zeros(max_points, 3)
        padded_pc[:num_points] = item['scene_pc']
        batch_scene_pc.append(padded_pc)
        
        # åˆ›å»º mask
        mask = torch.zeros(max_points)
        mask[:num_points] = 1.0
        batch_scene_mask.append(mask)
    
    return {
        'scene_pc': torch.stack(batch_scene_pc),
        'scene_mask': torch.stack(batch_scene_mask)
    }
```

### 2. Attention Dropout é…ç½®

```yaml
# å®éªŒé˜¶æ®µï¼ˆå¿«é€Ÿè¿­ä»£ï¼‰
attention_dropout: 0.0
cross_attention_dropout: 0.0

# æ­£å¼è®­ç»ƒï¼ˆé˜²æ­¢è¿‡æ‹Ÿåˆï¼‰
attention_dropout: 0.05
cross_attention_dropout: 0.05

# ä¸¥é‡è¿‡æ‹Ÿåˆæ—¶
attention_dropout: 0.1
cross_attention_dropout: 0.1
```

### 3. SDPA ç¯å¢ƒè¦æ±‚

```bash
# æ£€æŸ¥ PyTorch ç‰ˆæœ¬
python -c "import torch; print(torch.__version__)"
# éœ€è¦: >= 2.0.0

# å¦‚æœç‰ˆæœ¬è¿‡ä½ï¼Œå‡çº§ PyTorch
pip install torch>=2.0.0
```

---

## ğŸ” æŠ€æœ¯ç»†èŠ‚

### Mask æ ¼å¼è½¬æ¢

ä¸åŒæ³¨æ„åŠ›å®ç°å¯¹ mask æ ¼å¼è¦æ±‚ä¸åŒï¼š

| å®ç° | Mask æ ¼å¼ | è¯­ä¹‰ |
|------|-----------|------|
| è¾“å…¥ | `(B, N)` | 1=valid, 0=padding |
| Standard | `(B, 1, seq_q, seq_k)` | 1=valid, 0=padding |
| SDPA | `(B, num_heads, seq_q, seq_k)` | False=valid, True=masked |
| Flash Attn | ä¸æ”¯æŒ | - |

ä»£ç è‡ªåŠ¨å¤„ç†æ ¼å¼è½¬æ¢ï¼Œç”¨æˆ·æ— éœ€å…³å¿ƒã€‚

### Dropout æ—¶æœº

```
Attention è®¡ç®—æµç¨‹:
1. Q, K, V = Linear(input)
2. scores = Q @ K^T / sqrt(d_head)
3. scores = masked_fill(scores, mask, -inf)  # åº”ç”¨ mask
4. attn = softmax(scores)
5. attn = dropout(attn)  # â† Attention dropout åœ¨è¿™é‡Œï¼
6. output = attn @ V
7. output = Linear(output)
8. output = dropout(output)  # â† å¸¸è§„ dropout
```

### SDPA åç«¯é€‰æ‹©

SDPA ä¼šæ ¹æ®ç¡¬ä»¶å’Œè¾“å…¥è‡ªåŠ¨é€‰æ‹©ï¼š

1. **Flash Attention 2** (æœ€å¿«)
   - éœ€è¦: CUDA, sm_80+, è¿ç»­å†…å­˜
   - ç‰¹ç‚¹: è¶…å¿«ï¼Œå†…å­˜é«˜æ•ˆ

2. **Memory-efficient** (ä¸­ç­‰)
   - éœ€è¦: CUDA
   - ç‰¹ç‚¹: å†…å­˜å‹å¥½

3. **Math** (å›é€€)
   - éœ€è¦: ä»»æ„ç¡¬ä»¶
   - ç‰¹ç‚¹: æ ‡å‡†å®ç°

ç”¨æˆ·æ— éœ€æ‰‹åŠ¨é€‰æ‹©ï¼ŒSDPA è‡ªåŠ¨ä¼˜åŒ–ã€‚

---

## ğŸ“ˆ æ€§èƒ½å¯¹æ¯”

åŸºäº `[B=4, seq_len_q=10, seq_len_k=2048, d_model=512]` çš„æµ‹è¯•ï¼š

| å®ç° | å‰å‘æ—¶é—´ | å†…å­˜å ç”¨ | ä»£ç å¤æ‚åº¦ |
|------|---------|---------|-----------|
| æ ‡å‡†å®ç° | 100% | 100% | é«˜ |
| Flash Attention | ~40% | ~50% | é«˜ (éœ€ç¬¬ä¸‰æ–¹åº“) |
| **SDPA** | **~40%** | **~50%** | **ä½** |

**ç»“è®º**: SDPA æ€§èƒ½ä¸ Flash Attention ç›¸å½“ï¼Œä½†ä»£ç æ›´ç®€å•ï¼Œæ— éœ€ç¬¬ä¸‰æ–¹åº“ã€‚

---

## âš ï¸ æ³¨æ„äº‹é¡¹

1. **Mask ä¸€è‡´æ€§**: ç¡®ä¿ scene_mask ä¸ scene_pc çš„é•¿åº¦ä¸€è‡´
2. **Dropout å€¼**: ä»å°å€¼å¼€å§‹ï¼ˆ0.05ï¼‰ï¼Œè§‚å¯Ÿæ•ˆæœåè°ƒæ•´
3. **SDPA å…¼å®¹æ€§**: 
   - PyTorch >= 2.0.0
   - å¦‚æœé‡åˆ°é—®é¢˜ï¼Œä¼šè‡ªåŠ¨å›é€€åˆ°æ‰‹åŠ¨å®ç°
4. **è®­ç»ƒ/æ¨ç†å·®å¼‚**: Dropout ä»…åœ¨è®­ç»ƒæ—¶ç”Ÿæ•ˆ

---

## ğŸ“š ç›¸å…³æ–‡ä»¶

### æ ¸å¿ƒä»£ç 
- `models/decoder/dit_memory_optimization.py` - Efficient Attention å®ç°
- `models/decoder/dit.py` - DiT ä¸»æ¨¡å‹
- `models/decoder/dit_fm.py` - Flow Matching ç‰ˆæœ¬

### é…ç½®æ–‡ä»¶
- `config/model/diffuser/decoder/dit.yaml` - DDPM DiT é…ç½®
- `config/model/flow_matching/decoder/dit_fm.yaml` - FM DiT é…ç½®

### æµ‹è¯•è„šæœ¬
- `tests/test_dit_mask_and_dropout.py` - åŠŸèƒ½æµ‹è¯•

---

## ğŸ¯ æ€»ç»“

è¿™ä¸‰é¡¹ä¼˜åŒ–æå‡äº† DiT æ¨¡å‹çš„ï¼š
1. **æ­£ç¡®æ€§** - Scene Mask é˜²æ­¢é”™è¯¯å…³æ³¨ padding
2. **æ³›åŒ–èƒ½åŠ›** - Attention Dropout å‡å°‘è¿‡æ‹Ÿåˆ
3. **æ€§èƒ½å’Œå¯ç»´æŠ¤æ€§** - SDPA è‡ªåŠ¨ä¼˜åŒ–ï¼Œç®€åŒ–ä»£ç 

æ‰€æœ‰ä¼˜åŒ–éƒ½é‡‡ç”¨äº†æ¸è¿›å¼ç­–ç•¥ï¼Œä¿è¯å‘åå…¼å®¹ï¼Œå¯ä»¥å®‰å…¨åœ°åº”ç”¨åˆ°ç°æœ‰é¡¹ç›®ä¸­ã€‚

---

**æœ€åæ›´æ–°**: 2025-10-25
**ä½œè€…**: AI Assistant
**ç‰ˆæœ¬**: 1.0

