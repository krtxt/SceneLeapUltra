# xiantuo@Oppenheimer:~$ ssh -N -o ServerAliveInterval=60 -o ServerAliveCountMax=3 -o ExitOnForwardFailure=yes -R 1101:localhost:22 root@38.150.2.96 -p 38268

#!/bin/bash

# SceneLeapPro åˆ†å¸ƒå¼è®­ç»ƒè„šæœ¬ (æ¨èä½¿ç”¨)
# æ”¯æŒå•GPUå’Œå¤šGPUè®­ç»ƒï¼Œä½¿ç”¨æ ‡å‡†Hydraå‘½ä»¤è¡Œè¯­æ³•
#
# ğŸš€ å¿«é€Ÿå¼€å§‹:
#   è‡ªåŠ¨æ£€æµ‹GPU: ./train_distributed.sh
#   æŒ‡å®šGPUæ•°é‡: ./train_distributed.sh --gpus 4
#   è°ƒæ•´å‚æ•°:    ./train_distributed.sh --gpus 4 batch_size=128 model.optimizer.lr=0.002
#
# ğŸ“Š Batch Size é€»è¾‘:
#   batch_size=128 è¡¨ç¤ºå…¨å±€æœ‰æ•ˆbatch_size
#   4GPUè®­ç»ƒæ—¶ï¼Œæ¯ä¸ªGPUå¤„ç† 128Ã·4=32 ä¸ªæ ·æœ¬
#
# ğŸ¯ Learning Rate é€»è¾‘:
#   model.optimizer.lr=0.001 è¡¨ç¤ºåŸºç¡€å­¦ä¹ ç‡
#   ç³»ç»Ÿä¼šæ ¹æ®GPUæ•°é‡è‡ªåŠ¨ç¼©æ”¾ (é»˜è®¤sqrtç¼©æ”¾)
#
# ğŸ“ é…ç½®ä¼˜å…ˆçº§:
#   å‘½ä»¤è¡ŒHydraå‚æ•° > config.yaml > é»˜è®¤å€¼

set -e

# åˆ†å¸ƒå¼è®­ç»ƒä¸“ç”¨å‚æ•°ï¼ˆéHydraå‚æ•°ï¼‰
GPUS=""
NODES=1
MASTER_ADDR="localhost"
MASTER_PORT=29501
JOB_ID="sceneleap_$(date +%Y%m%d_%H%M%S)"

# Hydraé…ç½®è¦†ç›–å‚æ•°
HYDRA_OVERRIDES=""

# è§£æå‘½ä»¤è¡Œå‚æ•°
while [[ $# -gt 0 ]]; do
    case $1 in
        --gpus)
            GPUS="$2"
            shift 2
            ;;
        --nodes)
            NODES="$2"
            shift 2
            ;;
        --master_addr)
            MASTER_ADDR="$2"
            shift 2
            ;;
        --master_port)
            MASTER_PORT="$2"
            shift 2
            ;;
        --job_id)
            JOB_ID="$2"
            shift 2
            ;;
        --help)
            echo "SceneLeapPro åˆ†å¸ƒå¼è®­ç»ƒè„šæœ¬"
            echo ""
            echo "åˆ†å¸ƒå¼å‚æ•°:"
            echo "  --gpus N              ä½¿ç”¨Nä¸ªGPU (é»˜è®¤: è‡ªåŠ¨æ£€æµ‹)"
            echo "  --nodes N             èŠ‚ç‚¹æ•°é‡ (é»˜è®¤: 1)"
            echo "  --master_addr ADDR    ä¸»èŠ‚ç‚¹åœ°å€ (å¤šèŠ‚ç‚¹å¿…éœ€)"
            echo "  --master_port PORT    ä¸»èŠ‚ç‚¹ç«¯å£ (é»˜è®¤: 29500, è‡ªåŠ¨æ£€æµ‹å¯ç”¨ç«¯å£)"
            echo "  --job_id ID           ä½œä¸šID (é»˜è®¤: è‡ªåŠ¨ç”Ÿæˆ)"
            echo ""
            echo "Hydraé…ç½®è¦†ç›– (ç›´æ¥ä¼ é€’ç»™train_lightning.py):"
            echo "  batch_size=N          æ‰¹æ¬¡å¤§å°"
            echo "  model.optimizer.lr=X  å­¦ä¹ ç‡"
            echo "  epochs=N              è®­ç»ƒè½®æ•°"
            echo "  save_root=PATH        ä¿å­˜è·¯å¾„"
            echo "  distributed.lr_scaling=METHOD  å­¦ä¹ ç‡ç¼©æ”¾æ–¹æ³•"
            echo ""
            echo "ç¤ºä¾‹:"
            echo "  ./train_distributed.sh --gpus 4 batch_size=128 model.optimizer.lr=0.002"
            echo "  ./train_distributed.sh batch_size=64 epochs=500 save_root=experiments/test"
            exit 0
            ;;
        *)
            # æ‰€æœ‰å…¶ä»–å‚æ•°éƒ½ä½œä¸ºHydraé…ç½®è¦†ç›–
            HYDRA_OVERRIDES="$HYDRA_OVERRIDES $1"
            shift
            ;;
    esac
done

# æŸ¥æ‰¾å¯ç”¨ç«¯å£çš„å‡½æ•°
find_free_port() {
    local start_port=$1
    local port=$start_port
    while netstat -ln 2>/dev/null | grep -q ":$port " || ss -ln 2>/dev/null | grep -q ":$port "; do
        port=$((port + 1))
        # é¿å…æ— é™å¾ªç¯
        if [ $port -gt $((start_port + 100)) ]; then
            echo "é”™è¯¯: æ— æ³•æ‰¾åˆ°å¯ç”¨ç«¯å£ (å°è¯•äº† $start_port åˆ° $port)"
            exit 1
        fi
    done
    echo $port
}

# æ£€æµ‹GPUæ•°é‡
if [ -z "$GPUS" ]; then
    if command -v nvidia-smi &> /dev/null; then
        GPUS=$(nvidia-smi --list-gpus | wc -l)
        echo "è‡ªåŠ¨æ£€æµ‹åˆ° $GPUS ä¸ªGPU"
    else
        echo "é”™è¯¯: æ— æ³•æ£€æµ‹GPUæ•°é‡ï¼Œè¯·æ‰‹åŠ¨æŒ‡å®š --gpus å‚æ•°"
        exit 1
    fi
fi

# æ£€æŸ¥GPUæ•°é‡
if [ "$GPUS" -lt 1 ]; then
    echo "é”™è¯¯: GPUæ•°é‡å¿…é¡»å¤§äº0"
    exit 1
fi

# è‡ªåŠ¨æŸ¥æ‰¾å¯ç”¨ç«¯å£
if netstat -ln 2>/dev/null | grep -q ":$MASTER_PORT " || ss -ln 2>/dev/null | grep -q ":$MASTER_PORT "; then
    echo "âš ï¸  ç«¯å£ $MASTER_PORT å·²è¢«å ç”¨ï¼Œè‡ªåŠ¨æŸ¥æ‰¾å¯ç”¨ç«¯å£..."
    MASTER_PORT=$(find_free_port $MASTER_PORT)
    echo "âœ… ä½¿ç”¨ç«¯å£: $MASTER_PORT"
fi

# è‡ªåŠ¨æ·»åŠ åˆ†å¸ƒå¼é…ç½®åˆ°Hydraè¦†ç›–å‚æ•°
if [ "$GPUS" -gt 1 ]; then
    HYDRA_OVERRIDES="$HYDRA_OVERRIDES distributed.enabled=true"
    HYDRA_OVERRIDES="$HYDRA_OVERRIDES distributed.devices=$GPUS"
    HYDRA_OVERRIDES="$HYDRA_OVERRIDES trainer.devices=$GPUS"
    HYDRA_OVERRIDES="$HYDRA_OVERRIDES trainer.strategy=ddp"
fi

echo "ğŸš€ åˆ†å¸ƒå¼è®­ç»ƒé…ç½®:"
echo "  GPUæ•°é‡: $GPUS"
echo "  èŠ‚ç‚¹æ•°é‡: $NODES"
echo "  ä¸»èŠ‚ç‚¹åœ°å€: $MASTER_ADDR"
echo "  ä¸»èŠ‚ç‚¹ç«¯å£: $MASTER_PORT"
echo "  ä½œä¸šID: $JOB_ID"
echo "  CUDAè®¾å¤‡: $CUDA_VISIBLE_DEVICES"
echo ""
echo "ğŸ“ Hydraé…ç½®è¦†ç›–:"
echo "  $HYDRA_OVERRIDES"
echo ""

# æ£€æŸ¥æ˜¯å¦åœ¨SLURMç¯å¢ƒä¸­
if [ ! -z "$SLURM_JOB_ID" ]; then
    echo "æ£€æµ‹åˆ°SLURMç¯å¢ƒï¼Œä½œä¸šID: $SLURM_JOB_ID"
    
    # åœ¨SLURMç¯å¢ƒä¸­ä½¿ç”¨srun
    srun --ntasks-per-node=$GPUS \
         --nodes=$NODES \
         --gres=gpu:$GPUS \
         python train_lightning.py \
         distributed.num_nodes=$NODES \
         $HYDRA_OVERRIDES

elif [ ! -z "$LOCAL_RANK" ]; then
    echo "æ£€æµ‹åˆ°torchrunç¯å¢ƒ"
    
    # å·²ç»åœ¨torchrunç¯å¢ƒä¸­ï¼Œç›´æ¥è¿è¡Œ
    python train_lightning.py \
        distributed.num_nodes=$NODES \
        $HYDRA_OVERRIDES

else
    echo "ä½¿ç”¨torchrunå¯åŠ¨åˆ†å¸ƒå¼è®­ç»ƒ"
    
    # æ„å»ºtorchrunå‘½ä»¤
    TORCHRUN_CMD="torchrun --nproc_per_node=$GPUS --nnodes=$NODES"
    
    # æ·»åŠ ç«¯å£é…ç½®ï¼ˆå•èŠ‚ç‚¹å’Œå¤šèŠ‚ç‚¹éƒ½éœ€è¦ï¼‰
    TORCHRUN_CMD="$TORCHRUN_CMD --master_port=$MASTER_PORT"
    
    # å¤šèŠ‚ç‚¹é…ç½®
    if [ "$NODES" -gt 1 ]; then
        TORCHRUN_CMD="$TORCHRUN_CMD --rdzv_endpoint=$MASTER_ADDR:$MASTER_PORT"
        TORCHRUN_CMD="$TORCHRUN_CMD --rdzv_backend=c10d"
        TORCHRUN_CMD="$TORCHRUN_CMD --rdzv_id=$JOB_ID"
    fi
    
    # æ·»åŠ è®­ç»ƒè„šæœ¬å’Œå‚æ•°
    TORCHRUN_CMD="$TORCHRUN_CMD train_lightning.py"
    TORCHRUN_CMD="$TORCHRUN_CMD distributed.num_nodes=$NODES"
    TORCHRUN_CMD="$TORCHRUN_CMD $HYDRA_OVERRIDES"
    
    echo "æ‰§è¡Œå‘½ä»¤: $TORCHRUN_CMD"
    echo ""
    
    # æ‰§è¡Œè®­ç»ƒ
    eval $TORCHRUN_CMD
fi

echo "åˆ†å¸ƒå¼è®­ç»ƒå®Œæˆ"


# bash train_distributed.sh --gpus 1 save_root="./experiments/testtest_ptv3" model.steps=100 batch_size=40 'checkpoint_path="experiments/testtest_ptv3/checkpoints/epoch=34-val_loss=56.94.ckpt"'