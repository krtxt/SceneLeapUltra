#!/usr/bin/env python3
"""
é¢„æµ‹æŠ“å–å§¿æ€ä¸çœŸå®æŠ“å–å§¿æ€å¯¹æ¯”å¯è§†åŒ–è„šæœ¬

ä¸»è¦åŠŸèƒ½ï¼š
1. åŠ è½½é¢„è®­ç»ƒçš„DDPMLightningæ¨¡å‹
2. å¯¹æ•°æ®é›†ä¸­çš„åœºæ™¯è¿›è¡ŒæŠ“å–é¢„æµ‹
3. å°†é¢„æµ‹çš„æŠ“å–å§¿æ€ä¸çœŸå®æ ‡æ³¨çš„æŠ“å–å§¿æ€è¿›è¡ŒåŒ¹é…
4. åœ¨åŒä¸€ä¸ª3Dåœºæ™¯ä¸­åŒæ—¶æ˜¾ç¤ºé¢„æµ‹å’ŒçœŸå®çš„æŠ“å–å§¿æ€
5. è®¡ç®—å¹¶è¾“å‡ºå®šé‡å·®è·æŒ‡æ ‡ï¼ˆä½ç½®è¯¯å·®ã€æ—‹è½¬è¯¯å·®ç­‰ï¼‰
6. æä¾›ç»Ÿè®¡åˆ†æç»“æœ

å¯è§†åŒ–ç»„ä»¶ï¼š
- çº¢è‰²ç‚¹äº‘ï¼šç›®æ ‡ç‰©ä½“ç‚¹äº‘
- ç°è‰²ç‚¹äº‘ï¼šèƒŒæ™¯ç‚¹äº‘  
- ç»¿è‰²meshï¼šç›®æ ‡ç‰©ä½“mesh
- è“è‰²ç³»meshï¼šé¢„æµ‹çš„æŠ“å–å§¿æ€
- çº¢è‰²ç³»meshï¼šçœŸå®çš„æŠ“å–å§¿æ€
- RGBåæ ‡è½´ï¼šä¸–ç•Œåæ ‡ç³»å‚è€ƒ
"""

import os
import sys
import torch
import numpy as np
import open3d as o3d
from typing import Optional, Tuple, List, Dict
from pathlib import Path
import logging
from omegaconf import OmegaConf

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from datasets.sceneleapplus_dataset import SceneLeapPlusDataset
from models.diffuser_lightning import DDPMLightning
from utils.hand_model import HandModel, HandModelType
from utils.hand_helper import process_hand_pose_test
# from models.backbone.pointnet2 import farthest_point_sample

def load_pretrained_model(checkpoint_path: str, config_path: Optional[str] = None) -> DDPMLightning:
    """
    åŠ è½½é¢„è®­ç»ƒçš„DDPMLightningæ¨¡å‹ï¼ˆä¿®å¤ç‰ˆæœ¬ï¼Œæ­£ç¡®å¤„ç†æ–‡æœ¬ç¼–ç å™¨ï¼‰

    Args:
        checkpoint_path: æ¨¡å‹checkpointæ–‡ä»¶è·¯å¾„
        config_path: é…ç½®æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™å°è¯•ä»checkpointç›®å½•æ¨æ–­

    Returns:
        DDPMLightning: åŠ è½½çš„æ¨¡å‹å®ä¾‹
    """
    checkpoint_path = Path(checkpoint_path)

    if not checkpoint_path.exists():
        raise FileNotFoundError(f"Checkpointæ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")

    # å°è¯•åŠ è½½é…ç½®æ–‡ä»¶
    if config_path is None:
        # ä»checkpointè·¯å¾„æ¨æ–­é…ç½®æ–‡ä»¶ä½ç½®
        exp_dir = checkpoint_path.parent.parent
        config_path = exp_dir / "config" / "whole_config.yaml"

        if not config_path.exists():
            # å°è¯•å…¶ä»–å¯èƒ½çš„é…ç½®æ–‡ä»¶ä½ç½®
            config_path = exp_dir / ".hydra" / "config.yaml"

        if not config_path.exists():
            raise FileNotFoundError(f"æ— æ³•æ‰¾åˆ°é…ç½®æ–‡ä»¶ï¼Œè¯·æ‰‹åŠ¨æŒ‡å®šconfig_path")

    # åŠ è½½é…ç½®
    cfg = OmegaConf.load(config_path)

    # æ£€æŸ¥é…ç½®ç»“æ„ï¼Œç¡®ä¿åŒ…å«modeléƒ¨åˆ†
    if 'model' not in cfg:
        raise ValueError(f"é…ç½®æ–‡ä»¶ç¼ºå°‘ 'model' éƒ¨åˆ†: {config_path}")

    # æå–æ¨¡å‹é…ç½®
    model_cfg = cfg.model

    # åˆ›å»ºæ¨¡å‹å®ä¾‹
    model = DDPMLightning(model_cfg)
    
    # ã€å…³é”®ä¿®å¤ã€‘ï¼šåœ¨åŠ è½½checkpointå‰å¼ºåˆ¶åˆå§‹åŒ–text_encoder
    text_encoder_initialized = False
    if hasattr(model.eps_model, '_ensure_text_encoder'):
        try:
            print("ğŸ”§ æ­£åœ¨ä¿®å¤æ–‡æœ¬ç¼–ç å™¨åˆå§‹åŒ–...")
            model.eps_model._ensure_text_encoder()
            text_encoder_initialized = True
            print("âœ… æ–‡æœ¬ç¼–ç å™¨åˆå§‹åŒ–å®Œæˆ")
            
            # éªŒè¯åˆå§‹åŒ–ç»“æœ
            if model.eps_model.text_encoder is not None:
                print(f"  - Text encoderç±»å‹: {type(model.eps_model.text_encoder).__name__}")
            else:
                print("âŒ æ–‡æœ¬ç¼–ç å™¨åˆå§‹åŒ–å¤±è´¥")
                text_encoder_initialized = False
        except Exception as e:
            print(f"âŒ æ–‡æœ¬ç¼–ç å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            text_encoder_initialized = False

    # åŠ è½½checkpoint
    checkpoint = torch.load(checkpoint_path, map_location='cpu')

    # è·å–æ¨¡å‹å’Œcheckpointçš„state_dict
    model_state_dict = model.state_dict()
    checkpoint_state_dict = checkpoint['state_dict']

    print(f"\n=== æƒé‡åŠ è½½åˆ†æ ===")
    print(f"å½“å‰æ¨¡å‹æƒé‡æ•°é‡: {len(model_state_dict)}")
    print(f"Checkpointæƒé‡æ•°é‡: {len(checkpoint_state_dict)}")

    # åˆ†æå½“å‰æ¨¡å‹çš„æƒé‡ç»“æ„
    model_modules = {}
    for key in model_state_dict.keys():
        module_name = key.split('.')[0] if '.' in key else 'root'
        if module_name not in model_modules:
            model_modules[module_name] = 0
        model_modules[module_name] += 1

    print(f"å½“å‰æ¨¡å‹æ¨¡å—ç»“æ„:")
    for module, count in sorted(model_modules.items()):
        print(f"  - {module}: {count} ä¸ªæƒé‡")

    # åˆ†æcheckpointçš„æƒé‡ç»“æ„
    checkpoint_modules = {}
    for key in checkpoint_state_dict.keys():
        module_name = key.split('.')[0] if '.' in key else 'root'
        if module_name not in checkpoint_modules:
            checkpoint_modules[module_name] = 0
        checkpoint_modules[module_name] += 1

    print(f"Checkpointæ¨¡å—ç»“æ„:")
    for module, count in sorted(checkpoint_modules.items()):
        print(f"  - {module}: {count} ä¸ªæƒé‡")
    
    # ã€æ–°å¢ã€‘æ–‡æœ¬ç¼–ç å™¨æƒé‡è¯¦ç»†åˆ†æ
    print(f"\n=== æ–‡æœ¬ç¼–ç å™¨æƒé‡åˆ†æ ===")
    text_encoder_keys_model = [k for k in model_state_dict.keys() if 'text_encoder' in k]
    text_encoder_keys_checkpoint = [k for k in checkpoint_state_dict.keys() if 'text_encoder' in k]
    
    print(f"æ¨¡å‹ä¸­æ–‡æœ¬ç¼–ç å™¨æƒé‡: {len(text_encoder_keys_model)}")
    print(f"Checkpointä¸­æ–‡æœ¬ç¼–ç å™¨æƒé‡: {len(text_encoder_keys_checkpoint)}")
    
    if text_encoder_initialized:
        if len(text_encoder_keys_model) > 0 and len(text_encoder_keys_checkpoint) > 0:
            print("âœ… æ–‡æœ¬ç¼–ç å™¨æƒé‡åŒ¹é…æ£€æŸ¥:")
            matched_text_weights = 0
            for key in text_encoder_keys_model[:5]:  # æ£€æŸ¥å‰5ä¸ªæƒé‡
                if key in checkpoint_state_dict:
                    model_shape = model_state_dict[key].shape
                    checkpoint_shape = checkpoint_state_dict[key].shape
                    if model_shape == checkpoint_shape:
                        print(f"  âœ… {key}: {model_shape}")
                        matched_text_weights += 1
                    else:
                        print(f"  âŒ {key}: å½¢çŠ¶ä¸åŒ¹é… {model_shape} vs {checkpoint_shape}")
                else:
                    print(f"  âŒ {key}: checkpointä¸­ç¼ºå¤±")
            
            if matched_text_weights == len(text_encoder_keys_model[:5]):
                print(f"âœ… æ–‡æœ¬ç¼–ç å™¨æƒé‡å½¢çŠ¶åŒ¹é…æ­£å¸¸")
            else:
                print(f"âš ï¸  éƒ¨åˆ†æ–‡æœ¬ç¼–ç å™¨æƒé‡å½¢çŠ¶ä¸åŒ¹é…")
        else:
            if len(text_encoder_keys_model) == 0:
                print("âŒ æ¨¡å‹ä¸­æ²¡æœ‰æ–‡æœ¬ç¼–ç å™¨æƒé‡ï¼ˆå¯èƒ½åˆå§‹åŒ–å¤±è´¥ï¼‰")
            if len(text_encoder_keys_checkpoint) == 0:
                print("âŒ Checkpointä¸­æ²¡æœ‰æ–‡æœ¬ç¼–ç å™¨æƒé‡")
    else:
        print("âš ï¸  æ–‡æœ¬ç¼–ç å™¨æœªåˆå§‹åŒ–ï¼Œè·³è¿‡æƒé‡åˆ†æ")

    # åªåŠ è½½åŒ¹é…çš„æƒé‡
    matched_state_dict = {}
    unmatched_keys = []

    for key in model_state_dict.keys():
        if key in checkpoint_state_dict:
            if model_state_dict[key].shape == checkpoint_state_dict[key].shape:
                matched_state_dict[key] = checkpoint_state_dict[key]
            else:
                print(f"è­¦å‘Š: æƒé‡å½¢çŠ¶ä¸åŒ¹é… {key}: æ¨¡å‹ {model_state_dict[key].shape} vs checkpoint {checkpoint_state_dict[key].shape}")
                unmatched_keys.append(key)
        else:
            print(f"è­¦å‘Š: checkpointä¸­ç¼ºå°‘æƒé‡ {key}")
            unmatched_keys.append(key)

    # æ£€æŸ¥checkpointä¸­å¤šä½™çš„æƒé‡
    extra_keys = set(checkpoint_state_dict.keys()) - set(model_state_dict.keys())
    if extra_keys:
        print(f"ä¿¡æ¯: checkpointåŒ…å« {len(extra_keys)} ä¸ªé¢å¤–æƒé‡ï¼ˆå¯èƒ½æ¥è‡ªæ›´å¤æ‚çš„æ¨¡å‹ç‰ˆæœ¬ï¼‰")

        # è¯¦ç»†åˆ†æé¢å¤–æƒé‡
        print("\n=== é¢å¤–æƒé‡è¯¦ç»†åˆ†æ ===")

        # æŒ‰æ¨¡å—åˆ†ç»„åˆ†æé¢å¤–æƒé‡
        extra_by_module = {}
        for key in extra_keys:
            module_name = key.split('.')[0] if '.' in key else 'root'
            if module_name not in extra_by_module:
                extra_by_module[module_name] = []
            extra_by_module[module_name].append(key)

        for module, keys in extra_by_module.items():
            print(f"æ¨¡å— '{module}': {len(keys)} ä¸ªé¢å¤–æƒé‡")
            for key in sorted(keys)[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                if key in checkpoint_state_dict:
                    shape = checkpoint_state_dict[key].shape
                    print(f"  - {key}: {shape}")
            if len(keys) > 5:
                print(f"  ... è¿˜æœ‰ {len(keys) - 5} ä¸ªæƒé‡")

        # åˆ†æå¯èƒ½çš„åŸå› 
        print("\n=== å¯èƒ½çš„åŸå› åˆ†æ ===")
        score_related = [k for k in extra_keys if 'score' in k.lower()]
        text_related = [k for k in extra_keys if any(word in k.lower() for word in ['text', 'clip', 'bert', 'transformer'])]
        attention_related = [k for k in extra_keys if any(word in k.lower() for word in ['attention', 'attn', 'self_attn'])]

        if score_related:
            print(f"- åŒ…å« {len(score_related)} ä¸ªè¯„åˆ†ç›¸å…³æƒé‡ï¼ˆå¯èƒ½æ˜¯è¯„åˆ†æ¨¡å—ï¼‰")
        if text_related:
            print(f"- åŒ…å« {len(text_related)} ä¸ªæ–‡æœ¬ç›¸å…³æƒé‡ï¼ˆå¯èƒ½æ˜¯æ–‡æœ¬ç¼–ç å™¨ï¼‰")
        if attention_related:
            print(f"- åŒ…å« {len(attention_related)} ä¸ªæ³¨æ„åŠ›ç›¸å…³æƒé‡ï¼ˆå¯èƒ½æ˜¯æ›´å¤æ‚çš„æ³¨æ„åŠ›æœºåˆ¶ï¼‰")

        print("=== åˆ†æå®Œæˆ ===\n")

    # å°è¯•ä¸¥æ ¼åŠ è½½æƒé‡
    loading_success = False
    try:
        if text_encoder_initialized and len(text_encoder_keys_model) > 0 and len(text_encoder_keys_checkpoint) > 0:
            # å¦‚æœæ–‡æœ¬ç¼–ç å™¨æ­£ç¡®åˆå§‹åŒ–ä¸”æƒé‡åŒ¹é…ï¼Œå°è¯•ä¸¥æ ¼åŠ è½½
            model.load_state_dict(checkpoint_state_dict, strict=True)
            print("âœ… æƒé‡ä¸¥æ ¼åŠ è½½æˆåŠŸ (strict=True)")
            loading_success = True
        else:
            # å¦åˆ™ä½¿ç”¨åŒ¹é…åŠ è½½
            model.load_state_dict(matched_state_dict, strict=False)
            print(f"âš ï¸  æƒé‡éä¸¥æ ¼åŠ è½½ (åŠ è½½äº† {len(matched_state_dict)}/{len(model_state_dict)} ä¸ªæƒé‡)")
    except Exception as e:
        print(f"âŒ ä¸¥æ ¼åŠ è½½å¤±è´¥: {e}")
        # å›é€€åˆ°åŒ¹é…åŠ è½½
        model.load_state_dict(matched_state_dict, strict=False)
        print(f"âš ï¸  å›é€€åˆ°éä¸¥æ ¼åŠ è½½ (åŠ è½½äº† {len(matched_state_dict)}/{len(model_state_dict)} ä¸ªæƒé‡)")
    
    model.eval()

    print(f"âœ“ æˆåŠŸåŠ è½½é¢„è®­ç»ƒæ¨¡å‹: {checkpoint_path}")
    print(f"âœ“ ä½¿ç”¨é…ç½®æ–‡ä»¶: {config_path}")
    print(f"âœ“ æ¨¡å‹é…ç½®: {model_cfg.get('name', 'DDPMLightning')}")
    
    # ã€æ–°å¢ã€‘æ–‡æœ¬ç¼–ç å™¨çŠ¶æ€éªŒè¯
    print(f"\n=== æ–‡æœ¬ç¼–ç å™¨çŠ¶æ€éªŒè¯ ===")
    if text_encoder_initialized and hasattr(model.eps_model, 'text_encoder') and model.eps_model.text_encoder is not None:
        text_encoder = model.eps_model.text_encoder
        print(f"âœ… æ–‡æœ¬ç¼–ç å™¨çŠ¶æ€:")
        print(f"  - è®¾å¤‡: {text_encoder.device}")
        print(f"  - è®­ç»ƒæ¨¡å¼: {text_encoder.training}")
        
        # æ£€æŸ¥å…³é”®æƒé‡æ˜¯å¦å·²åŠ è½½
        if hasattr(text_encoder, 'text_encoder') and hasattr(text_encoder.text_encoder, 'clip_model'):
            clip_model = text_encoder.text_encoder.clip_model
            if hasattr(clip_model, 'text_projection'):
                proj_weight = clip_model.text_projection
                weight_std = proj_weight.std().item()
                weight_mean = proj_weight.mean().item()
                print(f"  - æ–‡æœ¬æŠ•å½±æƒé‡ç»Ÿè®¡: å‡å€¼={weight_mean:.4f}, æ ‡å‡†å·®={weight_std:.4f}")
                
                # åˆ¤æ–­æƒé‡æ˜¯å¦åˆç†ï¼ˆè®­ç»ƒè¿‡çš„æƒé‡é€šå¸¸æœ‰ç‰¹å®šçš„åˆ†å¸ƒç‰¹å¾ï¼‰
                if 0.01 < weight_std < 1.0 and abs(weight_mean) < 0.5:
                    print(f"  âœ… æ–‡æœ¬ç¼–ç å™¨æƒé‡çœ‹èµ·æ¥å·²æ­£ç¡®åŠ è½½")
                else:
                    print(f"  âš ï¸  æ–‡æœ¬ç¼–ç å™¨æƒé‡å¯èƒ½æœªæ­£ç¡®åŠ è½½ï¼ˆç»Ÿè®¡å¼‚å¸¸ï¼‰")
            else:
                print(f"  âŒ æ— æ³•è®¿é—®æ–‡æœ¬æŠ•å½±æƒé‡")
        else:
            print(f"  âŒ æ— æ³•è®¿é—®CLIPæ¨¡å‹ç»„ä»¶")
    else:
        print(f"âŒ æ–‡æœ¬ç¼–ç å™¨æœªæ­£ç¡®åˆå§‹åŒ–æˆ–åŠ è½½")
    
    if unmatched_keys:
        print(f"\nâš ï¸ {len(unmatched_keys)} ä¸ªæƒé‡æœªèƒ½åŠ è½½ï¼Œæ¨¡å‹å¯èƒ½éœ€è¦é‡æ–°è®­ç»ƒ")
    
    if loading_success:
        print(f"\nğŸ‰ æ¨¡å‹åŠ è½½å®Œå…¨æˆåŠŸï¼æ–‡æœ¬ç¼–ç å™¨åº”è¯¥å¯ä»¥æ­£å¸¸å·¥ä½œã€‚")
    else:
        print(f"\nâš ï¸  æ¨¡å‹åŠ è½½éƒ¨åˆ†æˆåŠŸï¼Œå»ºè®®éªŒè¯æ–‡æœ¬åŠŸèƒ½æ˜¯å¦æ­£å¸¸ã€‚")

    return model

def predict_grasps(model: DDPMLightning, batch: Dict, device: str = 'cuda', num_grasps: int = 8) -> torch.Tensor:
    """
    ä½¿ç”¨æ¨¡å‹é¢„æµ‹æŠ“å–å§¿æ€
    
    Args:
        model: é¢„è®­ç»ƒçš„DDPMLightningæ¨¡å‹
        batch: æ•°æ®æ‰¹æ¬¡
        device: è®¡ç®—è®¾å¤‡
        num_grasps: é¢„æµ‹çš„æŠ“å–æ•°é‡
    
    Returns:
        torch.Tensor: é¢„æµ‹çš„æŠ“å–å§¿æ€ [B, num_grasps, pose_dim]
    """
    model = model.to(device)
    
    # å°†batchæ•°æ®ç§»åŠ¨åˆ°è®¾å¤‡
    for key, value in batch.items():
        if isinstance(value, torch.Tensor):
            batch[key] = value.to(device)
    
    with torch.no_grad():
        # ä½¿ç”¨forward_get_pose_matchedæ–¹æ³•è·å–é¢„æµ‹å§¿æ€
        matched_preds, matched_targets, outputs, targets = model.forward_get_pose_matched(batch, k=num_grasps)
        
        # æ‰“å°ç»“æ„ä¿¡æ¯ç”¨äºè°ƒè¯•
        print_matched_preds_structure(matched_preds, matched_targets)
        
        # æ£€æŸ¥matched_predsçš„ç±»å‹
        if isinstance(matched_preds, dict):
            # å¦‚æœmatched_predsæ˜¯å­—å…¸ï¼Œå°è¯•æå–æ‰‹éƒ¨å§¿æ€
            if 'hand_model_pose' in matched_preds:
                pred_poses = matched_preds['hand_model_pose']
            elif 'pred_pose_norm' in matched_preds:
                pred_poses = matched_preds['pred_pose_norm']
            else:
                # å°è¯•å…¶ä»–å¯èƒ½çš„é”®
                pose_keys = [k for k in matched_preds.keys() if 'pose' in k.lower() or 'hand' in k.lower()]
                if pose_keys:
                    pred_poses = matched_preds[pose_keys[0]]
                else:
                    raise ValueError(f"æ— æ³•ä»matched_predså­—å…¸ä¸­æ‰¾åˆ°å§¿æ€æ•°æ®ã€‚å¯ç”¨çš„é”®: {list(matched_preds.keys())}")
        else:
            # å¦‚æœmatched_predsæ˜¯å¼ é‡ï¼Œç›´æ¥ä½¿ç”¨
            pred_poses = matched_preds
        
        # ç¡®ä¿pred_posesæ˜¯æ­£ç¡®çš„å½¢çŠ¶
        if pred_poses.dim() == 2:
            # å¦‚æœæ˜¯ [B*num_grasps, pose_dim]ï¼Œé‡å¡‘ä¸º [B, num_grasps, pose_dim]
            B = batch['scene_pc'].shape[0]
            pred_poses = pred_poses.view(B, num_grasps, -1)
        elif pred_poses.dim() == 3:
            # å¦‚æœå·²ç»æ˜¯ [B, num_grasps, pose_dim]ï¼Œç›´æ¥ä½¿ç”¨
            pass
        else:
            raise ValueError(f"é¢„æµ‹å§¿æ€çš„å½¢çŠ¶ä¸æ­£ç¡®: {pred_poses.shape}")
        
    return pred_poses

def predict_grasps_with_details(model: DDPMLightning, batch: Dict, device: str = 'cuda', num_grasps: int = 8) -> Dict:
    """
    ä½¿ç”¨æ¨¡å‹é¢„æµ‹æŠ“å–å§¿æ€ï¼Œå¹¶è¿”å›è¯¦ç»†ä¿¡æ¯
    
    Args:
        model: é¢„è®­ç»ƒçš„DDPMLightningæ¨¡å‹
        batch: æ•°æ®æ‰¹æ¬¡
        device: è®¡ç®—è®¾å¤‡
        num_grasps: é¢„æµ‹çš„æŠ“å–æ•°é‡
    
    Returns:
        Dict: åŒ…å«é¢„æµ‹ç»“æœçš„è¯¦ç»†ä¿¡æ¯
    """
    model = model.to(device)
    
    # å°†batchæ•°æ®ç§»åŠ¨åˆ°è®¾å¤‡
    for key, value in batch.items():
        if isinstance(value, torch.Tensor):
            batch[key] = value.to(device)
    
    with torch.no_grad():
        # ä½¿ç”¨forward_get_pose_matchedæ–¹æ³•è·å–é¢„æµ‹å§¿æ€
        matched_preds, matched_targets, outputs, targets = model.forward_get_pose_matched(batch, k=num_grasps)
        
        # æ‰“å°ç»“æ„ä¿¡æ¯ç”¨äºè°ƒè¯•
        print_matched_preds_structure(matched_preds, matched_targets)
        
        # æ£€æŸ¥matched_predsçš„ç±»å‹å¹¶æå–æ‰‹éƒ¨å§¿æ€
        if isinstance(matched_preds, dict):
            # å¦‚æœmatched_predsæ˜¯å­—å…¸ï¼Œå°è¯•æå–æ‰‹éƒ¨å§¿æ€
            if 'hand_model_pose' in matched_preds:
                pred_poses = matched_preds['hand_model_pose']
            elif 'pred_pose_norm' in matched_preds:
                pred_poses = matched_preds['pred_pose_norm']
            else:
                # å°è¯•å…¶ä»–å¯èƒ½çš„é”®
                pose_keys = [k for k in matched_preds.keys() if 'pose' in k.lower() or 'hand' in k.lower()]
                if pose_keys:
                    pred_poses = matched_preds[pose_keys[0]]
                else:
                    raise ValueError(f"æ— æ³•ä»matched_predså­—å…¸ä¸­æ‰¾åˆ°å§¿æ€æ•°æ®ã€‚å¯ç”¨çš„é”®: {list(matched_preds.keys())}")
        else:
            # å¦‚æœmatched_predsæ˜¯å¼ é‡ï¼Œç›´æ¥ä½¿ç”¨
            pred_poses = matched_preds
        
        # ç¡®ä¿pred_posesæ˜¯æ­£ç¡®çš„å½¢çŠ¶
        if pred_poses.dim() == 2:
            # å¦‚æœæ˜¯ [B*num_grasps, pose_dim]ï¼Œé‡å¡‘ä¸º [B, num_grasps, pose_dim]
            B = batch['scene_pc'].shape[0]
            pred_poses = pred_poses.view(B, num_grasps, -1)
        elif pred_poses.dim() == 3:
            # å¦‚æœå·²ç»æ˜¯ [B, num_grasps, pose_dim]ï¼Œç›´æ¥ä½¿ç”¨
            pass
        else:
            raise ValueError(f"é¢„æµ‹å§¿æ€çš„å½¢çŠ¶ä¸æ­£ç¡®: {pred_poses.shape}")
        
        # è¿”å›è¯¦ç»†ä¿¡æ¯
        result = {
            'pred_poses': pred_poses,
            'matched_preds': matched_preds,
            'matched_targets': matched_targets,
            'outputs': outputs,
            'targets': targets
        }
        
    return result

def print_forward_get_pose_matched_details(outputs: Dict, targets: Dict, batch_size: int, num_grasps: int):
    """
    æ‰“å°forward_get_pose_matchedè¾“å‡ºçš„è¯¦ç»†ä¿¡æ¯
    
    Args:
        outputs: æ¨¡å‹è¾“å‡ºå­—å…¸
        targets: ç›®æ ‡å­—å…¸
        batch_size: æ‰¹æ¬¡å¤§å°
        num_grasps: æŠ“å–æ•°é‡
    """
    print(f"\n=== forward_get_pose_matched è¾“å‡ºè¯¦ç»†ä¿¡æ¯ ===")
    print(f"æ‰¹æ¬¡å¤§å°: {batch_size}, æŠ“å–æ•°é‡: {num_grasps}")
    print(f"æ€»æ ·æœ¬æ•°: {batch_size * num_grasps}")
    
    if 'hand' in outputs:
        print(f"\noutputs['hand'] åŒ…å«ä»¥ä¸‹æ•°æ®é¡¹:")
        for key, value in outputs['hand'].items():
            if isinstance(value, torch.Tensor):
                print(f"  - {key}: å½¢çŠ¶ {value.shape}, è®¾å¤‡ {value.device}")
            else:
                print(f"  - {key}: {type(value)}")
    
    if 'hand' in targets:
        print(f"\ntargets['hand'] åŒ…å«ä»¥ä¸‹æ•°æ®é¡¹:")
        for key, value in targets['hand'].items():
            if isinstance(value, torch.Tensor):
                print(f"  - {key}: å½¢çŠ¶ {value.shape}, è®¾å¤‡ {value.device}")
            else:
                print(f"  - {key}: {type(value)}")
    
    # æ£€æŸ¥å…¶ä»–å¯èƒ½çš„é”®
    other_output_keys = [k for k in outputs.keys() if k != 'hand']
    if other_output_keys:
        print(f"\noutputs ä¸­çš„å…¶ä»–é”®: {other_output_keys}")
        for key in other_output_keys:
            if isinstance(outputs[key], torch.Tensor):
                print(f"  - {key}: å½¢çŠ¶ {outputs[key].shape}, è®¾å¤‡ {outputs[key].device}")
            else:
                print(f"  - {key}: {type(outputs[key])}")
    
    other_target_keys = [k for k in targets.keys() if k != 'hand']
    if other_target_keys:
        print(f"\ntargets ä¸­çš„å…¶ä»–é”®: {other_target_keys}")
        for key in other_target_keys:
            if isinstance(targets[key], torch.Tensor):
                print(f"  - {key}: å½¢çŠ¶ {targets[key].shape}, è®¾å¤‡ {targets[key].device}")
            else:
                print(f"  - {key}: {type(targets[key])}")
    
    print("=" * 50)

def print_matched_preds_structure(matched_preds, matched_targets):
    """
    æ‰“å°matched_predså’Œmatched_targetsçš„ç»“æ„ä¿¡æ¯
    
    Args:
        matched_preds: é¢„æµ‹ç»“æœ
        matched_targets: ç›®æ ‡ç»“æœ
    """
    print(f"\n=== matched_preds å’Œ matched_targets ç»“æ„åˆ†æ ===")
    
    print(f"matched_preds ç±»å‹: {type(matched_preds)}")
    if isinstance(matched_preds, dict):
        print(f"matched_preds é”®: {list(matched_preds.keys())}")
        for key, value in matched_preds.items():
            if isinstance(value, torch.Tensor):
                print(f"  - {key}: å½¢çŠ¶ {value.shape}, è®¾å¤‡ {value.device}")
            else:
                print(f"  - {key}: {type(value)}")
    elif isinstance(matched_preds, torch.Tensor):
        print(f"matched_preds å½¢çŠ¶: {matched_preds.shape}, è®¾å¤‡: {matched_preds.device}")
    else:
        print(f"matched_preds: {matched_preds}")
    
    print(f"\nmatched_targets ç±»å‹: {type(matched_targets)}")
    if isinstance(matched_targets, dict):
        print(f"matched_targets é”®: {list(matched_targets.keys())}")
        for key, value in matched_targets.items():
            if isinstance(value, torch.Tensor):
                print(f"  - {key}: å½¢çŠ¶ {value.shape}, è®¾å¤‡ {value.device}")
            else:
                print(f"  - {key}: {type(value)}")
    elif isinstance(matched_targets, torch.Tensor):
        print(f"matched_targets å½¢çŠ¶: {matched_targets.shape}, è®¾å¤‡: {matched_targets.device}")
    else:
        print(f"matched_targets: {matched_targets}")
    
    print("=" * 50)

def create_hand_meshes_from_outputs(outputs: Dict, targets: Dict, batch_size: int, num_grasps: int,
                                  max_grasps: int = 3) -> Tuple[List, List]:
    """
    ä»forward_get_pose_matchedçš„è¾“å‡ºåˆ›å»ºæ‰‹éƒ¨mesh
    
    Args:
        outputs: æ¨¡å‹è¾“å‡ºå­—å…¸
        targets: ç›®æ ‡å­—å…¸
        batch_size: æ‰¹æ¬¡å¤§å°
        num_grasps: æŠ“å–æ•°é‡
        max_grasps: æœ€å¤§æ˜¾ç¤ºæŠ“å–æ•°é‡
    
    Returns:
        Tuple[List, List]: (é¢„æµ‹meshåˆ—è¡¨, çœŸå®meshåˆ—è¡¨)
    """
    pred_meshes = []
    gt_meshes = []
    
    if 'hand' not in outputs or 'hand' not in targets:
        print("è­¦å‘Š: outputsæˆ–targetsä¸­ç¼ºå°‘'hand'é”®")
        return pred_meshes, gt_meshes
    
    outputs_hand = outputs['hand']
    targets_hand = targets['hand']
    
    # æ£€æŸ¥æ˜¯å¦æœ‰verticeså’Œfacesæ•°æ®
    if 'vertices' not in outputs_hand or 'faces' not in outputs_hand:
        print("è­¦å‘Š: outputs['hand']ä¸­ç¼ºå°‘verticesæˆ–facesæ•°æ®")
        return pred_meshes, gt_meshes
    
    if 'vertices' not in targets_hand or 'faces' not in targets_hand:
        print("è­¦å‘Š: targets['hand']ä¸­ç¼ºå°‘verticesæˆ–facesæ•°æ®")
        return pred_meshes, gt_meshes
    
    # è·å–verticeså’Œfacesæ•°æ®
    pred_vertices = outputs_hand['vertices']  # [B*num_grasps, num_vertices, 3]
    pred_faces = outputs_hand['faces']        # [num_faces, 3]
    gt_vertices = targets_hand['vertices']    # [B*num_grasps, num_vertices, 3]
    gt_faces = targets_hand['faces']          # [num_faces, 3]
    
    print(f"é¢„æµ‹verticeså½¢çŠ¶: {pred_vertices.shape}")
    print(f"é¢„æµ‹faceså½¢çŠ¶: {pred_faces.shape}")
    print(f"çœŸå®verticeså½¢çŠ¶: {gt_vertices.shape}")
    print(f"çœŸå®faceså½¢çŠ¶: {gt_faces.shape}")
    
    # é¢„æµ‹å§¿æ€é¢œè‰² (è“è‰²ç³»)
    pred_colors = [
        [0.0, 0.0, 1.0],  # è“è‰²
        [0.0, 0.5, 1.0],  # æµ…è“è‰²
        [0.0, 1.0, 1.0],  # é’è‰²
        [0.5, 0.0, 1.0],  # ç´«è“è‰²
        [0.0, 0.0, 0.5],  # æ·±è“è‰²
    ]
    
    # çœŸå®å§¿æ€é¢œè‰² (çº¢è‰²ç³»)
    gt_colors = [
        [1.0, 0.0, 0.0],  # çº¢è‰²
        [1.0, 0.5, 0.0],  # æ©™è‰²
        [1.0, 0.0, 0.5],  # ç²‰çº¢è‰²
        [0.8, 0.0, 0.0],  # æ·±çº¢è‰²
        [1.0, 0.2, 0.2],  # æµ…çº¢è‰²
    ]
    
    display_grasps = min(num_grasps, max_grasps)
    
    for b in range(batch_size):
        for g in range(display_grasps):
            try:
                # è®¡ç®—åœ¨å±•å¹³æ•°ç»„ä¸­çš„ç´¢å¼•
                idx = b * num_grasps + g
                
                # åˆ›å»ºé¢„æµ‹mesh
                pred_verts = pred_vertices[idx].detach().cpu().numpy()
                pred_faces_np = pred_faces.detach().cpu().numpy()
                
                pred_mesh = o3d.geometry.TriangleMesh()
                pred_mesh.vertices = o3d.utility.Vector3dVector(pred_verts)
                pred_mesh.triangles = o3d.utility.Vector3iVector(pred_faces_np.astype(np.int32))
                pred_mesh.paint_uniform_color(pred_colors[g % len(pred_colors)])
                pred_mesh.compute_vertex_normals()
                pred_meshes.append(pred_mesh)
                
                # åˆ›å»ºçœŸå®mesh
                gt_verts = gt_vertices[idx].detach().cpu().numpy()
                gt_faces_np = gt_faces.detach().cpu().numpy()
                
                gt_mesh = o3d.geometry.TriangleMesh()
                gt_mesh.vertices = o3d.utility.Vector3dVector(gt_verts)
                gt_mesh.triangles = o3d.utility.Vector3iVector(gt_faces_np.astype(np.int32))
                gt_mesh.paint_uniform_color(gt_colors[g % len(gt_colors)])
                gt_mesh.compute_vertex_normals()
                gt_meshes.append(gt_mesh)
                
            except Exception as e:
                print(f"åˆ›å»ºç¬¬{b}æ‰¹æ¬¡ç¬¬{g}ä¸ªæŠ“å–çš„meshå¤±è´¥: {e}")
                continue
    
    return pred_meshes, gt_meshes

def calculate_pose_errors(pred_poses: torch.Tensor, gt_poses: torch.Tensor, 
                         rot_type: str = 'r6d') -> Dict[str, float]:
    """
    è®¡ç®—é¢„æµ‹å§¿æ€ä¸çœŸå®å§¿æ€ä¹‹é—´çš„è¯¯å·®
    
    Args:
        pred_poses: é¢„æµ‹å§¿æ€ [B, num_grasps, 23] æˆ– [B, num_grasps, 25]
        gt_poses: çœŸå®å§¿æ€ [B, num_grasps, 23] 
        rot_type: æ—‹è½¬è¡¨ç¤ºç±»å‹
    
    Returns:
        Dict[str, float]: è¯¯å·®æŒ‡æ ‡å­—å…¸
    """
    # ç¡®ä¿ä¸¤ä¸ªå¼ é‡åœ¨åŒä¸€è®¾å¤‡ä¸Š
    device = pred_poses.device
    gt_poses = gt_poses.to(device)

    # ä¸ºå¤„ç†ä¸åŒé•¿åº¦çš„å§¿æ€ï¼Œæˆ‘ä»¬åªæ¯”è¾ƒå…±åŒçš„éƒ¨åˆ†
    common_dim = min(pred_poses.shape[-1], gt_poses.shape[-1])
    
    # æå–ä½ç½®ã€å…³èŠ‚è§’åº¦
    pred_trans = pred_poses[..., :3]
    pred_qpos = pred_poses[..., 3:19]
    
    gt_trans = gt_poses[..., :3]
    gt_qpos = gt_poses[..., 3:19] 
    
    # è®¡ç®—ä½ç½®è¯¯å·® (æ¬§å‡ é‡Œå¾—è·ç¦»)
    trans_error = torch.norm(pred_trans - gt_trans, dim=-1)
    
    # è®¡ç®—å…³èŠ‚è§’åº¦è¯¯å·® (MSE)
    qpos_error = torch.mean((pred_qpos - gt_qpos) ** 2, dim=-1)
    
    # è®¡ç®—æ—‹è½¬è¯¯å·® (ä»…å½“ç»´åº¦åŒ¹é…æ—¶)
    rot_error = torch.tensor(0.0, device=pred_poses.device) # é»˜è®¤å€¼
    if pred_poses.shape[-1] == gt_poses.shape[-1]:
        pred_rot = pred_poses[..., 19:]
        gt_rot = gt_poses[..., 19:]
        if rot_type == 'r6d':
            # å¯¹äº6Dæ—‹è½¬è¡¨ç¤ºï¼Œä½¿ç”¨MSE
            rot_error = torch.mean((pred_rot - gt_rot) ** 2, dim=-1)
        elif rot_type == 'quat':
            # å¯¹äºå››å…ƒæ•°ï¼Œä½¿ç”¨è§’åº¦è¯¯å·®
            dot_product = torch.sum(pred_rot * gt_rot, dim=-1)
            rot_error = 1.0 - torch.abs(dot_product)
        else:
            rot_error = torch.mean((pred_rot - gt_rot) ** 2, dim=-1)
    else:
        print(f"è­¦å‘Š: é¢„æµ‹å§¿æ€å’ŒçœŸå®å§¿æ€çš„æ—‹è½¬ç»´åº¦ä¸åŒ¹é… ({pred_poses.shape[-1]} vs {gt_poses.shape[-1]})ï¼Œè·³è¿‡æ—‹è½¬è¯¯å·®è®¡ç®—ã€‚")

    # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
    errors = {
        'translation_mean': float(torch.mean(trans_error)),
        'translation_std': float(torch.std(trans_error)),
        'translation_max': float(torch.max(trans_error)),
        'translation_min': float(torch.min(trans_error)),
        
        'qpos_mean': float(torch.mean(qpos_error)),
        'qpos_std': float(torch.std(qpos_error)),
        'qpos_max': float(torch.max(qpos_error)),
        'qpos_min': float(torch.min(qpos_error)),
        
        'rotation_mean': float(torch.mean(rot_error)),
        'rotation_std': float(torch.std(rot_error)),
        'rotation_max': float(torch.max(rot_error)),
        'rotation_min': float(torch.min(rot_error)),
    }
    
    return errors

def create_hand_meshes_comparison(pred_poses: torch.Tensor, gt_poses: torch.Tensor,
                                hand_model: HandModel, max_grasps: int = 3) -> Tuple[List, List]:
    """
    åˆ›å»ºé¢„æµ‹å’ŒçœŸå®æŠ“å–å§¿æ€çš„æ‰‹éƒ¨meshç”¨äºå¯¹æ¯”
    
    Args:
        pred_poses: é¢„æµ‹å§¿æ€ [B, num_grasps, 23]
        gt_poses: çœŸå®å§¿æ€ [B, num_grasps, 23]
        hand_model: æ‰‹éƒ¨æ¨¡å‹å®ä¾‹
        max_grasps: æœ€å¤§æ˜¾ç¤ºæŠ“å–æ•°é‡
    
    Returns:
        Tuple[List, List]: (é¢„æµ‹meshåˆ—è¡¨, çœŸå®meshåˆ—è¡¨)
    """
    pred_meshes = []
    gt_meshes = []
    
    # ç¡®ä¿è¾“å…¥æ˜¯3Då¼ é‡
    if pred_poses.dim() == 2:
        pred_poses = pred_poses.unsqueeze(0)
    if gt_poses.dim() == 2:
        gt_poses = gt_poses.unsqueeze(0)
    
    B, num_grasps, _ = pred_poses.shape
    display_grasps = min(num_grasps, max_grasps)
    
    # é¢„æµ‹å§¿æ€é¢œè‰² (è“è‰²ç³»)
    pred_colors = [
        [0.0, 0.0, 1.0],  # è“è‰²
        [0.0, 0.5, 1.0],  # æµ…è“è‰²
        [0.0, 1.0, 1.0],  # é’è‰²
        [0.5, 0.0, 1.0],  # ç´«è“è‰²
        [0.0, 0.0, 0.5],  # æ·±è“è‰²
    ]
    
    # çœŸå®å§¿æ€é¢œè‰² (çº¢è‰²ç³»)
    gt_colors = [
        [1.0, 0.0, 0.0],  # çº¢è‰²
        [1.0, 0.5, 0.0],  # æ©™è‰²
        [1.0, 0.0, 0.5],  # ç²‰çº¢è‰²
        [0.8, 0.0, 0.0],  # æ·±çº¢è‰²
        [1.0, 0.2, 0.2],  # æµ…çº¢è‰²
    ]
    
    for b in range(B):
        for g in range(display_grasps):
            try:
                # åˆ›å»ºé¢„æµ‹å§¿æ€mesh
                hand_model.set_parameters(pred_poses[b, g])
                pred_trimesh = hand_model.get_trimesh_data(0)
                
                pred_mesh = o3d.geometry.TriangleMesh()
                pred_mesh.vertices = o3d.utility.Vector3dVector(pred_trimesh.vertices)
                pred_mesh.triangles = o3d.utility.Vector3iVector(pred_trimesh.faces)
                pred_mesh.paint_uniform_color(pred_colors[g % len(pred_colors)])
                pred_mesh.compute_vertex_normals()
                pred_meshes.append(pred_mesh)
                
                # åˆ›å»ºçœŸå®å§¿æ€mesh
                hand_model.set_parameters(gt_poses[b, g])
                gt_trimesh = hand_model.get_trimesh_data(0)
                
                gt_mesh = o3d.geometry.TriangleMesh()
                gt_mesh.vertices = o3d.utility.Vector3dVector(gt_trimesh.vertices)
                gt_mesh.triangles = o3d.utility.Vector3iVector(gt_trimesh.faces)
                gt_mesh.paint_uniform_color(gt_colors[g % len(gt_colors)])
                gt_mesh.compute_vertex_normals()
                gt_meshes.append(gt_mesh)
                
            except Exception as e:
                print(f"åˆ›å»ºç¬¬{b}æ‰¹æ¬¡ç¬¬{g}ä¸ªæŠ“å–çš„meshå¤±è´¥: {e}")
                continue
    
    return pred_meshes, gt_meshes

def create_point_cloud_from_sample(scene_pc: torch.Tensor, object_mask: Optional[torch.Tensor] = None) -> o3d.geometry.PointCloud:
    """ä»æ ·æœ¬æ•°æ®åˆ›å»ºOpen3Dç‚¹äº‘"""
    if isinstance(scene_pc, torch.Tensor):
        scene_pc_np = scene_pc.detach().cpu().numpy()
    else:
        scene_pc_np = scene_pc
    
    assert scene_pc_np.shape[1] == 6, f"æœŸæœ›6ç»´ç‚¹äº‘ (xyz+rgb)ï¼Œå®é™…å¾—åˆ° {scene_pc_np.shape[1]} ç»´"
    
    xyz = scene_pc_np[:, :3]
    rgb = scene_pc_np[:, 3:6]
    
    pcd = o3d.geometry.PointCloud()
    pcd.points = o3d.utility.Vector3dVector(xyz)
    pcd.colors = o3d.utility.Vector3dVector(rgb)
    
    return pcd

def create_highlighted_point_cloud(scene_pc: torch.Tensor, object_mask: torch.Tensor) -> Tuple[o3d.geometry.PointCloud, o3d.geometry.PointCloud]:
    """åˆ›å»ºé«˜äº®æ˜¾ç¤ºç›®æ ‡ç‰©ä½“çš„ç‚¹äº‘"""
    scene_pc_np = scene_pc.detach().cpu().numpy()
    object_mask_np = object_mask.detach().cpu().numpy()
    
    if len(object_mask_np) != len(scene_pc_np):
        print(f"è­¦å‘Š: object_maskå¤§å° ({len(object_mask_np)}) ä¸ç‚¹äº‘å¤§å° ({len(scene_pc_np)}) ä¸åŒ¹é…")
        return create_point_cloud_from_sample(scene_pc), None
    
    background_points = scene_pc_np[~object_mask_np]
    object_points = scene_pc_np[object_mask_np]
    
    # åˆ›å»ºèƒŒæ™¯ç‚¹äº‘
    background_pcd = o3d.geometry.PointCloud()
    if len(background_points) > 0:
        background_pcd.points = o3d.utility.Vector3dVector(background_points[:, :3])
        background_colors = background_points[:, 3:6] * 0.5
        background_pcd.colors = o3d.utility.Vector3dVector(background_colors)
    
    # åˆ›å»ºç›®æ ‡ç‰©ä½“ç‚¹äº‘
    object_pcd = o3d.geometry.PointCloud()
    if len(object_points) > 0:
        object_pcd.points = o3d.utility.Vector3dVector(object_points[:, :3])
        object_colors = np.ones((len(object_points), 3)) * [1.0, 0.0, 0.0]
        object_pcd.colors = o3d.utility.Vector3dVector(object_colors)
    
    return background_pcd, object_pcd

def create_object_mesh(obj_verts: torch.Tensor, obj_faces: torch.Tensor,
                      color: Tuple[float, float, float] = (0.0, 1.0, 0.0)) -> Optional[o3d.geometry.TriangleMesh]:
    """ä»é¡¶ç‚¹å’Œé¢æ•°æ®åˆ›å»ºç›®æ ‡ç‰©ä½“mesh"""
    try:
        if obj_verts.numel() == 0 or obj_faces.numel() == 0:
            return None

        vertices_np = obj_verts.detach().cpu().numpy()
        faces_np = obj_faces.detach().cpu().numpy()

        object_mesh = o3d.geometry.TriangleMesh()
        object_mesh.vertices = o3d.utility.Vector3dVector(vertices_np)
        object_mesh.triangles = o3d.utility.Vector3iVector(faces_np.astype(np.int32))
        object_mesh.paint_uniform_color(color)
        object_mesh.compute_vertex_normals()

        return object_mesh

    except Exception as e:
        print(f"åˆ›å»ºç‰©ä½“meshå¤±è´¥: {e}")
        return None

def create_coordinate_frame(size: float = 0.1) -> o3d.geometry.TriangleMesh:
    """åˆ›å»ºåæ ‡è½´å‚è€ƒæ¡†æ¶"""
    return o3d.geometry.TriangleMesh.create_coordinate_frame(size=size)

def visualize_prediction_vs_ground_truth(dataset: SceneLeapPlusDataset, model: DDPMLightning,
                                       sample_idx: int = 0, max_grasps: int = 3,
                                       device: str = 'cuda'):
    """
    å¯è§†åŒ–é¢„æµ‹æŠ“å–å§¿æ€ä¸çœŸå®æŠ“å–å§¿æ€çš„å¯¹æ¯”

    Args:
        dataset: SceneLeapPlusDatasetå®ä¾‹
        model: é¢„è®­ç»ƒçš„DDPMLightningæ¨¡å‹
        sample_idx: æ ·æœ¬ç´¢å¼•
        max_grasps: æœ€å¤§æ˜¾ç¤ºæŠ“å–æ•°é‡
        device: è®¡ç®—è®¾å¤‡
    """
    print(f"æ­£åœ¨å¯è§†åŒ–æ ·æœ¬ {sample_idx} çš„é¢„æµ‹ä¸çœŸå®æŠ“å–å¯¹æ¯”...")

    # è·å–æ ·æœ¬æ•°æ®
    try:
        sample = dataset[sample_idx]
    except Exception as e:
        print(f"è·å–æ ·æœ¬å¤±è´¥: {e}")
        return

    # æ‰“å°æ ·æœ¬ä¿¡æ¯
    print(f"æ ·æœ¬ä¿¡æ¯:")
    print(f"  - åœºæ™¯ID: {sample['scene_id']}")
    print(f"  - è§†è§’ç´¢å¼•: {sample['depth_view_index']}")
    print(f"  - ç‰©ä½“ä»£ç : {sample['obj_code']}")
    print(f"  - æ­£é¢æç¤ºè¯: '{sample['positive_prompt']}'")
    print(f"  - ç‚¹äº‘å½¢çŠ¶: {sample['scene_pc'].shape}")
    print(f"  - çœŸå®æŠ“å–å½¢çŠ¶: {sample['hand_model_pose'].shape}")

    # å‡†å¤‡æ‰¹æ¬¡æ•°æ®è¿›è¡Œé¢„æµ‹
    batch = {}
    for key, value in sample.items():
        if isinstance(value, torch.Tensor):
            batch[key] = value.unsqueeze(0)  # æ·»åŠ æ‰¹æ¬¡ç»´åº¦
        else:
            batch[key] = [value]

    # è¿›è¡ŒæŠ“å–é¢„æµ‹
    print("æ­£åœ¨è¿›è¡ŒæŠ“å–é¢„æµ‹...")
    try:
        # ä»æ ·æœ¬ä¸­è·å–æŠ“å–æ•°é‡
        num_grasps = sample['hand_model_pose'].shape[0] if 'hand_model_pose' in sample else 8
        batch_size = 1  # å½“å‰æ˜¯å•æ ·æœ¬é¢„æµ‹
        
        # ä½¿ç”¨è¯¦ç»†é¢„æµ‹å‡½æ•°
        prediction_result = predict_grasps_with_details(model, batch, device, num_grasps=num_grasps)
        pred_poses = prediction_result['pred_poses']
        outputs = prediction_result['outputs']
        targets = prediction_result['targets']
        
        print(f"âœ“ é¢„æµ‹å®Œæˆï¼Œé¢„æµ‹å§¿æ€å½¢çŠ¶: {pred_poses.shape}")
        
        # æ‰“å°è¯¦ç»†ä¿¡æ¯
        print_forward_get_pose_matched_details(outputs, targets, batch_size, num_grasps)
        
    except Exception as e:
        print(f"é¢„æµ‹å¤±è´¥: {e}")
        return

    # é™ç»´å¹¶ç§»åŠ¨åˆ°è®¾å¤‡
    if pred_poses.dim() == 4:
        pred_poses = pred_poses.squeeze(1) # ä» [B, 1, G, D] -> [B, G, D]

    # è·å–çœŸå®æŠ“å–å§¿æ€å¹¶ç§»åŠ¨åˆ°è®¾å¤‡
    gt_poses = sample['hand_model_pose'].unsqueeze(0).to(device)  # æ·»åŠ æ‰¹æ¬¡ç»´åº¦å¹¶ç§»åŠ¨åˆ°è®¾å¤‡

    # è®¡ç®—è¯¯å·®æŒ‡æ ‡
    print("æ­£åœ¨è®¡ç®—è¯¯å·®æŒ‡æ ‡...")
    errors = calculate_pose_errors(pred_poses, gt_poses, rot_type=model.rot_type)

    # æ‰“å°è¯¯å·®ç»Ÿè®¡
    print(f"\nè¯¯å·®ç»Ÿè®¡ç»“æœ:")
    print(f"ä½ç½®è¯¯å·® (ç±³):")
    print(f"  - å¹³å‡: {errors['translation_mean']:.4f}")
    print(f"  - æ ‡å‡†å·®: {errors['translation_std']:.4f}")
    print(f"  - æœ€å¤§: {errors['translation_max']:.4f}")
    print(f"  - æœ€å°: {errors['translation_min']:.4f}")

    print(f"å…³èŠ‚è§’åº¦è¯¯å·® (MSE):")
    print(f"  - å¹³å‡: {errors['qpos_mean']:.4f}")
    print(f"  - æ ‡å‡†å·®: {errors['qpos_std']:.4f}")
    print(f"  - æœ€å¤§: {errors['qpos_max']:.4f}")
    print(f"  - æœ€å°: {errors['qpos_min']:.4f}")

    print(f"æ—‹è½¬è¯¯å·®:")
    print(f"  - å¹³å‡: {errors['rotation_mean']:.4f}")
    print(f"  - æ ‡å‡†å·®: {errors['rotation_std']:.4f}")
    print(f"  - æœ€å¤§: {errors['rotation_max']:.4f}")
    print(f"  - æœ€å°: {errors['rotation_min']:.4f}")

    # åˆ›å»ºå¯è§†åŒ–å¯¹è±¡åˆ—è¡¨
    vis_objects = []

    # æ·»åŠ åæ ‡è½´
    coordinate_frame = create_coordinate_frame(size=0.1)
    vis_objects.append(coordinate_frame)

    # åˆ›å»ºç‚¹äº‘
    scene_pc = sample['scene_pc']
    object_mask = sample['object_mask']

    if len(object_mask) == len(scene_pc):
        background_pcd, object_pcd = create_highlighted_point_cloud(scene_pc, object_mask)
        if background_pcd is not None:
            vis_objects.append(background_pcd)
        if object_pcd is not None:
            vis_objects.append(object_pcd)
    else:
        pcd = create_point_cloud_from_sample(scene_pc)
        vis_objects.append(pcd)

    # åˆ›å»ºç›®æ ‡ç‰©ä½“mesh
    try:
        obj_verts = sample['obj_verts']
        obj_faces = sample['obj_faces']
        object_mesh = create_object_mesh(obj_verts, obj_faces, color=(0.0, 0.8, 0.0))
        if object_mesh is not None:
            vis_objects.append(object_mesh)
            print("âœ“ ç›®æ ‡ç‰©ä½“meshåˆ›å»ºæˆåŠŸ")
    except Exception as e:
        print(f"ç›®æ ‡ç‰©ä½“meshåˆ›å»ºå¤±è´¥: {e}")

    # åˆ›å»ºæ‰‹éƒ¨mesh - å°è¯•ä»outputså’Œtargetsåˆ›å»º
    try:
        print("æ­£åœ¨ä»æ¨¡å‹è¾“å‡ºåˆ›å»ºæ‰‹éƒ¨mesh...")
        
        # é¦–å…ˆå°è¯•ä»outputså’Œtargetsåˆ›å»ºmesh
        pred_meshes, gt_meshes = create_hand_meshes_from_outputs(
            outputs, targets, batch_size, num_grasps, max_grasps
        )
        
        if pred_meshes and gt_meshes:
            vis_objects.extend(pred_meshes)
            vis_objects.extend(gt_meshes)
            print(f"âœ“ ä»æ¨¡å‹è¾“å‡ºåˆ›å»ºäº† {len(pred_meshes)} ä¸ªé¢„æµ‹meshå’Œ {len(gt_meshes)} ä¸ªçœŸå®mesh")
        else:
            # å›é€€åˆ°ä½¿ç”¨HandModelåˆ›å»ºmesh
            print("å›é€€åˆ°ä½¿ç”¨HandModelåˆ›å»ºmesh...")
            hand_model = HandModel(hand_model_type=HandModelType.LEAP, device='cpu')

            # åˆ›å»ºé¢„æµ‹å’ŒçœŸå®çš„æ‰‹éƒ¨mesh
            pred_meshes, gt_meshes = create_hand_meshes_comparison(
                pred_poses.cpu(), gt_poses.cpu(), hand_model, max_grasps
            )

            if pred_meshes:
                vis_objects.extend(pred_meshes)
                print(f"âœ“ {len(pred_meshes)} ä¸ªé¢„æµ‹æ‰‹éƒ¨meshåˆ›å»ºæˆåŠŸ")

            if gt_meshes:
                vis_objects.extend(gt_meshes)
                print(f"âœ“ {len(gt_meshes)} ä¸ªçœŸå®æ‰‹éƒ¨meshåˆ›å»ºæˆåŠŸ")

    except Exception as e:
        print(f"æ‰‹éƒ¨meshåˆ›å»ºå¤±è´¥: {e}")

    # æ‰“å°å¯è§†åŒ–ç»„ä»¶æ€»ç»“
    print(f"\nå¯è§†åŒ–ç»„ä»¶æ€»ç»“:")
    print(f"  - åæ ‡è½´: âœ“")
    print(f"  - ç‚¹äº‘: âœ“ ({len(scene_pc)} ä¸ªç‚¹)")
    print(f"  - ç›®æ ‡ç‰©ä½“mesh: âœ“")
    print(f"  - é¢„æµ‹æ‰‹éƒ¨mesh: âœ“ ({len(pred_meshes) if 'pred_meshes' in locals() else 0} ä¸ª)")
    print(f"  - çœŸå®æ‰‹éƒ¨mesh: âœ“ ({len(gt_meshes) if 'gt_meshes' in locals() else 0} ä¸ª)")

    # åˆ›å»ºå¯è§†åŒ–çª—å£
    print("\næ­£åœ¨å¯åŠ¨Open3Då¯è§†åŒ–...")
    print("å¯è§†åŒ–è¯´æ˜:")
    print("  - çº¢è‰²ç‚¹: ç›®æ ‡ç‰©ä½“ç‚¹äº‘")
    print("  - ç°è‰²ç‚¹: èƒŒæ™¯ç‚¹äº‘")
    print("  - ç»¿è‰²mesh: ç›®æ ‡ç‰©ä½“mesh")
    print("  - è“è‰²ç³»mesh: é¢„æµ‹çš„æŠ“å–å§¿æ€")
    print("  - çº¢è‰²ç³»mesh: çœŸå®çš„æŠ“å–å§¿æ€")
    print("  - RGBåæ ‡è½´: ä¸–ç•Œåæ ‡ç³»")
    print("\næ“ä½œæç¤º:")
    print("  - é¼ æ ‡å·¦é”®æ‹–æ‹½: æ—‹è½¬è§†è§’")
    print("  - é¼ æ ‡å³é”®æ‹–æ‹½: å¹³ç§»è§†è§’")
    print("  - é¼ æ ‡æ»šè½®: ç¼©æ”¾")

    o3d.visualization.draw_geometries(
        vis_objects,
        window_name=f"é¢„æµ‹ vs çœŸå®æŠ“å–å¯¹æ¯” - Sample {sample_idx} - {sample['obj_code']}",
        width=1400,
        height=900,
        left=50,
        top=50
    )

def batch_analyze_predictions(dataset: SceneLeapPlusDataset, model: DDPMLightning,
                            num_samples: int = 10, device: str = 'cuda') -> Dict[str, float]:
    """
    æ‰¹é‡åˆ†æå¤šä¸ªæ ·æœ¬çš„é¢„æµ‹è¯¯å·®

    Args:
        dataset: SceneLeapPlusDatasetå®ä¾‹
        model: é¢„è®­ç»ƒçš„DDPMLightningæ¨¡å‹
        num_samples: åˆ†æçš„æ ·æœ¬æ•°é‡
        device: è®¡ç®—è®¾å¤‡

    Returns:
        Dict[str, float]: èšåˆçš„è¯¯å·®ç»Ÿè®¡
    """
    print(f"æ­£åœ¨æ‰¹é‡åˆ†æ {num_samples} ä¸ªæ ·æœ¬çš„é¢„æµ‹è¯¯å·®...")

    all_errors = {
        'translation_errors': [],
        'qpos_errors': [],
        'rotation_errors': []
    }

    for i in range(min(num_samples, len(dataset))):
        try:
            print(f"å¤„ç†æ ·æœ¬ {i+1}/{num_samples}...")

            # è·å–æ ·æœ¬æ•°æ®
            sample = dataset[i]

            # å‡†å¤‡æ‰¹æ¬¡æ•°æ®
            batch = {}
            for key, value in sample.items():
                if isinstance(value, torch.Tensor):
                    batch[key] = value.unsqueeze(0)
                else:
                    batch[key] = [value]

            # è¿›è¡Œé¢„æµ‹
            num_grasps = sample['hand_model_pose'].shape[0] if 'hand_model_pose' in sample else 8
            prediction_result = predict_grasps_with_details(model, batch, device, num_grasps=num_grasps)
            pred_poses = prediction_result['pred_poses']
            # æ³¨æ„ï¼šforward_get_pose_matchedè¿”å›çš„å½¢çŠ¶å·²ç»æ˜¯ [B, num_grasps, pose_dim]ï¼Œä¸éœ€è¦é¢å¤–çš„squeezeæ“ä½œ

            gt_poses = sample['hand_model_pose'].unsqueeze(0).to(device)

            # è®¡ç®—è¯¯å·®
            errors = calculate_pose_errors(pred_poses, gt_poses, rot_type=model.rot_type)

            # æ”¶é›†è¯¯å·®æ•°æ®
            all_errors['translation_errors'].append(errors['translation_mean'])
            all_errors['qpos_errors'].append(errors['qpos_mean'])
            all_errors['rotation_errors'].append(errors['rotation_mean'])

        except Exception as e:
            print(f"å¤„ç†æ ·æœ¬ {i} å¤±è´¥: {e}")
            continue

    # è®¡ç®—èšåˆç»Ÿè®¡
    aggregated_stats = {}
    for error_type, error_list in all_errors.items():
        if error_list:
            aggregated_stats[f'{error_type}_mean'] = float(np.mean(error_list))
            aggregated_stats[f'{error_type}_std'] = float(np.std(error_list))
            aggregated_stats[f'{error_type}_max'] = float(np.max(error_list))
            aggregated_stats[f'{error_type}_min'] = float(np.min(error_list))

    # æ‰“å°èšåˆç»“æœ
    print(f"\næ‰¹é‡åˆ†æç»“æœ (åŸºäº {len(all_errors['translation_errors'])} ä¸ªæœ‰æ•ˆæ ·æœ¬):")
    print(f"ä½ç½®è¯¯å·®ç»Ÿè®¡:")
    print(f"  - å¹³å‡: {aggregated_stats.get('translation_errors_mean', 0):.4f} Â± {aggregated_stats.get('translation_errors_std', 0):.4f}")
    print(f"  - èŒƒå›´: [{aggregated_stats.get('translation_errors_min', 0):.4f}, {aggregated_stats.get('translation_errors_max', 0):.4f}]")

    print(f"å…³èŠ‚è§’åº¦è¯¯å·®ç»Ÿè®¡:")
    print(f"  - å¹³å‡: {aggregated_stats.get('qpos_errors_mean', 0):.4f} Â± {aggregated_stats.get('qpos_errors_std', 0):.4f}")
    print(f"  - èŒƒå›´: [{aggregated_stats.get('qpos_errors_min', 0):.4f}, {aggregated_stats.get('qpos_errors_max', 0):.4f}]")

    print(f"æ—‹è½¬è¯¯å·®ç»Ÿè®¡:")
    print(f"  - å¹³å‡: {aggregated_stats.get('rotation_errors_mean', 0):.4f} Â± {aggregated_stats.get('rotation_errors_std', 0):.4f}")
    print(f"  - èŒƒå›´: [{aggregated_stats.get('rotation_errors_min', 0):.4f}, {aggregated_stats.get('rotation_errors_max', 0):.4f}]")

    return aggregated_stats

def main():
    """ä¸»å‡½æ•°"""
    print("=" * 80)
    print("é¢„æµ‹æŠ“å–å§¿æ€ä¸çœŸå®æŠ“å–å§¿æ€å¯¹æ¯”å¯è§†åŒ–")
    print("=" * 80)

    # æ•°æ®è·¯å¾„é…ç½® (ä½¿ç”¨çœŸå®è·¯å¾„)
    root_dir = "/home/xiantuo/source/grasp/SceneLeapPro/data/520_0_sub_3"
    succ_grasp_dir = "/home/xiantuo/source/grasp/SceneLeapPro/data/succ_collect"
    obj_root_dir = "/home/xiantuo/source/grasp/SceneLeapPro/data/object/objaverse_v1/flat_models"

    # æ¨¡å‹é…ç½®
    checkpoint_path = "path/to/your/checkpoint.ckpt"  # éœ€è¦ç”¨æˆ·æŒ‡å®šå®é™…çš„checkpointè·¯å¾„
    device = 'cuda' if torch.cuda.is_available() else 'cpu'

    # å¯è§†åŒ–å‚æ•°
    sample_idx = 0
    max_grasps_to_show = 3
    mode = "camera_centric_scene_mean_normalized"

    try:
        # æ£€æŸ¥checkpointè·¯å¾„
        if not os.path.exists(checkpoint_path):
            print(f"é”™è¯¯: checkpointæ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")
            print("è¯·ä¿®æ”¹è„šæœ¬ä¸­çš„checkpoint_pathå˜é‡ä¸ºå®é™…çš„æ¨¡å‹æ–‡ä»¶è·¯å¾„")
            return

        # åˆå§‹åŒ–æ•°æ®é›†
        print(f"æ­£åœ¨åˆå§‹åŒ–æ•°æ®é›† (æ¨¡å¼: {mode})...")
        dataset = SceneLeapPlusDataset(
            root_dir=root_dir,
            succ_grasp_dir=succ_grasp_dir,
            obj_root_dir=obj_root_dir,
            num_grasps=8,
            mode=mode,
            max_grasps_per_object=2,  # ä½¿ç”¨è¾ƒå°çš„å€¼åŠ å¿«æµ‹è¯•
            mesh_scale=0.1,
            num_neg_prompts=4,
            enable_cropping=True,
            max_points=20000,
            grasp_sampling_strategy="random"
        )

        print(f"âœ“ æ•°æ®é›†åˆå§‹åŒ–æˆåŠŸï¼ŒåŒ…å« {len(dataset)} ä¸ªæ ·æœ¬")

        if len(dataset) == 0:
            print("æ•°æ®é›†ä¸ºç©ºï¼Œè¯·æ£€æŸ¥æ•°æ®è·¯å¾„")
            return

        # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
        print(f"æ­£åœ¨åŠ è½½é¢„è®­ç»ƒæ¨¡å‹...")
        model = load_pretrained_model(checkpoint_path)
        model = model.to(device)

        # å•æ ·æœ¬å¯è§†åŒ–
        print(f"\n{'='*60}")
        print(f"å•æ ·æœ¬å¯è§†åŒ–åˆ†æ")
        print(f"{'='*60}")

        visualize_prediction_vs_ground_truth(
            dataset, model, sample_idx, max_grasps_to_show, device
        )

        # æ‰¹é‡åˆ†æ (å¯é€‰)
        print(f"\n{'='*60}")
        print(f"æ‰¹é‡è¯¯å·®åˆ†æ")
        print(f"{'='*60}")

        batch_stats = batch_analyze_predictions(dataset, model, num_samples=5, device=device)

        print(f"\n{'='*60}")
        print(f"åˆ†æå®Œæˆï¼")
        print(f"{'='*60}")

    except Exception as e:
        print(f"ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
